[
  {
    "objectID": "verification.html",
    "href": "verification.html",
    "title": "Notebook 3 ‚Äî Cross-user verification of daily exploration patterns",
    "section": "",
    "text": "Notebook 3 moves from single-user analysis to a cross-user perspective on daily mobility. Building on the visit-level tables constructed in the previous notebooks, it aggregates data for a set of long-coverage Geolife users (data/visit_level_table_XXX.csv) and asks how their trip-level ‚Äúgoing home‚Äù and exploration behaviour compares.\nThe workflow has three components.\nFirst, I define a consistent sample of users with sufficiently long observation windows and, for each of them, reconstruct home‚Äìhome trips by attaching a trip identifier and within-trip stop order to every visit.\nSecond, I summarise each user‚Äôs mobility with a compact set of trip-level statistics, including the number of active days, the frequency and length of home‚Äìhome trips, and simple indicators of how often non-home stops correspond to first-time places (Pv) versus revisits.\nThird, I compare going-home hazards and within-trip exploration patterns across individuals by fitting user-specific discrete-time logit models in the stop order index, which yield a small number of interpretable parameters describing both the overall level and the within-trip dynamics of ‚Äúgoing home‚Äù and ‚Äúexploring‚Äù for each person.\n\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"DataFrameGroupBy.apply operated on the grouping columns\",\n    category=FutureWarning,\n)\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"Parsing dates in %d/%m/%Y format\",\n    category=UserWarning,\n)\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\nplt.style.use(\"default\")\n\nDATA_DIR = \"data\"  # folder containing visit_level_table_XXX.csv\n\n# Pre-selected long-coverage users (can be updated if needed)\nLONG_USERS = [\n    \"004\", \"003\", \"017\", \"025\", \"030\", \"126\",\n    \"062\", \"084\", \"039\", \"041\", \"022\", \"014\", \"000\",\n    \"002\", \"092\", \"112\", \"104\", \"052\"\n]\n\n\ndef load_visit_table_from_csv(user_id: str,\n                              data_dir: str = DATA_DIR,\n                              verbose: bool = True) -&gt; pd.DataFrame | None:\n    \"\"\"\n    Load visit_level_table_XXX.csv for one user and parse timestamps.\n    Assumes the schema from Notebook 1 / 2:\n      - person\n      - date, first_time, last_time\n      - is_home, next_step, dist_home_m\n      - visit_order_in_day, is_pv, is_pn\n    \"\"\"\n    fname = f\"visit_level_table_{user_id}.csv\"\n    path = os.path.join(data_dir, fname)\n\n    if not os.path.exists(path):\n        if verbose:\n            print(f\"[skip] {fname} not found at {path}\")\n        return None\n\n    vt = pd.read_csv(path)\n    if vt.empty:\n        if verbose:\n            print(f\"[skip] {fname} is empty\")\n        return None\n\n    vt[\"date\"] = pd.to_datetime(vt[\"date\"]).dt.date\n    vt[\"first_dt\"] = pd.to_datetime(vt[\"date\"].astype(str) + \" \" + vt[\"first_time\"])\n    vt[\"last_dt\"] = pd.to_datetime(vt[\"date\"].astype(str) + \" \" + vt[\"last_time\"])\n\n    vt = (\n        vt.sort_values([\"person\", \"date\", \"first_dt\"])\n          .reset_index(drop=True)\n    )\n    return vt\n\n\ndef add_trip_cols_one_day(df_day: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    For a single (person, date), label home‚Äìhome trip episodes.\n\n    trip_id:\n      0 if no trip yet; 1,2,... for successive trips in that day\n    action_order_in_trip:\n      0 for HOME visits; 1,2,... for non-home stops within a trip\n    \"\"\"\n    df_day = df_day.sort_values(\"first_dt\").copy()\n\n    trip_id = 0\n    action_order = 0\n    out = False  # currently away from HOME?\n\n    trip_ids = []\n    action_orders = []\n\n    for _, row in df_day.iterrows():\n        if row[\"is_home\"] == 1:\n            out = False\n            action_order = 0\n            trip_ids.append(trip_id)\n            action_orders.append(0)\n        else:\n            if not out:\n                trip_id += 1\n                action_order = 1\n                out = True\n            else:\n                action_order += 1\n\n            trip_ids.append(trip_id)\n            action_orders.append(action_order)\n\n    df_day[\"trip_id\"] = trip_ids\n    df_day[\"action_order_in_trip\"] = action_orders\n    return df_day\n\n\ndef attach_trip_structure(vt: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Apply add_trip_cols_one_day to each (person, date).\"\"\"\n    vt_trips = (\n        vt.groupby([\"person\", \"date\"], group_keys=False)\n          .apply(add_trip_cols_one_day)\n          .reset_index(drop=True)\n    )\n    return vt_trips",
    "crumbs": [
      "Analysis",
      "Notebook 3 ‚Äì Cross-user verification"
    ]
  },
  {
    "objectID": "verification.html#cross-user-within-trip-exploration-pv",
    "href": "verification.html#cross-user-within-trip-exploration-pv",
    "title": "Notebook 3 ‚Äî Cross-user verification of daily exploration patterns",
    "section": "4. Cross-user within-trip exploration (Pv)",
    "text": "4. Cross-user within-trip exploration (Pv)\nIn this step I summarise within-trip exploration behaviour across users using the Pv logit model. For each user, I first restrict attention to non-home stops within home‚Äìhome trips and count, for each stop order (k ), how many of these stops are first-time places (Pv) versus revisits (Pn). Using these counts, I estimate a user-specific logit model where the stop order in the trip enters as a linear predictor (after recentring), and use the fitted model to obtain a smooth Pv probability curve over (k). The resulting fitted curves are then overlaid across users, so that differences in both the baseline exploration rate and the slope with respect to stop order can be inspected on a common scale.\n\nMAXK_PV = 12      # maximum stop order for Pv curves\nMIN_N_PV = 20     # minimum n per k for fitting / error\n\n\ndef compute_pv_curve(vt_trips: pd.DataFrame,\n                     maxk: int = MAXK_PV,\n                     min_n: int = MIN_N_PV) -&gt; dict | None:\n    \"\"\"\n    Given a visit table with trip_id and action_order_in_trip,\n    compute:\n      - empirical Pv share by stop order k\n      - fitted Pv probability via logit in k\n    Returns dict with:\n      - k_axis, pv_emp, pv_hat, pv_df\n    \"\"\"\n    vis_places = vt_trips[\n        (vt_trips[\"is_home\"] == 0) &\n        (vt_trips[\"trip_id\"] &gt; 0) &\n        (vt_trips[\"action_order_in_trip\"] &gt; 0)\n    ].copy()\n    vis_places = vis_places[vis_places[\"action_order_in_trip\"] &lt;= maxk].copy()\n\n    if vis_places.empty:\n        return None\n\n    pv_by_k = (\n        vis_places\n        .groupby(\"action_order_in_trip\")[[\"is_pv\", \"is_pn\"]]\n        .agg(\n            pv_cnt=(\"is_pv\", \"sum\"),\n            pn_cnt=(\"is_pn\", \"sum\"),\n            n=(\"is_pv\", \"size\")\n        )\n    )\n\n    pv_by_k = pv_by_k.reindex(range(1, maxk + 1), fill_value=0)\n\n    denom = pv_by_k[\"pv_cnt\"] + pv_by_k[\"pn_cnt\"]\n    pv_emp = (pv_by_k[\"pv_cnt\"] / denom.where(denom &gt; 0, np.nan)).to_numpy()\n\n    k_axis = np.arange(1, maxk + 1)\n\n    valid_k = pv_by_k.index[pv_by_k[\"n\"] &gt;= min_n]\n    if len(valid_k) == 0:\n        return {\n            \"k_axis\": k_axis,\n            \"pv_emp\": pv_emp,\n            \"pv_hat\": pv_emp.copy(),\n            \"pv_df\": pv_by_k.reset_index().rename(columns={\"index\": \"k\"})\n        }\n\n    vis_lr_sub = vis_places[\n        vis_places[\"action_order_in_trip\"].isin(valid_k)\n    ].copy()\n\n    mean_k = vis_lr_sub[\"action_order_in_trip\"].mean()\n    vis_lr_sub[\"k_centered\"] = vis_lr_sub[\"action_order_in_trip\"] - mean_k\n\n    X = sm.add_constant(vis_lr_sub[\"k_centered\"])\n    y = vis_lr_sub[\"is_pv\"]\n\n    logit_model = sm.Logit(y, X)\n    res = logit_model.fit(disp=False)\n\n    beta0 = res.params[\"const\"]\n    beta1 = res.params[\"k_centered\"]\n\n    k_centered_grid = k_axis - mean_k\n    lin_pred = beta0 + beta1 * k_centered_grid\n    pv_hat = 1.0 / (1.0 + np.exp(-lin_pred))\n\n    pv_df = pv_by_k.reset_index().rename(columns={\"index\": \"k\"})\n\n    return {\n        \"k_axis\": k_axis,\n        \"pv_emp\": pv_emp,\n        \"pv_hat\": pv_hat,\n        \"pv_df\": pv_df,\n    }\n\n\n# compute Pv curves for all users\npv_curves = {}\n\nfor uid in summary_df[\"uid\"]:\n    vt = load_visit_table_from_csv(uid, verbose=False)\n    if vt is None or vt.empty:\n        continue\n\n    vt_trips = attach_trip_structure(vt)\n    curve = compute_pv_curve(vt_trips, maxk=MAXK_PV, min_n=MIN_N_PV)\n    if curve is not None:\n        pv_curves[uid] = curve\n\nprint(\"Users with usable Pv curves:\", len(pv_curves))\n\n\n# overlay fitted Pv curves across users\nplt.figure(figsize=(8, 5))\n\nfor uid, curve in pv_curves.items():\n    k_axis = curve[\"k_axis\"]\n    pv_hat = curve[\"pv_hat\"]\n    n_k = curve[\"pv_df\"][\"n\"].to_numpy()\n\n    mask_fit = n_k &gt;= MIN_N_PV\n    if not mask_fit.any():\n        continue\n\n    plt.plot(\n        k_axis[mask_fit],\n        pv_hat[mask_fit],\n        alpha=0.6,\n        linewidth=1,\n        label=uid\n    )\n\nplt.ylim(0, 0.5)\nplt.xlim(1, MAXK_PV)\nplt.xlabel(\"Stop order k in trip (non-home)\")\nplt.ylabel(\"P(stop is Pv)\")\nplt.title(\"Within-trip Pv probability ‚Äî fitted curves across users\")\nplt.legend(ncol=3, fontsize=8, frameon=False)\nplt.tight_layout()\nplt.show()\n\nUsers with usable Pv curves: 18\n\n\n\n\n\n\n\n\n\nThe panel shows the fitted within-trip Pv probability curves for all long-coverage users, truncated at the first ten non-home stops and only displayed where each stop order has at least ten observations. Vertical differences between curves reflect how exploratory a user is on average, while the slope of each curve captures how exploration changes along the sequence of stops within a trip. Many users exhibit a gently upward-sloping pattern, indicating that the probability of visiting a first-time place tends to rise as the trip proceeds, whereas a few users have nearly flat curves, consistent with a roughly constant exploration rate across stops.",
    "crumbs": [
      "Analysis",
      "Notebook 3 ‚Äì Cross-user verification"
    ]
  },
  {
    "objectID": "verification.html#model-fit-for-hazard-and-pv-curves",
    "href": "verification.html#model-fit-for-hazard-and-pv-curves",
    "title": "Notebook 3 ‚Äî Cross-user verification of daily exploration patterns",
    "section": "5. Model fit for hazard and Pv curves",
    "text": "5. Model fit for hazard and Pv curves\nTo assess how well the simple logit specifications reproduce the empirical patterns, I construct a user-level goodness-of-fit measure for both models. For each user and each stop order within the range used in the analysis, I compare the empirical probabilities with the corresponding fitted values and compute a mean squared error (MSE) over the available stop orders. For the going-home hazard, the MSE is based on all non-missing hazard points, while for the Pv model it is restricted to stop orders with at least MIN_N_PV observations. The resulting MSEs are then merged back into the user summary table, providing a compact diagnostic of how well the fitted curves approximate the empirical profiles for each individual.\n\n# === 5.0 Compute per-user MSE for hazard and Pv ===\n\n# 5.0.1 Hazard MSE for each user\nhaz_fit_rows = []\nfor uid, hc in hazard_curves.items():\n    emp = hc[\"haz_emp\"]      # empirical hazard h_k\n    hat = hc[\"haz_hat\"]      # fitted hazard \\hat{h}_k\n\n    mask = ~np.isnan(emp)\n    if not mask.any():\n        continue\n\n    mse = np.mean((emp[mask] - hat[mask])**2)\n    haz_fit_rows.append({\n        \"uid\": uid,\n        \"hazard_mse\": mse,\n    })\n\nhaz_fit_df = (\n    pd.DataFrame(haz_fit_rows)\n      .sort_values(\"uid\")\n      .reset_index(drop=True)\n)\n\n\n# 5.0.2 Pv MSE for each user\npv_fit_rows = []\nfor uid, curve in pv_curves.items():\n    emp = curve[\"pv_emp\"]    # empirical Pv share by k\n    hat = curve[\"pv_hat\"]    # fitted Pv probability by k\n    n_k = curve[\"pv_df\"][\"n\"].to_numpy()  # sample size per k\n\n    mask = (~np.isnan(emp)) & (n_k &gt;= MIN_N_PV)\n    if not mask.any():\n        continue\n\n    mse = np.mean((emp[mask] - hat[mask])**2)\n    pv_fit_rows.append({\n        \"uid\": uid,\n        \"pv_mse\": mse,\n    })\n\npv_fit_df = (\n    pd.DataFrame(pv_fit_rows)\n      .sort_values(\"uid\")\n      .reset_index(drop=True)\n)\n\n\n# 5.1 Hazard MSE summary\nh_min   = haz_fit_df[\"hazard_mse\"].min()\nh_med   = haz_fit_df[\"hazard_mse\"].median()\nh_max   = haz_fit_df[\"hazard_mse\"].max()\n\nbest_haz  = haz_fit_df.nsmallest(2, \"hazard_mse\")\nworst_haz = haz_fit_df.nlargest(2, \"hazard_mse\")\n\nprint(f\"Hazard MSE range: {h_min:.4f} ‚Äì {h_max:.4f}, median {h_med:.4f}\")\nprint(\"Best hazard fits:\")\nprint(best_haz.to_string(index=False))\n\nprint(\"Worst hazard fits:\")\nprint(worst_haz.to_string(index=False))\n\n\n# 5.2 Pv MSE summary\np_min   = pv_fit_df[\"pv_mse\"].min()\np_med   = pv_fit_df[\"pv_mse\"].median()\np_max   = pv_fit_df[\"pv_mse\"].max()\n\nbest_pv  = pv_fit_df.nsmallest(2, \"pv_mse\")\nworst_pv = pv_fit_df.nlargest(2, \"pv_mse\")\n\nprint(f\"\\nPv MSE range: {p_min:.4f} ‚Äì {p_max:.4f}, median {p_med:.4f}\")\nprint(\"Best Pv fits:\")\nprint(best_pv.to_string(index=False))\n\nprint(\"Worst Pv fits:\")\nprint(worst_pv.to_string(index=False))\n\n# 5.3 Merge fit stats into summary_df and save\nsummary_fit = (\n    summary_df\n    .merge(haz_fit_df, on=\"uid\", how=\"left\")\n    .merge(pv_fit_df,  on=\"uid\", how=\"left\")\n)\nsummary_fit.to_csv(\"data/user_trip_summary_with_fit.csv\", index=False)\n\nHazard MSE range: 0.0002 ‚Äì 0.0138, median 0.0013\nBest hazard fits:\nuid  hazard_mse\n003    0.000184\n030    0.000259\nWorst hazard fits:\nuid  hazard_mse\n112    0.013826\n104    0.009189\n\nPv MSE range: 0.0008 ‚Äì 0.0091, median 0.0022\nBest Pv fits:\nuid   pv_mse\n030 0.000807\n084 0.001045\nWorst Pv fits:\nuid   pv_mse\n112 0.009124\n041 0.003860\n\n\n\n5.a Hazard fit: best and worst users\nFigure 5a compares the empirical trip-level going-home hazard with the fitted logit curves for the two best-fitting users (003, 030) and the two worst-fitting users (112, 104). For Users 003 and 030, the points lie almost exactly on the fitted line across stop orders 1‚Äì12, and the MSE is essentially zero, which means the simple logit trend in k captures their going-home probabilities very well. For Users 112 and 104 the overall MSE is still small (‚âà0.014 and ‚âà0.009), but the last few stops show visible deviations: with few long trips, individual ‚Äúgo home‚Äù events at high k create noisy empirical hazards that the smooth logit curve only approximates.\n\n# 5.a Hazard fit: 2√ó2 panels for best and worst users\n\nhaz_best  = [\"003\", \"030\"]\nhaz_worst = [\"112\", \"104\"]\nhaz_order = haz_best + haz_worst   # top row: best, bottom row: worst\n\nhaz_mse_dict = haz_fit_df.set_index(\"uid\")[\"hazard_mse\"].to_dict()\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 6), sharex=True, sharey=True)\n\nfor ax, uid in zip(axes.ravel(), haz_order):\n    hc  = hazard_curves[uid]\n    k   = hc[\"k_axis\"]\n    emp = hc[\"haz_emp\"]\n    hat = hc[\"haz_hat\"]\n\n    mask_emp = ~np.isnan(emp)\n\n    ax.scatter(k[mask_emp], emp[mask_emp], s=25, label=\"Empirical hazard\")\n    ax.plot(k, hat, label=\"Fitted hazard\")\n\n    ax.set_title(f\"User {uid} (MSE={haz_mse_dict[uid]:.3f})\")\n    ax.set_xlim(1, MAXK)\n    ax.set_ylim(0, 1)\n\naxes[0, 0].legend()\n\nfig.suptitle(\"Trip-level going-home hazard ‚Äî best and worst fits\", y=0.98)\nfig.text(0.5, 0.04, \"Stop order k in trip\", ha=\"center\")\nfig.text(0.04, 0.5, \"P(go home | trip has reached k)\", va=\"center\", rotation=\"vertical\")\n\nplt.tight_layout(rect=[0.06, 0.06, 1, 0.94])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5.b Pv fit: best and worst users\nFigure 5b repeats the same exercise for within-trip exploration. For Users 030 and 084 (best fits), the fitted Pv probability is almost indistinguishable from the empirical Pv share for k up to 10, and the MSE stays around 0.001, indicating that a linear logit trend in stop order is an adequate summary of how exploration evolves within their trips. For Users 112 and 041 (worst fits), the fitted line still gets the general level and slope roughly right, but the empirical points at higher k fluctuate more strongly (especially for 112), reflecting sparse data for long trips rather than a systematic failure of the model.\n\n# 5.b Pv fit: 2√ó2 panels for best and worst users\n\npv_best  = [\"030\", \"084\"]\npv_worst = [\"112\", \"041\"]\npv_order = pv_best + pv_worst\n\npv_mse_dict = pv_fit_df.set_index(\"uid\")[\"pv_mse\"].to_dict()\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 6), sharex=True, sharey=True)\n\nfor ax, uid in zip(axes.ravel(), pv_order):\n    curve = pv_curves[uid]\n    k     = curve[\"k_axis\"]\n    emp   = curve[\"pv_emp\"]\n    hat   = curve[\"pv_hat\"]\n    n_k   = curve[\"pv_df\"][\"n\"].to_numpy()\n\n    mask_emp = ~np.isnan(emp)\n    mask_fit = (n_k &gt;= MIN_N_PV)\n\n    ax.scatter(k[mask_emp], emp[mask_emp], s=25, label=\"Empirical Pv share\")\n    ax.plot(k[mask_fit], hat[mask_fit], label=\"Fitted Pv probability\")\n\n    ax.set_title(f\"User {uid} (MSE={pv_mse_dict[uid]:.3f})\")\n    ax.set_xlim(1, MAXK_PV)\n    ax.set_ylim(0, 0.5)\n\naxes[0, 0].legend()\n\nfig.suptitle(\"Within-trip Pv probability ‚Äî best and worst fits\", y=0.98)\nfig.text(0.5, 0.04, \"Stop order k in trip (non-home)\", ha=\"center\")\nfig.text(0.04, 0.5, \"P(stop is Pv)\", va=\"center\", rotation=\"vertical\")\n\nplt.tight_layout(rect=[0.06, 0.06, 1, 0.94])\nplt.show()",
    "crumbs": [
      "Analysis",
      "Notebook 3 ‚Äì Cross-user verification"
    ]
  },
  {
    "objectID": "verification.html#user-typology-in-multivariate-trip-space",
    "href": "verification.html#user-typology-in-multivariate-trip-space",
    "title": "Notebook 3 ‚Äî Cross-user verification of daily exploration patterns",
    "section": "6. User typology in multivariate trip space",
    "text": "6. User typology in multivariate trip space\nFor the 18 long-coverage Geolife users, I construct a nine-dimensional mobility profile capturing trip frequency, trip length, going-home behaviour, and within-trip exploration. After standardising these features, a k-means model with k = 3 identifies three distinct user types.\nCluster 0 ‚Äì multi-stop revisitors Users take relatively long trips (around seven non-home stops), have consistently low going-home hazards, and show the lowest exploration rate (about 17% Pv). Their mobility is dominated by chains of familiar places.\nCluster 1 ‚Äì errand-homebodies Trips are shorter (four to five stops), the probability of going home after the first stop is the highest, and exploration is moderate (Pv around 24%). Many outings resemble short errand trips that end quickly.\nCluster 2 ‚Äì exploratory roamers Users make fewer and shorter trips (around three stops) but show strong exploration. Once they remain out beyond the first stops, later-stop hazards stay low and Pv increases, indicating a tendency to continue visiting new places.\nTo understand which behavioural dimensions drive these groups, the nine features are projected onto their first three principal components. PC1 contrasts long, routinised multi-stop mobility with shorter and more exploratory patterns. PC2 reflects overall going-out frequency and activity levels. PC3 captures late-trip behaviour, separating users whose return-home probability rises at higher stop orders from those who continue to stay out.\n\nfeat_cols = [\n    \"mean_stops_per_trip\",     \n    \"median_stops_per_trip\",\n    \"hazard_h1\",              \n    \"hazard_h2_const\",        \n    \"hazard_beta_k\",          \n    \"overall_pv_share\",       \n    \"pv_odds_ratio_per_stop\",  \n    \"n_trips\",                \n    \"n_days_nonhome\",        \n]\n\nX = summary_fit[feat_cols].dropna()\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nkmeans = KMeans(n_clusters=3, random_state=0)\nsummary_fit[\"cluster3\"] = kmeans.fit_predict(X_scaled)\n\n\npca = PCA(n_components=3)\nPCs = pca.fit_transform(X_scaled)\nsummary_fit[[\"PC1\", \"PC2\", \"PC3\"]] = PCs\n\nloadings = pd.DataFrame(\n    pca.components_.T,\n    index=feat_cols,\n    columns=[\"PC1\", \"PC2\", \"PC3\"]\n)\ndisplay(loadings.round(2))\n\nc:\\Users\\Jingqi\\anaconda3\\envs\\geospatial\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\n\n\n\n\nmean_stops_per_trip\n0.46\n-0.16\n0.06\n\n\nmedian_stops_per_trip\n0.43\n-0.29\n0.14\n\n\nhazard_h1\n-0.36\n0.33\n0.08\n\n\nhazard_h2_const\n-0.29\n0.14\n0.62\n\n\nhazard_beta_k\n0.25\n-0.01\n0.71\n\n\noverall_pv_share\n-0.31\n-0.35\n0.25\n\n\npv_odds_ratio_per_stop\n-0.34\n-0.07\n-0.09\n\n\nn_trips\n0.12\n0.66\n0.07\n\n\nn_days_nonhome\n0.32\n0.45\n-0.05\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D \n\nfig = plt.figure(figsize=(7, 6))\nax = fig.add_subplot(111, projection=\"3d\")\n\ncolors = {0: \"tab:orange\", 1: \"tab:blue\", 2: \"tab:green\"}\n\nfor c in sorted(summary_fit[\"cluster3\"].unique()):\n    sub = summary_fit[summary_fit[\"cluster3\"] == c]\n    ax.scatter(\n        sub[\"PC1\"], sub[\"PC2\"], sub[\"PC3\"],\n        c=colors[c], label=f\"Cluster {c}\", s=40\n    )\n    for _, row in sub.iterrows():\n        ax.text(\n            row[\"PC1\"], row[\"PC2\"], row[\"PC3\"],\n            row[\"uid\"],\n            fontsize=8\n        )\n\nax.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.0%})\")\nax.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.0%})\")\nax.set_zlabel(f\"PC3 ({pca.explained_variance_ratio_[2]:.0%})\")\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCluster 0 (orange) ‚Äî multi-stop revisitors This group sits mainly on the right side of PC1, indicating long trips with many non-home stops. Their PC2 values are moderate, showing a normal level of going-out frequency. PC3 tends to be low, meaning they rarely return home in the later part of a trip. In short: long, multi-stop routines concentrated around familiar places.\nCluster 1 (blue) ‚Äî errand-homebodies These users cluster toward higher PC2, reflecting frequent going-out. Their PC1 values are slightly left or near the centre, consistent with short to medium trips. PC3 is higher than other groups, indicating a strong tendency to return home after the first one or two stops. In short: they go out often, but most outings are short errand-like trips that end quickly.\nCluster 2 (green) ‚Äî exploratory roamers This cluster contains user 092, positioned at the low end of all three PCs: low PC1 (short trips), low PC2 (infrequent going-out), and low PC3 (very low late-trip hazard). In short: rare outings, but whenever they go out, they behave highly exploratively rather than following routines.",
    "crumbs": [
      "Analysis",
      "Notebook 3 ‚Äì Cross-user verification"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geolife Daily Mobility",
    "section": "",
    "text": "Welcome!\nThis mini site presents an exploratory case study of daily mobility using the Geolife GPS trajectories. The analysis is organised into three main notebooks plus a short synthesis:\n\nLiterature Review & Project Overview\nNotebook 1 ‚Äì Data cleaning\nFrom raw Geolife trajectories to a visit-level table: loading .plt files, filtering to Beijing, projecting to a 500 m grid, and detecting home / stay locations.\nNotebook 2 ‚Äì Trips by action\nBuilds home‚Äìhome trips from visit events, then studies within-trip behaviour, including the discrete-time hazard of going home and the probability of visiting new places (Pv).\nNotebook 3 ‚Äì Cross-user verification\nExtends the trip-level analysis to multiple long-coverage users, compares going-home hazards and exploration profiles, and clusters users into behavioural types.\nFindings and conclusion\nSummarises the main empirical patterns and discusses which aspects of daily mobility appear systematic at the population level and which are strongly user-specific.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Results and Discussion",
    "section": "",
    "text": "Notebook 2 (within-user modelling) and Notebook 3 (cross-user comparison) together show which aspects of daily mobility are common across people and which vary substantially between individuals. This section therefore emphasises the behavioural patterns implied by the Geolife sample, rather than the implementation details.",
    "crumbs": [
      "Analysis",
      "Findings and conclusion"
    ]
  },
  {
    "objectID": "conclusion.html#population-level-behavioural-structure",
    "href": "conclusion.html#population-level-behavioural-structure",
    "title": "Results and Discussion",
    "section": "1. Population-level behavioural structure",
    "text": "1. Population-level behavioural structure\nAcross long-coverage users, a consistent structural pattern appears: daily movement is organised into short, home-anchored outings rather than long, continuous trajectories. Most observed behaviour can be represented as sequences of HOME ‚Üí non-home stop(s) ‚Üí HOME, supporting the use of home‚Äìhome trips (non-home visits bracketed by home) as a meaningful unit for describing everyday mobility.",
    "crumbs": [
      "Analysis",
      "Findings and conclusion"
    ]
  },
  {
    "objectID": "conclusion.html#heterogeneity-in-trip-termination-going-home",
    "href": "conclusion.html#heterogeneity-in-trip-termination-going-home",
    "title": "Results and Discussion",
    "section": "2. Heterogeneity in trip termination (‚Äúgoing home‚Äù)",
    "text": "2. Heterogeneity in trip termination (‚Äúgoing home‚Äù)\nWhile the home‚Äìhome structure is shared, how trips end differs sharply across users. The discrete-time hazard curves show substantial heterogeneity in both level and shape. Many users exhibit a pronounced one-stop errand effect‚Äîa high probability of returning home immediately after the first non-home stop‚Äîwhereas others show a weaker early spike. A smaller subset display relatively higher hazards later in the trip, though these late-trip patterns often coincide with sparse data at large stop orders. The most consistent population-level feature is that, once a trip extends beyond the first few stops, the probability of returning home typically drops and stabilises at a lower level, rather than continuing to rise steadily with trip length.",
    "crumbs": [
      "Analysis",
      "Findings and conclusion"
    ]
  },
  {
    "objectID": "conclusion.html#heterogeneity-in-exploration-first-time-places-pv",
    "href": "conclusion.html#heterogeneity-in-exploration-first-time-places-pv",
    "title": "Results and Discussion",
    "section": "3. Heterogeneity in exploration (first-time places, Pv)",
    "text": "3. Heterogeneity in exploration (first-time places, Pv)\nExploration behaviour is also diverse. For many users, the probability that a stop is a first-time place (Pv) increases with stop order, implying that novelty-seeking tends to occur after the outing has already continued for multiple stops (i.e., mid- to late-trip exploration). For other users, Pv probability is nearly flat across stop order, consistent with trips dominated by revisits to familiar locations. Overall, stop order is a useful axis for explaining exploration dynamics, but there is no single universal Pv curve that fits everyone.",
    "crumbs": [
      "Analysis",
      "Findings and conclusion"
    ]
  },
  {
    "objectID": "conclusion.html#user-typology-and-systematic-differences",
    "href": "conclusion.html#user-typology-and-systematic-differences",
    "title": "Results and Discussion",
    "section": "4. User typology and systematic differences",
    "text": "4. User typology and systematic differences\nWhen trip frequency, trip complexity (stops per trip), going-home parameters, and exploration parameters are combined into a multivariate profile, users do not form a single homogeneous group. Clustering in this feature space consistently separates three behavioural types: (1) multi-stop revisitors, characterised by longer trips with low exploration; (2) errand-oriented users, characterised by high first-stop return probabilities and shorter outings; and (3) exploratory roamers, characterised by fewer outings but comparatively stronger exploration conditional on being out. Their separation in PCA space suggests that these differences reflect systematic behavioural variation, not only random sampling noise.",
    "crumbs": [
      "Analysis",
      "Findings and conclusion"
    ]
  },
  {
    "objectID": "conclusion.html#summary",
    "href": "conclusion.html#summary",
    "title": "Results and Discussion",
    "section": "5. Summary",
    "text": "5. Summary\nDaily mobility in Geolife therefore shows a clear shared backbone‚Äîmovement structured into home‚Äìhome trips and a tendency for ‚Äúgoing home‚Äù probabilities to settle into a lower regime after early stops‚Äîalongside substantial, interpretable individual differences. The strongest sources of differentiation are (i) the strength of the one-stop errand pattern, (ii) whether exploration increases with stop order, and (iii) how users balance novelty against routine across trips.",
    "crumbs": [
      "Analysis",
      "Findings and conclusion"
    ]
  },
  {
    "objectID": "by action.html",
    "href": "by action.html",
    "title": "Notebook 2 ‚Äî Trip-level exploration and ‚Äúgoing home‚Äù behaviour",
    "section": "",
    "text": "In this notebook the analysis starts from the visit-level table constructed in Notebook 1. The file visit_level_table_000.csv is loaded from the data folder and the basic Python environment (NumPy, pandas, Matplotlib) is set up. Date and time fields are combined into full start and end timestamps for each visit (first_dt and last_dt), and the user identifier is taken from the person column for use in plots. The table is then sorted by person, date, and start time to produce a clean, time-ordered record of visit events rather than daily aggregates; subsequent sections build on this event sequence to identify complete home‚Äìhome trips (individual ‚Äúoutings‚Äù).\n\nimport warnings\n\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"DataFrameGroupBy.apply operated on the grouping columns\",\n    category=FutureWarning,\n)\n\n\n# 1. Environment and data loading\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nplt.style.use(\"default\")\n\n# Path to the visit-level table saved from Notebook 1\npath = \"data/visit_level_table_000.csv\"\nvisit_table = pd.read_csv(path)\n\nprint(\"Rows in visit_table:\", len(visit_table))\ndisplay(visit_table.head())\n\n# Parse date + times into full timestamps\nvisit_table[\"date\"] = pd.to_datetime(visit_table[\"date\"]).dt.date\n\nvisit_table[\"first_dt\"] = pd.to_datetime(\n    visit_table[\"date\"].astype(str) + \" \" + visit_table[\"first_time\"]\n)\nvisit_table[\"last_dt\"] = pd.to_datetime(\n    visit_table[\"date\"].astype(str) + \" \" + visit_table[\"last_time\"]\n)\n\n# Identify the user ID from the table\nUSER_ID = str(visit_table[\"person\"].iloc[0])\nprint(\"Unique persons in this file:\", visit_table[\"person\"].unique())\nprint(\"USER_ID used in plots:\", USER_ID)\n\n# Ensure rows are ordered in time\nvisit_table = (\n    visit_table\n    .sort_values([\"person\", \"date\", \"first_dt\"])\n    .reset_index(drop=True)\n)\nprint(\"Rows in visit_table after sorting:\", len(visit_table))\n\nRows in visit_table: 1841\n\n\n\n\n\n\n\n\n\nperson\ndate\nfirst_time\nlast_time\nduration_min\ngrid_ci\ngrid_rj\ngrid_center_lon\ngrid_center_lat\ndist_home_m\nis_home\nis_sw\nis_pv\nis_pn\nplace_id\nnext_step\nvisit_order_in_day\naction_order\n\n\n\n\n0\n0\n2008-10-23\n03:00:55\n04:13:32\n72.6\n40\n50\n116.300184\n39.984308\n3201.6\n0\n0\n1\n0\nPv1\nsw\n1\n1\n\n\n1\n0\n2008-10-23\n09:42:25\n09:42:30\n0.1\n43\n55\n116.317527\n40.006935\n500.0\n0\n1\n0\n0\nSW\nhome\n2\n2\n\n\n2\n0\n2008-10-23\n09:42:35\n10:05:29\n22.9\n44\n55\n116.323385\n40.006970\n0.0\n1\n0\n0\n0\nHOME\nsw\n3\n0\n\n\n3\n0\n2008-10-23\n10:05:34\n10:30:15\n24.7\n43\n55\n116.317527\n40.006935\n500.0\n0\n1\n0\n0\nSW\nhome\n4\n1\n\n\n4\n0\n2008-10-23\n10:30:20\n11:10:57\n40.6\n44\n55\n116.323385\n40.006970\n0.0\n1\n0\n0\n0\nHOME\nnone\n5\n0\n\n\n\n\n\n\n\nUnique persons in this file: [0]\nUSER_ID used in plots: 0\nRows in visit_table after sorting: 1841",
    "crumbs": [
      "Analysis",
      "Notebook 2 ‚Äì Trips by action"
    ]
  },
  {
    "objectID": "by action.html#import-the-datasets-from-000",
    "href": "by action.html#import-the-datasets-from-000",
    "title": "Notebook 2 ‚Äî Trip-level exploration and ‚Äúgoing home‚Äù behaviour",
    "section": "",
    "text": "In this notebook the analysis starts from the visit-level table constructed in Notebook 1. The file visit_level_table_000.csv is loaded from the data folder and the basic Python environment (NumPy, pandas, Matplotlib) is set up. Date and time fields are combined into full start and end timestamps for each visit (first_dt and last_dt), and the user identifier is taken from the person column for use in plots. The table is then sorted by person, date, and start time to produce a clean, time-ordered record of visit events rather than daily aggregates; subsequent sections build on this event sequence to identify complete home‚Äìhome trips (individual ‚Äúoutings‚Äù).\n\nimport warnings\n\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"DataFrameGroupBy.apply operated on the grouping columns\",\n    category=FutureWarning,\n)\n\n\n# 1. Environment and data loading\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nplt.style.use(\"default\")\n\n# Path to the visit-level table saved from Notebook 1\npath = \"data/visit_level_table_000.csv\"\nvisit_table = pd.read_csv(path)\n\nprint(\"Rows in visit_table:\", len(visit_table))\ndisplay(visit_table.head())\n\n# Parse date + times into full timestamps\nvisit_table[\"date\"] = pd.to_datetime(visit_table[\"date\"]).dt.date\n\nvisit_table[\"first_dt\"] = pd.to_datetime(\n    visit_table[\"date\"].astype(str) + \" \" + visit_table[\"first_time\"]\n)\nvisit_table[\"last_dt\"] = pd.to_datetime(\n    visit_table[\"date\"].astype(str) + \" \" + visit_table[\"last_time\"]\n)\n\n# Identify the user ID from the table\nUSER_ID = str(visit_table[\"person\"].iloc[0])\nprint(\"Unique persons in this file:\", visit_table[\"person\"].unique())\nprint(\"USER_ID used in plots:\", USER_ID)\n\n# Ensure rows are ordered in time\nvisit_table = (\n    visit_table\n    .sort_values([\"person\", \"date\", \"first_dt\"])\n    .reset_index(drop=True)\n)\nprint(\"Rows in visit_table after sorting:\", len(visit_table))\n\nRows in visit_table: 1841\n\n\n\n\n\n\n\n\n\nperson\ndate\nfirst_time\nlast_time\nduration_min\ngrid_ci\ngrid_rj\ngrid_center_lon\ngrid_center_lat\ndist_home_m\nis_home\nis_sw\nis_pv\nis_pn\nplace_id\nnext_step\nvisit_order_in_day\naction_order\n\n\n\n\n0\n0\n2008-10-23\n03:00:55\n04:13:32\n72.6\n40\n50\n116.300184\n39.984308\n3201.6\n0\n0\n1\n0\nPv1\nsw\n1\n1\n\n\n1\n0\n2008-10-23\n09:42:25\n09:42:30\n0.1\n43\n55\n116.317527\n40.006935\n500.0\n0\n1\n0\n0\nSW\nhome\n2\n2\n\n\n2\n0\n2008-10-23\n09:42:35\n10:05:29\n22.9\n44\n55\n116.323385\n40.006970\n0.0\n1\n0\n0\n0\nHOME\nsw\n3\n0\n\n\n3\n0\n2008-10-23\n10:05:34\n10:30:15\n24.7\n43\n55\n116.317527\n40.006935\n500.0\n0\n1\n0\n0\nSW\nhome\n4\n1\n\n\n4\n0\n2008-10-23\n10:30:20\n11:10:57\n40.6\n44\n55\n116.323385\n40.006970\n0.0\n1\n0\n0\n0\nHOME\nnone\n5\n0\n\n\n\n\n\n\n\nUnique persons in this file: [0]\nUSER_ID used in plots: 0\nRows in visit_table after sorting: 1841",
    "crumbs": [
      "Analysis",
      "Notebook 2 ‚Äì Trips by action"
    ]
  },
  {
    "objectID": "by action.html#from-daily-visits-to-homehome-trips",
    "href": "by action.html#from-daily-visits-to-homehome-trips",
    "title": "Notebook 2 ‚Äî Trip-level exploration and ‚Äúgoing home‚Äù behaviour",
    "section": "2. From daily visits to home‚Äìhome trips",
    "text": "2. From daily visits to home‚Äìhome trips\nThe next stage groups individual visit events into complete home-to-home trips. For each person and each day, the visit records are ordered by their start times. Any record marked is_home == 1 is treated as being at HOME. A new trip begins the moment the user leaves HOME for a non-home location, continues through any consecutive non-home stops, and ends as soon as the user returns home. If the user never comes back home later that day, the trip is considered to end at the final non-home visit. This approach turns a simple list of places into meaningful ‚Äúoutings‚Äù that begin at home and eventually return there.\nTo make later analysis easier, each visit receives two indexing variables. The first, trip_id, indicates which trip of the day the visit belongs to, with 0 reserved for visits that do not fall inside any trip. The second, action_order_in_trip, captures the position of each stop within its trip. Non-home stops are numbered sequentially as 1, 2, 3, and so on, while HOME visits at the beginning or end of a trip are assigned 0. These indices create a clean structure for studying how behaviour unfolds over the course of each outing.\n\ndef add_trip_cols(df_day):\n    \"\"\"\n    For a single (person, date), label home‚Äìhome trip episodes.\n\n    - trip_id: which trip on that day (1, 2, ...), 0 if not in a trip.\n    - action_order_in_trip: order within the current trip\n      (1, 2, 3, ... for non-home visits, 0 for HOME).\n    \"\"\"\n    df_day = df_day.sort_values(\"first_dt\").copy()\n\n    trip_id = 0\n    action_order = 0\n    out = False  # currently away from HOME?\n\n    trip_ids = []\n    action_orders = []\n\n    for _, row in df_day.iterrows():\n        if row[\"is_home\"] == 1:\n            # Being at HOME: mark as outside any within-trip sequence\n            out = False\n            action_order = 0\n            trip_ids.append(trip_id)\n            action_orders.append(0)\n        else:\n            # Non-home visit\n            if not out:\n                # Leaving home: start a new trip\n                trip_id += 1\n                action_order = 1\n                out = True\n            else:\n                # Continuing within the same trip\n                action_order += 1\n\n            trip_ids.append(trip_id)\n            action_orders.append(action_order)\n\n    df_day[\"trip_id\"] = trip_ids\n    df_day[\"action_order_in_trip\"] = action_orders\n    return df_day\n\nvisit_table = (\n    visit_table\n    .groupby([\"person\", \"date\"], group_keys=False)\n    .apply(add_trip_cols)\n    .reset_index(drop=True)\n)\n\nprint(\"Trip-related columns (first 20 rows):\")\ndisplay(\n    visit_table[\n        [\"person\", \"date\", \"place_id\", \"is_home\",\n         \"visit_order_in_day\", \"trip_id\", \"action_order_in_trip\",\n         \"first_time\", \"last_time\"]\n    ].head(10)\n)\n\nprint(\"Number of trips (trip_id &gt; 0):\", (visit_table[\"trip_id\"] &gt; 0).sum())\n\nTrip-related columns (first 20 rows):\n\n\n\n\n\n\n\n\n\nperson\ndate\nplace_id\nis_home\nvisit_order_in_day\ntrip_id\naction_order_in_trip\nfirst_time\nlast_time\n\n\n\n\n0\n0\n2008-10-23\nPv1\n0\n1\n1\n1\n03:00:55\n04:13:32\n\n\n1\n0\n2008-10-23\nSW\n0\n2\n1\n2\n09:42:25\n09:42:30\n\n\n2\n0\n2008-10-23\nHOME\n1\n3\n1\n0\n09:42:35\n10:05:29\n\n\n3\n0\n2008-10-23\nSW\n0\n4\n2\n1\n10:05:34\n10:30:15\n\n\n4\n0\n2008-10-23\nHOME\n1\n5\n2\n0\n10:30:20\n11:10:57\n\n\n5\n0\n2008-10-24\nSW\n0\n1\n1\n1\n02:09:59\n02:10:54\n\n\n6\n0\n2008-10-24\nHOME\n1\n2\n1\n0\n02:10:59\n02:47:06\n\n\n7\n0\n2008-10-26\nPv4\n0\n1\n1\n1\n14:04:27\n14:12:42\n\n\n8\n0\n2008-10-26\nPv5\n0\n2\n1\n2\n14:23:42\n14:35:17\n\n\n9\n0\n2008-10-27\nHOME\n1\n1\n0\n0\n12:03:59\n12:05:54\n\n\n\n\n\n\n\nNumber of trips (trip_id &gt; 0): 1805",
    "crumbs": [
      "Analysis",
      "Notebook 2 ‚Äì Trips by action"
    ]
  },
  {
    "objectID": "by action.html#trip-level-descriptive-statistics",
    "href": "by action.html#trip-level-descriptive-statistics",
    "title": "Notebook 2 ‚Äî Trip-level exploration and ‚Äúgoing home‚Äù behaviour",
    "section": "3. Trip-level descriptive statistics",
    "text": "3. Trip-level descriptive statistics\nThe visit-level data are next collapsed to the trip level. For each combination of person, date, and trip_id, a single row is created that records the number of non-home stops (n_stops), the start and end times of the trip (trip_start, trip_end), and the maximum and average distance from HOME reached during that outing (max_dist_home, mean_dist_home). Simple summaries and histograms based on this trip-level table, especially for n_stops and max_dist_home, show how long typical outings last and how far the user usually travels away from home.\n\n# Keep only visits that belong to a trip\nvt_trip = visit_table[visit_table[\"trip_id\"] &gt; 0].copy()\n\ntrip_summary = (\n    vt_trip\n    .groupby([\"person\", \"date\", \"trip_id\"], as_index=False)\n    .agg(\n        n_stops=(\"action_order_in_trip\", lambda x: (x &gt; 0).sum()),\n        trip_start=(\"first_dt\", \"min\"),\n        trip_end=(\"last_dt\", \"max\"),\n        max_dist_home=(\"dist_home_m\", \"max\"),\n        mean_dist_home=(\"dist_home_m\", \"mean\"),\n    )\n)\n\ntrip_summary[\"trip_duration_min\"] = (\n    (trip_summary[\"trip_end\"] - trip_summary[\"trip_start\"])\n    .dt.total_seconds() / 60.0\n)\n\nprint(\"Number of trips:\", len(trip_summary))\ndisplay(trip_summary.head())\n\n# --- 3.1 Histogram of stops per trip ---\nplt.figure(figsize=(6, 4))\nplt.hist(trip_summary[\"n_stops\"], bins=20)\nplt.xlabel(\"Number of non-home stops in trip\")\nplt.ylabel(\"Number of trips\")\nplt.title(f\"Stops per trip ‚Äî user {USER_ID}\")\nplt.tight_layout()\nplt.show()\n\n# --- 3.2 Histogram of max distance from HOME per trip ---\nplt.figure(figsize=(6, 4))\nplt.hist(trip_summary[\"max_dist_home\"], bins=30)\nplt.xlabel(\"Maximum distance from HOME (m)\")\nplt.ylabel(\"Number of trips\")\nplt.title(f\"Maximum distance from HOME per trip ‚Äî user {USER_ID}\")\nplt.tight_layout()\nplt.show()\n\nNumber of trips: 429\n\n\n\n\n\n\n\n\n\nperson\ndate\ntrip_id\nn_stops\ntrip_start\ntrip_end\nmax_dist_home\nmean_dist_home\ntrip_duration_min\n\n\n\n\n0\n0\n2008-10-23\n1\n2\n2008-10-23 03:00:55\n2008-10-23 10:05:29\n3201.6\n1233.866667\n424.566667\n\n\n1\n0\n2008-10-23\n2\n1\n2008-10-23 10:05:34\n2008-10-23 11:10:57\n500.0\n250.000000\n65.383333\n\n\n2\n0\n2008-10-24\n1\n1\n2008-10-24 02:09:59\n2008-10-24 02:47:06\n500.0\n250.000000\n37.116667\n\n\n3\n0\n2008-10-26\n1\n2\n2008-10-26 14:04:27\n2008-10-26 14:35:17\n13416.4\n12857.400000\n30.833333\n\n\n4\n0\n2008-10-28\n1\n20\n2008-10-28 00:38:26\n2008-10-28 02:52:11\n2549.5\n2214.280952\n133.750000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor user 000 there are 429 identified home‚Äìhome trips. The histogram of n_stops shows that most trips are very simple: the median is one non-home stop and the vast majority of outings contain only one or two stops, with a long but thin right tail of unusually complex trips (up to nearly 100 recorded stops). The distribution of max_dist_home tells a similar story on the spatial side. Most trips remain within a few kilometres of HOME, but there are occasional long-distance excursions that reach tens of kilometres away. Together, these summaries highlight a pattern of frequent short errands around home punctuated by rare, much longer journeys.",
    "crumbs": [
      "Analysis",
      "Notebook 2 ‚Äì Trips by action"
    ]
  },
  {
    "objectID": "by action.html#next-step-behaviour-within-trips",
    "href": "by action.html#next-step-behaviour-within-trips",
    "title": "Notebook 2 ‚Äî Trip-level exploration and ‚Äúgoing home‚Äù behaviour",
    "section": "4. Next-step behaviour within trips",
    "text": "4. Next-step behaviour within trips\n\n4.1 Calculate the non home stops\nThe analysis now shifts from entire home‚Äìhome trips to the individual non-home stops that occur within each trip. At each such stop, the key question becomes: what happens next? Rather than focusing on the whole sequence, we examine the immediate next step following each non-home visit.\nWithin each trip episode, three possible next-step outcomes are defined. The first is home, meaning that the next recorded stop is a return to HOME. The second is explore, in which the individual continues the trip by visiting another non-home location‚Äîthis may be a SW, Pv, or Pn place. The third is end, indicating that the trip terminates in the data and no further stop is observed.\nA special rule is applied to the last non-home stop of each trip. When this final stop is labeled end, it often reflects the end of the day rather than a genuine termination of activity away from home. To avoid treating these end-of-day records as if the person remained out indefinitely, we reclassify an end outcome as effectively going home if the stop occurs late at night (hour ‚â• 23) or is very close to HOME (within 750 m). This adjustment ensures that outcome categories are more comparable across different stops and reduces artificial inflation of ‚Äúend‚Äù events.\nAfter this reclassification, the resulting next-step outcomes are tabulated by the stop order ùëò k within each trip. This tabulation describes how the relative frequencies of ‚Äúgo home,‚Äù ‚Äúkeep exploring,‚Äù and ‚Äúend‚Äù evolve as a trip progresses, providing a clearer picture of within-trip behavioral dynamics.\n\nMAXK = 30        # only consider the first 30 stops within each trip\nNIGHT_HOUR = 23  # &gt;= 23:00 considered \"late\"\nNEAR_HOME_M = 750\n\n# Non-home visits that are part of a trip and have a positive action order\nvis = visit_table[\n    (visit_table[\"is_home\"] == 0) &\n    (visit_table[\"trip_id\"] &gt; 0) &\n    (visit_table[\"action_order_in_trip\"] &gt; 0)\n].copy()\n\nvis = vis[vis[\"action_order_in_trip\"] &lt;= MAXK].copy()\n\nprint(\"Non-home visit events inside trips:\", len(vis))\nprint(\"\\nRaw next_step value counts (non-home):\")\nprint(vis[\"next_step\"].value_counts())\n\n# --- Classify next-step outcomes ---\n\nvis[\"outcome\"] = np.where(\n    vis[\"next_step\"] == \"home\", \"home\",\n    np.where(vis[\"next_step\"].isin([\"sw\", \"pv\", \"pn\"]), \"explore\", \"end\")\n)\n\n# Identify the last non-home visit of each trip\nlast_mask = vis.groupby(\n    [\"person\", \"date\", \"trip_id\"]\n).cumcount(ascending=False).eq(0)\n\n# Late or near home?\nlast_time_dt = pd.to_datetime(vis[\"last_time\"], format=\"%H:%M:%S\", errors=\"coerce\")\nlate = last_time_dt.dt.hour.ge(NIGHT_HOUR)\nnear = vis[\"dist_home_m\"].le(NEAR_HOME_M)\n\n# Reclassify terminal 'end' as 'home' if late or near HOME\nvis.loc[\n    last_mask & (vis[\"outcome\"] == \"end\") & (late | near),\n    \"outcome\"\n] = \"home\"\n\nprint(\"\\nOutcome counts after reclassification:\")\nprint(vis[\"outcome\"].value_counts())\n\n# --- Outcome proportions by stop order k in trip ---\n\ncounts = (\n    vis.groupby(\"action_order_in_trip\")[\"outcome\"]\n       .value_counts()\n       .unstack(fill_value=0)\n       .reindex(range(1, MAXK + 1), fill_value=0)\n)\ncounts[\"total\"] = counts.sum(axis=1)\n\nprops = counts.div(\n    counts[\"total\"].where(counts[\"total\"] &gt; 0, np.nan),\n    axis=0\n)\n\nprint(\"\\nOutcome composition (first 10 k):\")\ndisplay(props[[\"explore\", \"home\", \"end\"]].head(10))\n\nNon-home visit events inside trips: 1306\n\nRaw next_step value counts (non-home):\nnext_step\npn      632\nhome    338\npv      219\nnone     85\nsw       32\nName: count, dtype: int64\n\nOutcome counts after reclassification:\noutcome\nexplore    883\nhome       403\nend         20\nName: count, dtype: int64\n\nOutcome composition (first 10 k):\n\n\n\n\n\n\n\n\noutcome\nexplore\nhome\nend\n\n\naction_order_in_trip\n\n\n\n\n\n\n\n1\n0.333333\n0.657343\n0.009324\n\n\n2\n0.874126\n0.090909\n0.034965\n\n\n3\n0.648000\n0.328000\n0.024000\n\n\n4\n0.901235\n0.086420\n0.012346\n\n\n5\n0.684932\n0.301370\n0.013699\n\n\n6\n0.880000\n0.100000\n0.020000\n\n\n7\n0.840909\n0.113636\n0.045455\n\n\n8\n0.918919\n0.081081\n0.000000\n\n\n9\n0.911765\n0.088235\n0.000000\n\n\n10\n0.935484\n0.064516\n0.000000\n\n\n\n\n\n\n\nFor user 000 there are 1,306 non-home stops inside trips, of which 883 are followed by another non-home place (explore), 403 by a return home, and only 20 end the trip without a further visit after the late/near-home reclassification. The composition by stop order (k) shows a clear pattern: at the first stop, going home is very common (around two thirds of trips return home immediately and only one third continue to explore). Once a trip survives this first stop, however, exploration dominates: for (k = 2) and beyond, 65‚Äì90% of stops are followed by another non-home place and the hazard of going home falls to around 10‚Äì30%. In other words, many trips are short one-stop errands, but conditional on not going straight home, the user tends to string together several exploratory stops before ending the outing.\n\n\n4.2 Discrete-time hazard of going home\nWithin each trip, non-home stops are treated as discrete time steps: the first stop after leaving HOME, the second stop, the third stop, and so on. At each step, the analysis focuses on what happens next conditional on the trip having reached that point. For every step number (k), the data across all trips are pooled to compute two quantities: the total number of trips that have reached at least the (k)-th non-home stop, and, among those, the number of trips that return directly to HOME immediately after stop (k).\nThe ratio of these two counts defines the hazard of going home at step (k),\nh_k = P(go home next | trip has reached stop k),\nwhich answers: given that this trip has already made it to stop (k), what is the probability that the next move is to go home rather than continue exploring? This discrete-time hazard summarizes how the immediate ‚Äúgo home now‚Äù tendency changes as the trip accumulates more non-home stops.\nBased on the hazards, the analysis also derives the survival probability (S_k), defined as the probability that a trip is still ongoing after (k) non-home stops. In discrete time, this is obtained by multiplying (1 - h_k)¬†across successive steps, as the chance that the individual has not yet decided to return home. Intuitively, (S_k) represents the probability that the person is ‚Äústill out‚Äù after making (k) stops away from home.\nThe first panel visualizes the empirical hazards (h_k) together with a simple logit trend in (k) fitted for (k ), showing how the propensity to go home varies with stop order. The second panel compares the empirical survival curve (S_k) with the corresponding model-based survival curve implied by the fitted hazard specification. Together, these plots illustrate how both the ‚Äúgo home now‚Äù probability and the ‚Äústill out‚Äù probability evolve as trips become longer.\n\n# Per-trip maximum stop order (within the MAXK truncation)\npertrip_maxk = (\n    vis.groupby([\"person\", \"date\", \"trip_id\"])[\"action_order_in_trip\"]\n       .max()\n)\n\n# Risk set: number of trips that reach at least stop k\nat_risk = pd.Series(\n    {k: int((pertrip_maxk &gt;= k).sum()) for k in range(1, MAXK + 1)},\n    name=\"at_risk\"\n)\n\n# Number of trips where the k-th stop is followed by \"home\"\nhome_k = (\n    vis[vis[\"outcome\"] == \"home\"]\n    .groupby(\"action_order_in_trip\")\n    .size()\n    .reindex(range(1, MAXK + 1), fill_value=0)\n    .rename(\"home_k\")\n)\n\nhazard = (home_k / at_risk.replace(0, np.nan)).rename(\"hazard\")\n\nhaz_df = pd.concat([home_k, at_risk, hazard], axis=1)\nprint(\"Hazard table (first 10 k):\")\ndisplay(haz_df.head(10))\n\n# ---------- Empirical survival S_k ----------\nk_axis = np.arange(1, MAXK + 1)\nhaz_vals_emp = hazard.reindex(k_axis).fillna(0).values\n\nS_emp = []\ns = 1.0\nfor hk in haz_vals_emp:\n    s *= (1 - hk)\n    S_emp.append(s)\nS_emp = np.array(S_emp)\n\n# ---------- Simple logit trend for hazard (k&gt;=2) ----------\nimport statsmodels.api as sm\n\nMIN_RISK = 20\n\nhaz_fit = (\n    haz_df\n    .loc[haz_df.index &gt;= 2] \n    .loc[lambda df: df[\"at_risk\"] &gt;= MIN_RISK]\n    .copy()\n)\n\nprint(\"Ks used in hazard fit:\", haz_fit.index.tolist())\n\nk_fit = haz_fit.index.values.astype(float)\nX = sm.add_constant(k_fit)\n\ny = np.column_stack([\n    haz_fit[\"home_k\"].values,\n    (haz_fit[\"at_risk\"] - haz_fit[\"home_k\"]).values\n])\n\nglm = sm.GLM(y, X, family=sm.families.Binomial())\nres_haz = glm.fit()\nprint(res_haz.summary2().tables[1])\n\nalpha = res_haz.params[0]\nbeta  = res_haz.params[1]\n\nX_pred = sm.add_constant(k_axis[1:])\nhaz_hat_tail = res_haz.predict(X_pred)\n\nh1_emp = haz_df.loc[1, \"hazard\"]\nhaz_hat = np.concatenate([[h1_emp], haz_hat_tail])\n\nS_hat = []\ns = 1.0\nfor hk in haz_hat:\n    s *= (1 - hk)\n    S_hat.append(s)\nS_hat = np.array(S_hat)\n\n# ---------- Plot hazard: empirical vs fitted ----------\nplt.figure(figsize=(10, 4))\nplt.plot(k_axis, haz_vals_emp, marker=\"o\", label=\"Empirical hazard\")\nplt.plot(k_axis, haz_hat, linestyle=\"--\", label=\"Logit trend (k‚â•2)\")\nplt.ylim(0, 1)\nplt.xlabel(\"Stop order k in trip\")\nplt.ylabel(\"P(go home | trip has reached k)\")\nplt.title(f\"Hazard of going home after k-th stop in a trip ‚Äî user {USER_ID}\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# ---------- Plot survival: empirical vs fitted ----------\nplt.figure(figsize=(10, 4))\nplt.plot(k_axis, S_emp, marker=\"o\", label=\"Empirical S_k\")\nplt.plot(k_axis, S_hat, linestyle=\"--\", label=\"Model-based S_k\")\nplt.ylim(0, 1)\nplt.xlabel(\"Stop order k in trip\")\nplt.ylabel(\"P(still out after k)\")\nplt.title(f\"Survival of staying out within a trip ‚Äî user {USER_ID}\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nHazard table (first 10 k):\n\n\n\n\n\n\n\n\n\nhome_k\nat_risk\nhazard\n\n\n\n\n1\n282\n429\n0.657343\n\n\n2\n13\n143\n0.090909\n\n\n3\n41\n125\n0.328000\n\n\n4\n7\n81\n0.086420\n\n\n5\n22\n73\n0.301370\n\n\n6\n5\n50\n0.100000\n\n\n7\n5\n44\n0.113636\n\n\n8\n3\n37\n0.081081\n\n\n9\n3\n34\n0.088235\n\n\n10\n2\n31\n0.064516\n\n\n\n\n\n\n\nKs used in hazard fit: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n          Coef.  Std.Err.         z         P&gt;|z|    [0.025   0.975]\nconst -1.217758  0.194916 -6.247605  4.167954e-10 -1.599786 -0.83573\nx1    -0.092337  0.033081 -2.791279  5.250023e-03 -0.157174 -0.02750\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3 Within-Trip Exploration: When Do New Places (Pv) Appear?\nThe analysis now moves from going-home decisions to exploration within a trip. For every non-home stop, we know whether the location is a first-time place for the user (Pv) or a return to an already visited place (Pn).\nTo study how exploration changes as a trip unfolds, the stops are ordered by their position within the trip. For each stop number (k), the share of stops that are Pv is calculated. This produces an empirical curve showing how often new places appear at different stages of a trip.\nFor user 000, about 18% of all non-home stops across trips are first-time places. The exploration rate is relatively low in the first few stops but gradually increases as the trip continues, reaching roughly 30% around the 8th to 10th stop. A simple logit regression, applied only to stop numbers with enough observations, provides the smooth trend line shown in the figure. The estimated effect suggests that each additional stop in a trip increases the odds of discovering a new place by around one quarter.\n\n# Non-home visits inside trips (same base as for hazard)\nvis_places = visit_table[\n    (visit_table[\"is_home\"] == 0) &\n    (visit_table[\"trip_id\"] &gt; 0) &\n    (visit_table[\"action_order_in_trip\"] &gt; 0)\n].copy()\n\nvis_places = vis_places[vis_places[\"action_order_in_trip\"] &lt;= MAXK].copy()\n\n# Counts by stop order: how often is the stop Pv vs Pn?\npv_by_k = (\n    vis_places\n    .groupby(\"action_order_in_trip\")[[\"is_pv\", \"is_pn\"]]\n    .agg(\n        pv_cnt=(\"is_pv\", \"sum\"),\n        pn_cnt=(\"is_pn\", \"sum\"),\n        n=(\"is_pv\", \"size\")\n    )\n    .reset_index()\n)\n\npv_by_k[\"pv_rate\"] = pv_by_k[\"pv_cnt\"] / (\n    pv_by_k[\"pv_cnt\"] + pv_by_k[\"pn_cnt\"]\n).replace(0, np.nan)\n\noverall_pv = vis_places[\"is_pv\"].mean()\n\nprint(\"Overall share of first-time places (Pv) among non-home trip stops:\",\n      round(overall_pv, 3))\nprint(\"\\nPv rate by stop order k (first 10 k):\")\ndisplay(pv_by_k[[\"action_order_in_trip\", \"pv_rate\"]].head(10))\n\nOverall share of first-time places (Pv) among non-home trip stops: 0.181\n\nPv rate by stop order k (first 10 k):\n\n\n\n\n\n\n\n\n\naction_order_in_trip\npv_rate\n\n\n\n\n0\n1\n0.097561\n\n\n1\n2\n0.153285\n\n\n2\n3\n0.126126\n\n\n3\n4\n0.164557\n\n\n4\n5\n0.144928\n\n\n5\n6\n0.265306\n\n\n6\n7\n0.279070\n\n\n7\n8\n0.297297\n\n\n8\n9\n0.272727\n\n\n9\n10\n0.322581\n\n\n\n\n\n\n\n\npv_by_k = pv_by_k.copy() \n\nMIN_N = 30\nvalid_k = pv_by_k.loc[pv_by_k[\"n\"] &gt;= MIN_N, \"action_order_in_trip\"]\nprint(\"Ks with n &gt;= 30:\", valid_k.tolist())\n\nvis_lr_sub = vis_places[\n    vis_places[\"action_order_in_trip\"].isin(valid_k)\n].copy()\n\nvis_lr_sub[\"k_centered\"] = (\n    vis_lr_sub[\"action_order_in_trip\"]\n    - vis_lr_sub[\"action_order_in_trip\"].mean()\n)\n\nX = sm.add_constant(vis_lr_sub[\"k_centered\"])\ny = vis_lr_sub[\"is_pv\"]\n\nlogit_model = sm.Logit(y, X)\nres = logit_model.fit(disp=False)\nprint(res.summary2().tables[1])\n\nbeta0 = res.params[\"const\"]\nbeta1 = res.params[\"k_centered\"]\n\nodds_ratio = np.exp(beta1)\nprint(f\"Odds ratio for Pv per additional stop (n&gt;={MIN_N}): {odds_ratio:.3f}\")\n\nk_max = int(valid_k.max())\nk_grid = np.arange(1, k_max + 1)\nk_grid_c = k_grid - vis_lr_sub[\"action_order_in_trip\"].mean()\nlin_pred = beta0 + beta1 * k_grid_c\np_hat = 1 / (1 + np.exp(-lin_pred))\n\npv_plot = pv_by_k[pv_by_k[\"action_order_in_trip\"] &lt;= k_max]\n\nplt.figure(figsize=(8,4))\nplt.plot(\n    pv_plot[\"action_order_in_trip\"],\n    pv_plot[\"pv_rate\"],\n    marker=\"o\",\n    label=\"Empirical Pv share\"\n)\nplt.plot(\n    k_grid,\n    p_hat,\n    linestyle=\"--\",\n    label=f\"Logit trend (n‚â•{MIN_N})\"\n)\nplt.axhline(\n    overall_pv,\n    color=\"gray\",\n    linestyle=\"--\",\n    linewidth=1,\n    label=\"Overall Pv share\"\n)\nplt.ylim(0, 0.5)\nplt.xlim(1, k_max)\nplt.xlabel(\"Stop order k in trip (non-home)\")\nplt.ylabel(\"P(stop is Pv | stop at k)\")\nplt.title(f\"Within-trip Pv probability ‚Äî user {USER_ID}\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nKs with n &gt;= 30: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n               Coef.  Std.Err.          z         P&gt;|z|    [0.025    0.975]\nconst      -2.074610  0.103630 -20.019307  3.738832e-89 -2.277722 -1.871498\nk_centered  0.236422  0.031935   7.403330  1.328116e-13  0.173831  0.299013\nOdds ratio for Pv per additional stop (n&gt;=30): 1.267\n\n\n\n\n\n\n\n\n\n\n\n4.4 Behavioural summary\n\nEveryday mobility is dominated by short, nearby home‚Äìhome trips. Most outings contain only a few non-home stops, and the typical activity radius is tightly clustered around HOME; long-distance excursions show up as rare outliers.\nWithin a single trip, there is a large mass of ‚Äúone-stop errands‚Äù that return straight home, producing a very high probability of going home after the first stop. Conditional on surviving this initial stage, the per-stop hazard of returning home declines with stop order: once a trip has been running for a while, the person becomes increasingly ‚Äústicky‚Äù to staying out.\nAlong the same trip, early stops are mostly revisits to familiar locations, whereas the share of first-time places (Pv) rises steadily with stop order. Exploration is therefore concentrated in the middle and later parts of a trip, once the outing has ‚Äúwarmed up‚Äù.\n\nTaken together, these patterns can be summarised as follows: most days involve short errands close to home; when a longer outing does occur, the person tends to stay out for a while and becomes progressively more likely to explore new places.",
    "crumbs": [
      "Analysis",
      "Notebook 2 ‚Äì Trips by action"
    ]
  },
  {
    "objectID": "Data Cleaning.html",
    "href": "Data Cleaning.html",
    "title": "Notebook 1 ‚Äî From raw Geolife trajectories to visit-level events",
    "section": "",
    "text": "This notebook shows how raw Geolife GPS trajectories for one user are turned into a cleaned visit-level dataset. I load and filter the original .plt files, restrict the data to Beijing, and project all points onto a 500 m grid. Night-time points are used to infer the user‚Äôs home (HOME) and a secondary frequently visited place (SW), and a simple movement-based rule selects additional grid cells as daily activity locations. Consecutive points within the same activity cell are then collapsed into single visit events and labelled as HOME, SW, first-time place (Pv), or return place (Pn). The resulting visit-level table is the starting point for all subsequent analyses.",
    "crumbs": [
      "Analysis",
      "Notebook 1 ‚Äì Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#environment-and-data-paths",
    "href": "Data Cleaning.html#environment-and-data-paths",
    "title": "Notebook 1 ‚Äî From raw Geolife trajectories to visit-level events",
    "section": "1. Environment and data paths",
    "text": "1. Environment and data paths\nThe Geolife data folder Geolife Trajectories 1.3 is stored in the same directory as this notebook. Below I set the base directory and import the Python packages used throughout the analysis.\n\nimport os\nimport glob\nimport math\n\nimport numpy as np\nimport pandas as pd\nimport pyproj\n\n# Path to the Geolife data (relative to the notebook)\nBASE_DIR = os.path.join(\"Geolife Trajectories 1.3\", \"Data\")\n\n# Example user (we start with user 000)\nUSER_ID = \"000\"\n\n# Optional: restrict to Beijing area\nFILTER_BEIJING = True\nBEIJING_BBOX = (115.42, 39.44, 117.50, 41.06)   # (minLon, minLat, maxLon, maxLat)\n\n# Column names for Geolife .plt files (after skipping the 6-line header)\nPLT_COLS = [\"lat\", \"lon\", \"unused\", \"altitude_feet\", \"days\", \"date\", \"time\"]",
    "crumbs": [
      "Analysis",
      "Notebook 1 ‚Äì Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#inspecting-raw-gps-trajectories",
    "href": "Data Cleaning.html#inspecting-raw-gps-trajectories",
    "title": "Notebook 1 ‚Äî From raw Geolife trajectories to visit-level events",
    "section": "2. Inspecting raw GPS trajectories",
    "text": "2. Inspecting raw GPS trajectories\nBefore any spatial filtering or grid construction, the raw Geolife records for user 000 are inspected. Table 1 previews the cleaned point-level data: each row corresponds to a single GPS fix with a timestamp, derived calendar date and hour, latitude/longitude, and altitude.\nThe spatial footprint of these records is visualised by plotting the raw trajectories as daily polylines in geographic coordinates (Fig. 2). Connecting consecutive points into lines highlights dense clusters of movement as well as the long-distance corridor between Beijing and Shanghai, and motivates the later projection to UTM and discretisation onto a 500 m grid.\nTemporal coverage is assessed by aggregating the data by date and counting the number of recorded GPS points per day (Fig. 3). The resulting time series clearly shows highly irregular tracking: some days contain several thousand points, others only a few, and there are extended gaps with no data at all. This pattern reflects the device being switched on only intermittently and at changing sampling rates, a key limitation that subsequent behavioural analyses must be interpreted conditional on.\n\ndef read_one_plt(path, user_id=\"000\"):\n    \"\"\"Read a single Geolife .plt file, clean basic issues, and return a DataFrame.\"\"\"\n    df = pd.read_csv(path, skiprows=6, names=PLT_COLS)\n\n    # Basic cleaning: drop rows with missing coords or time\n    df = df[\n        pd.notnull(df[\"lat\"]) &\n        pd.notnull(df[\"lon\"]) &\n        pd.notnull(df[\"date\"]) &\n        pd.notnull(df[\"time\"])\n    ].copy()\n\n    # Build timestamp\n    df[\"datetime\"] = pd.to_datetime(\n        df[\"date\"].astype(str) + \" \" + df[\"time\"].astype(str),\n        errors=\"coerce\"\n    )\n    df = df[pd.notnull(df[\"datetime\"])].copy()\n\n    # Derive date and hour\n    df[\"user\"] = user_id\n    df[\"file\"] = os.path.basename(path)\n    df[\"date_only\"] = df[\"datetime\"].dt.date\n    df[\"hour\"] = df[\"datetime\"].dt.hour\n\n    # Keep core columns\n    df = df[[\"user\", \"file\", \"datetime\", \"date_only\", \"hour\",\n             \"lat\", \"lon\", \"altitude_feet\"]]\n    return df\n\n\n# Scan all trajectory files for the chosen user\ntraj_glob = os.path.join(BASE_DIR, USER_ID, \"Trajectory\", \"*.plt\")\nfiles = sorted(glob.glob(traj_glob))\n\n\ndfs = []\nfor fp in files:\n    try:\n        dfs.append(read_one_plt(fp, user_id=USER_ID))\n    except Exception as e:\n        print(f\"[warning] failed to read {fp}: {e}\")\n\nif not dfs:\n    raise RuntimeError(\"No trajectories were successfully read. Check the data path.\")\n\ntraj = pd.concat(dfs, ignore_index=True)\ntraj.sort_values(\"datetime\", inplace=True)\ntraj.reset_index(drop=True, inplace=True)\ntraj.head(10)\n\n\n\n\n\n\n\n\nuser\nfile\ndatetime\ndate_only\nhour\nlat\nlon\naltitude_feet\n\n\n\n\n0\n000\n20081023025304.plt\n2008-10-23 02:53:04\n2008-10-23\n2\n39.984702\n116.318417\n492\n\n\n1\n000\n20081023025304.plt\n2008-10-23 02:53:10\n2008-10-23\n2\n39.984683\n116.318450\n492\n\n\n2\n000\n20081023025304.plt\n2008-10-23 02:53:15\n2008-10-23\n2\n39.984686\n116.318417\n492\n\n\n3\n000\n20081023025304.plt\n2008-10-23 02:53:20\n2008-10-23\n2\n39.984688\n116.318385\n492\n\n\n4\n000\n20081023025304.plt\n2008-10-23 02:53:25\n2008-10-23\n2\n39.984655\n116.318263\n492\n\n\n5\n000\n20081023025304.plt\n2008-10-23 02:53:30\n2008-10-23\n2\n39.984611\n116.318026\n493\n\n\n6\n000\n20081023025304.plt\n2008-10-23 02:53:35\n2008-10-23\n2\n39.984608\n116.317761\n493\n\n\n7\n000\n20081023025304.plt\n2008-10-23 02:53:40\n2008-10-23\n2\n39.984563\n116.317517\n496\n\n\n8\n000\n20081023025304.plt\n2008-10-23 02:53:45\n2008-10-23\n2\n39.984539\n116.317294\n500\n\n\n9\n000\n20081023025304.plt\n2008-10-23 02:53:50\n2008-10-23\n2\n39.984606\n116.317065\n505\n\n\n\n\n\n\n\n\nimport folium\ntraj_for_map = traj.copy()\n\n# Center the map at the median lat/lon of all points\ncenter_lat = traj_for_map[\"lat\"].median()\ncenter_lon = traj_for_map[\"lon\"].median()\n\nm_raw = folium.Map(\n    location=[center_lat, center_lon],\n    zoom_start=8,\n    tiles=\"cartodbpositron\"\n)\n\n# Draw one polyline per day to show daily trajectories\nfor d, sub in traj_for_map.groupby(\"date_only\"):\n    sub = sub.sort_values(\"datetime\")\n    if len(sub) &lt; 2:\n        continue\n\n    coords = list(zip(sub[\"lat\"].values, sub[\"lon\"].values))  # (lat, lon) pairs\n    folium.PolyLine(\n        locations=coords,\n        weight=1,\n        opacity=0.4\n    ).add_to(m_raw)\n\nm_raw\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n# Daily number of GPS points\nday_counts = (\n    traj\n    .groupby(\"date_only\")\n    .size()\n    .rename(\"n_points\")\n    .reset_index()\n)\n\n# Simple time-series style plot of counts per day\nday_counts_plot = day_counts.copy()\nday_counts_plot[\"date_only\"] = pd.to_datetime(day_counts_plot[\"date_only\"])\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(7, 3.5))\nplt.plot(\n    day_counts_plot[\"date_only\"],\n    day_counts_plot[\"n_points\"],\n    marker=\"o\",\n    linewidth=1\n)\nplt.xlabel(\"Date\")\nplt.ylabel(\"Number of GPS points\")\nplt.title(\"Daily number of recorded GPS points for user 000\")\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Analysis",
      "Notebook 1 ‚Äì Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#spatial-filter-restricting-to-the-beijing-area",
    "href": "Data Cleaning.html#spatial-filter-restricting-to-the-beijing-area",
    "title": "Notebook 1 ‚Äî From raw Geolife trajectories to visit-level events",
    "section": "3. Spatial filter: restricting to the Beijing area",
    "text": "3. Spatial filter: restricting to the Beijing area\nGeolife users may have trajectories in multiple cities. In this project we focus on the Beijing region. We therefore apply a simple bounding box filter on longitude and latitude.\n\nif FILTER_BEIJING:\n    minLon, minLat, maxLon, maxLat = BEIJING_BBOX\n    before = len(traj)\n    traj = traj[\n        (traj[\"lon\"] &gt;= minLon) & (traj[\"lon\"] &lt;= maxLon) &\n        (traj[\"lat\"] &gt;= minLat) & (traj[\"lat\"] &lt;= maxLat)\n    ].copy()\n    after = len(traj)\n    print(f\"Beijing filter: {before} -&gt; {after} points\")\nelse:\n    print(\"Beijing filter is disabled.\")\n\nBeijing filter: 173870 -&gt; 157646 points",
    "crumbs": [
      "Analysis",
      "Notebook 1 ‚Äì Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#projection-to-utm-and-construction-of-a-500-m-grid",
    "href": "Data Cleaning.html#projection-to-utm-and-construction-of-a-500-m-grid",
    "title": "Notebook 1 ‚Äî From raw Geolife trajectories to visit-level events",
    "section": "4. Projection to UTM and construction of a 500 m grid",
    "text": "4. Projection to UTM and construction of a 500 m grid\nAll GPS points are transformed from WGS84 geographic coordinates (lat/lon) to UTM Zone 50N so that distances can be measured in metres. In this projected space, a regular 500 m √ó 500 m grid is built to fully cover the user‚Äôs trajectory. Each GPS fix is then mapped to its corresponding grid cell (ci, rj).\nThis discretisation step reduces the influence of GPS jitter and avoids tracing every minor wiggle of the raw trajectories. Subsequent analyses operate at the cell level‚Äîusing visits to grid cells instead of raw points‚Äîwhich stabilises behaviour patterns and prevents over-interpreting micro-movements that arise from measurement error or inconsistent sampling.\nUsing a fixed grid inevitably introduces a mild modifiable areal unit problem (MAUP): the exact boundaries and grid size may slightly influence which cells a point falls into. In this context, however, the 500 m resolution strikes a balance between reducing noise and retaining meaningful activity locations, making the grid a practical spatial unit for behavioural modelling.\n\ndef cell_center_lonlat(ci, rj):\n    cx = gx0 + (ci + 0.5) * GRID_SIZE\n    cy = gy0 + (rj + 0.5) * GRID_SIZE\n    lon, lat = to_wgs(cx, cy)\n    return lon, lat\n\n# Coordinate reference systems\nCRS_WGS84 = pyproj.CRS(\"EPSG:4326\")\nCRS_UTM50 = pyproj.CRS(\"EPSG:32650\")\nto_utm = pyproj.Transformer.from_crs(CRS_WGS84, CRS_UTM50, always_xy=True).transform\nto_wgs = pyproj.Transformer.from_crs(CRS_UTM50, CRS_WGS84, always_xy=True).transform\n\n# Lat/lon -&gt; projected (metres)\nx, y = to_utm(traj[\"lon\"].values, traj[\"lat\"].values)\ntraj[\"x\"] = x\ntraj[\"y\"] = y\n\nGRID_SIZE = 500  # metres\n\n# Grid extent with one extra cell of padding on each side\nminx, miny = traj[\"x\"].min(), traj[\"y\"].min()\nmaxx, maxy = traj[\"x\"].max(), traj[\"y\"].max()\n\ngx0 = math.floor(minx / GRID_SIZE) * GRID_SIZE - GRID_SIZE\ngy0 = math.floor(miny / GRID_SIZE) * GRID_SIZE - GRID_SIZE\ngx1 = math.ceil (maxx / GRID_SIZE) * GRID_SIZE + GRID_SIZE\ngy1 = math.ceil (maxy / GRID_SIZE) * GRID_SIZE + GRID_SIZE\n\nncol = int((gx1 - gx0) / GRID_SIZE)\nnrow = int((gy1 - gy0) / GRID_SIZE)\nprint(f\"Grid columns √ó rows: {ncol} √ó {nrow} (total {ncol*nrow:,} cells)\")\n\n# Assign each point to a grid cell\ntraj[\"ci\"] = ((traj[\"x\"] - gx0) // GRID_SIZE).astype(int)\ntraj[\"rj\"] = ((traj[\"y\"] - gy0) // GRID_SIZE).astype(int)\n\ntraj[[\"datetime\", \"lat\", \"lon\", \"x\", \"y\", \"ci\", \"rj\"]].head()\n\nGrid columns √ó rows: 128 √ó 102 (total 13,056 cells)\n\n\n\n\n\n\n\n\n\ndatetime\nlat\nlon\nx\ny\nci\nrj\n\n\n\n\n0\n2008-10-23 02:53:04\n39.984702\n116.318417\n441807.056623\n4.426282e+06\n43\n50\n\n\n1\n2008-10-23 02:53:10\n39.984683\n116.318450\n441809.858037\n4.426280e+06\n43\n50\n\n\n2\n2008-10-23 02:53:15\n39.984686\n116.318417\n441807.043048\n4.426280e+06\n43\n50\n\n\n3\n2008-10-23 02:53:20\n39.984688\n116.318385\n441804.312590\n4.426280e+06\n43\n50\n\n\n4\n2008-10-23 02:53:25\n39.984655\n116.318263\n441793.868245\n4.426277e+06\n43\n50",
    "crumbs": [
      "Analysis",
      "Notebook 1 ‚Äì Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#detecting-home-and-sw-from-night-time-points",
    "href": "Data Cleaning.html#detecting-home-and-sw-from-night-time-points",
    "title": "Notebook 1 ‚Äî From raw Geolife trajectories to visit-level events",
    "section": "5. Detecting HOME and SW from night-time points",
    "text": "5. Detecting HOME and SW from night-time points\nI identify the user‚Äôs home location (HOME) and a secondary frequent place (SW) from night-time points between 00:00 and 06:00, assuming that locations where the user repeatedly appears at night are likely to be home-like places. In practice, I first filter the trajectory to these hours and count how many night-time points fall in each 500 m grid cell, also recording the time span between the first and last night-time observation in that cell. HOME is defined as the cell with the largest night-time count, breaking ties by the longest time span. After removing this cell, SW is defined as the second-strongest night-time cell, if such a candidate exists.\n\nnight = traj[(traj[\"hour\"] &gt;= 0) & (traj[\"hour\"] &lt; 6)].copy()\nnight[\"cell\"] = list(zip(night[\"ci\"], night[\"rj\"]))\nprint(\"Number of night-time points:\", len(night))\n\ncell_counts = night[\"cell\"].value_counts()\ncell_counts.head()\n\nNumber of night-time points: 47296\n\n\ncell\n(44, 55)    4779\n(43, 55)    3531\n(44, 54)    3157\n(45, 53)    2547\n(43, 56)    2354\nName: count, dtype: int64\n\n\n\ndef night_span_seconds(cell):\n    sub = night[night[\"cell\"] == cell][\"datetime\"]\n    return (sub.max() - sub.min()).total_seconds() if not sub.empty else 0\n\n# HOME: cell with the largest night-time count; if tied, pick the one with largest time span\ncandidates = cell_counts[cell_counts == cell_counts.max()].index.tolist()\nhome_cell = max(candidates, key=night_span_seconds) if len(candidates) &gt; 1 else cell_counts.index[0]\n\n# SW: second-strongest night-time cell after removing HOME\ncell_counts_wo_home = cell_counts[cell_counts.index != home_cell]\nsw_cell = None\nif not cell_counts_wo_home.empty:\n    cand2 = cell_counts_wo_home[cell_counts_wo_home == cell_counts_wo_home.max()].index.tolist()\n    sw_cell = max(cand2, key=night_span_seconds) if len(cand2) &gt; 1 else cell_counts_wo_home.index[0]\n\nhome_lon, home_lat = cell_center_lonlat(*home_cell)\nprint(f\"HOME cell = {home_cell} @ ({home_lon:.6f}, {home_lat:.6f})\")\n\nif sw_cell is not None:\n    sw_lon, sw_lat = cell_center_lonlat(*sw_cell)\n    print(f\"SW   cell = {sw_cell} @ ({sw_lon:.6f}, {sw_lat:.6f})\")\nelse:\n    print(\"SW   cell = None\")\n\nHOME cell = (44, 55) @ (116.323385, 40.006970)\nSW   cell = (43, 55) @ (116.317527, 40.006935)",
    "crumbs": [
      "Analysis",
      "Notebook 1 ‚Äì Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#daily-activity-cells-via-linegrid-intersections",
    "href": "Data Cleaning.html#daily-activity-cells-via-linegrid-intersections",
    "title": "Notebook 1 ‚Äî From raw Geolife trajectories to visit-level events",
    "section": "6. Daily activity cells via line‚Äìgrid intersections",
    "text": "6. Daily activity cells via line‚Äìgrid intersections\nNot every grid cell that contains a raw GPS point should be treated as an ‚Äúactivity location‚Äù. Points along fast road segments are typically in transit rather than places where the user actually stops. To obtain a more conservative set of daily activity cells, the analysis works with line segments rather than points: for each day, consecutive projected points are connected into segments in the (x, y) plane, and for every grid cell the number of segments intersecting its polygon is counted.\nA cell is classified as an activity location if its number of intersecting segments exceeds a daily threshold. The baseline threshold is set to 100 crossings, which is high enough to filter out most purely in-transit cells while retaining the dense clusters around home and other frequently visited areas. This value is chosen empirically by inspecting several days of data: smaller thresholds admit long stretches of road as ‚Äúplaces‚Äù, whereas larger thresholds begin to discard plausible stops. The threshold is then adapted on a day-by-day basis to avoid unrealistically rich days; if more than 30 cells exceed the threshold on a given day, the threshold is increased and the classification is recomputed.\n\nfrom shapely.geometry import LineString, box\nfrom shapely.strtree import STRtree\n\n# Parameters for the line‚Äìgrid crossing rule\nTHRESH_START = 100          # initial threshold for \"enough crossings\"\nTHRESH_STEP  = 25          # how much to increase the threshold if a day has too many cells\nTHRESH_MAX   = 1000        # upper bound on the threshold\n\nMAX_PLACES_PER_DAY = 30    # per day: at most this many DISTINCT activity cells (places)\n\n# Cache for grid-cell polygons (speeds things up)\n_poly_cache = {}\n\ndef cell_poly(ci, rj):\n    \"\"\"Return the shapely Polygon for grid cell (ci, rj).\"\"\"\n    key = (ci, rj)\n    if key not in _poly_cache:\n        x0 = gx0 + ci * GRID_SIZE\n        y0 = gy0 + rj * GRID_SIZE\n        _poly_cache[key] = box(x0, y0, x0 + GRID_SIZE, y0 + GRID_SIZE)\n    return _poly_cache[key]\n\ndef query_segments(tree, poly, segs):\n    \"\"\"\n    Wrapper around STRtree.query to handle both 'index' and 'geometry' return types,\n    depending on Shapely version.\n    \"\"\"\n    res = tree.query(poly)\n    if len(res) == 0:\n        return []\n    first = res[0]\n    # Some Shapely versions return indices, some return geometries\n    if isinstance(first, (int, np.integer)):\n        return [segs[i] for i in res]\n    return res\n\n# Containers for results\nvisited_cells_by_day = {}   # date -&gt; set of (ci, rj) cells considered \"activity locations\"\ncross_threshold_used = {}   # date -&gt; threshold value actually used for that day\n\n# We iterate over each date present in the trajectory\ndates = sorted(traj[\"date_only\"].unique().tolist())\nprint(f\"Number of days with data for user {USER_ID}: {len(dates)}\")\n\nfor d in dates:\n    # All points for this day, ordered in time\n    day = traj[traj[\"date_only\"] == d].sort_values(\"datetime\").copy()\n    if len(day) &lt; 2:\n        # Not enough points to form line segments\n        visited_cells_by_day[d] = set()\n        cross_threshold_used[d] = THRESH_START\n        continue\n\n    # Build line segments between consecutive points (in projected coordinates)\n    pts = list(zip(day[\"x\"].values, day[\"y\"].values))\n    segs = [LineString([pts[i], pts[i+1]]) for i in range(len(pts) - 1)]\n\n    # Drop empty / zero-length segments\n    segs = [s for s in segs if (not s.is_empty) and (s.length &gt; 0)]\n    if not segs:\n        visited_cells_by_day[d] = set()\n        cross_threshold_used[d] = THRESH_START\n        continue\n\n    tree = STRtree(segs)\n\n    # Only check the grid cells that actually appear for this day\n    cmin, cmax = int(day[\"ci\"].min()), int(day[\"ci\"].max())\n    rmin, rmax = int(day[\"rj\"].min()), int(day[\"rj\"].max())\n\n    # Start from the base threshold and adapt if needed\n    thr = THRESH_START\n    while True:\n        today_cells = set()\n\n        # Loop over all relevant grid cells for that day\n        for ci in range(cmin, cmax + 1):\n            for rj in range(rmin, rmax + 1):\n                poly = cell_poly(ci, rj)\n                cnt = 0\n\n                # Candidate segments intersecting this cell\n                for seg in query_segments(tree, poly, segs):\n                    if seg.intersects(poly):\n                        cnt += 1\n                        if cnt &gt;= thr:\n                            today_cells.add((ci, rj))\n                            break   # no need to count further for this cell\n\n        # If the day has too many activity CELLS, raise the threshold and try again\n        if len(today_cells) &gt; MAX_PLACES_PER_DAY and thr &lt; THRESH_MAX:\n            thr = min(thr + THRESH_STEP, THRESH_MAX)\n        else:\n            visited_cells_by_day[d] = today_cells\n            cross_threshold_used[d] = thr\n            break\n\n# Summarise: how many activity cells per day, and what threshold was used\nperday_summary = (\n    pd.DataFrame({\n        \"date\": [str(d) for d in dates],\n        \"visited_cells\": [len(visited_cells_by_day[d]) for d in dates],\n        \"threshold_used\": [cross_threshold_used[d] for d in dates],\n    })\n    .sort_values(\"date\")\n    .reset_index(drop=True)\n)\n\nprint(\"Daily activity-cell counts (first 10 days):\")\nperday_summary.head(10)\n\nNumber of days with data for user 000: 122\nDaily activity-cell counts (first 10 days):\n\n\n\n\n\n\n\n\n\ndate\nvisited_cells\nthreshold_used\n\n\n\n\n0\n2008-10-23\n3\n100\n\n\n1\n2008-10-24\n1\n100\n\n\n2\n2008-10-26\n2\n100\n\n\n3\n2008-10-27\n0\n100\n\n\n4\n2008-10-28\n3\n100\n\n\n5\n2008-10-29\n0\n100\n\n\n6\n2008-11-03\n0\n100\n\n\n7\n2008-11-04\n4\n100\n\n\n8\n2008-11-10\n0\n100\n\n\n9\n2008-11-11\n4\n100",
    "crumbs": [
      "Analysis",
      "Notebook 1 ‚Äì Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#from-daily-activity-cells-to-visit-level-events",
    "href": "Data Cleaning.html#from-daily-activity-cells-to-visit-level-events",
    "title": "Notebook 1 ‚Äî From raw Geolife trajectories to visit-level events",
    "section": "7. From daily activity cells to visit-level events",
    "text": "7. From daily activity cells to visit-level events\nGiven the set of daily activity cells, I next distinguish first-time places from returns and compress the trajectories into visit-level events. Across the whole observation window, a grid cell is labelled as a first-time place (Pv) on the first date on which it appears as an activity cell; on any later day when the same cell is active it is treated as a return place (Pn). HOME and SW are kept as separate categories and override the Pv/Pn labels.\nUsing these labels, I then build the visit-level table day by day. For each calendar day, the sequence of grid cells visited by the user is first compressed with a simple run-length encoding, so that consecutive points in the same cell form a single block. Each block is looked up in the activity cell set for that day and assigned a tag HOME, SW, Pv or Pn; blocks that are not classified as activity locations are dropped as in-transit segments. Adjacent blocks with the same cell and tag are merged so that brief GPS losses do not split a single stay into multiple visits.\nThe remaining blocks define the visit events. For each visit I record its start and end time, duration, grid indices and cell centre (in both projected and geographic coordinates), distance to HOME, and a place identifier (HOME, SW, Pv#, Pn#). I also keep two within-day order variables: a running visit order that counts all visits during the day, and an action-order index that restarts at 1 when the user leaves HOME and resets to 0 when they return. Finally, each visit is given a next_step label indicating whether the following visit is to home, SW, a Pv, a Pn, or none (end of the day). Stacking all days produces the final visit-level table, which is saved as visit_level_table_000.csv and used as input for the subsequent notebooks.\n\nfrom datetime import timedelta\n\npv_label = {}              # (ci, rj) -&gt; \"Pv#\"\npn_label = {}              # ((ci, rj), date) -&gt; \"Pn#\"\nfirst_visit_date = {}      # (ci, rj) -&gt; first date when this cell is an activity cell\n\npv_counter = 0\npn_counter = 0\n\nfor d in sorted(visited_cells_by_day.keys()):\n    cells = visited_cells_by_day[d]\n    for cell in sorted(cells):\n        if cell not in first_visit_date:\n            # First time this activity cell appears in the whole sample -&gt; Pv\n            pv_counter += 1\n            first_visit_date[cell] = d\n            pv_label[cell] = f\"Pv{pv_counter}\"\n        else:\n            # Subsequent days when the same activity cell is active -&gt; Pn\n            pn_counter += 1\n            pn_label[(cell, d)] = f\"Pn{pn_counter}\"\n\nprint(f\"Total unique Pv cells: {len(pv_label)}\")\nprint(f\"Total Pn labels assigned: {len(pn_label)}\")\n\n\n# --- 7.2 Helper: classify a cell on a given day ---\n\ndef classify_cell(cell, day_date):\n    \"\"\"\n    Classify a grid cell on a given date into:\n      - ('home', 'HOME')\n      - ('sw', 'SW')\n      - ('pv', 'Pv#')\n      - ('pn', 'Pn#')\n      - ('none', '')  for cells that are not treated as activity locations on that day.\n    \"\"\"\n    if cell == home_cell:\n        return \"home\", \"HOME\"\n    if (sw_cell is not None) and (cell == sw_cell):\n        return \"sw\", \"SW\"\n\n    visited_today = visited_cells_by_day.get(day_date, set())\n    if cell in visited_today:\n        if first_visit_date.get(cell) == day_date:\n            return \"pv\", pv_label[cell]\n        else:\n            return \"pn\", pn_label.get((cell, day_date), \"PN\")\n\n    return \"none\", \"\"\n\n\ndef cell_center_xy(ci, rj):\n    \"\"\"Return the projected (x, y) centre of grid cell (ci, rj).\"\"\"\n    cx = gx0 + (ci + 0.5) * GRID_SIZE\n    cy = gy0 + (rj + 0.5) * GRID_SIZE\n    return cx, cy\n\n\n# --- 7.3 Build the merged visit-level table ---\n\nvisit_rows = []\nall_dates = sorted(traj[\"date_only\"].unique().tolist())\n\nfor d in all_dates:\n    day = traj[traj[\"date_only\"] == d].sort_values(\"datetime\").copy()\n    if day.empty:\n        continue\n\n    # Sequence of grid cells and timestamps for this day\n    cells = list(zip(day[\"ci\"].astype(int), day[\"rj\"].astype(int)))\n    times = day[\"datetime\"].tolist()\n\n    # 1) Raw run-length encoding by grid cell:\n    #    consecutive identical (ci, rj) are grouped into one block.\n    runs = []  # list of (cell, i0, i1) with indices into 'times'\n    if cells:\n        start = 0\n        for i in range(1, len(cells)):\n            if cells[i] != cells[i-1]:\n                runs.append((cells[i-1], start, i-1))\n                start = i\n        runs.append((cells[-1], start, len(cells) - 1))\n\n    # 2) Attach labels and time bounds; drop blocks that are not activity cells\n    tagged = []\n    for (cell, i0, i1) in runs:\n        tag, pid = classify_cell(cell, d)\n        if tag == \"none\":\n            continue  # ignore transit-only cells\n        tagged.append({\n            \"cell\": cell,\n            \"ci\": cell[0],\n            \"rj\": cell[1],\n            \"tag\": tag,\n            \"place_id\": pid,\n            \"i0\": i0,\n            \"i1\": i1,\n            \"first_dt\": times[i0],\n            \"last_dt\": times[i1],\n        })\n\n    if not tagged:\n        continue\n\n    # 3) Merge adjacent blocks with the same (ci, rj, tag)\n    merged = []\n    current = tagged[0]\n    for nxt in tagged[1:]:\n        if (\n            (nxt[\"ci\"] == current[\"ci\"]) and\n            (nxt[\"rj\"] == current[\"rj\"]) and\n            (nxt[\"tag\"] == current[\"tag\"])\n        ):\n            # extend the current block\n            current[\"i1\"] = nxt[\"i1\"]\n            current[\"last_dt\"] = nxt[\"last_dt\"]\n        else:\n            merged.append(current)\n            current = nxt\n    merged.append(current)\n\n    # 4) For each merged visit, compute attributes and within-day order\n    visit_order = 0     \n    action_order = 0    \n\n    for k, rec in enumerate(merged):\n        ci, rj = rec[\"ci\"], rec[\"rj\"]\n        tag, pid = rec[\"tag\"], rec[\"place_id\"]\n        first_dt, last_dt = rec[\"first_dt\"], rec[\"last_dt\"]\n\n        visit_order += 1\n\n        if tag == \"home\":\n            action_order = 0    \n            action_ord = 0\n        else:\n            if action_order == 0:\n                action_order = 1  \n            else:\n                action_order += 1\n            action_ord = action_order\n\n        # Next-step label (based on the next merged block)\n        if k &lt; len(merged) - 1:\n            next_tag = merged[k + 1][\"tag\"]\n        else:\n            next_tag = \"none\"\n\n        # Geometry-based attributes\n        cx, cy = cell_center_xy(ci, rj)\n        hx, hy = cell_center_xy(*home_cell)\n        dist_home = math.hypot(cx - hx, cy - hy)\n        center_lon, center_lat = cell_center_lonlat(ci, rj)\n\n        visit_rows.append({\n            \"person\": USER_ID,\n            \"date\": d.isoformat(),\n            \"first_time\": first_dt.time().isoformat(),\n            \"last_time\": last_dt.time().isoformat(),\n            \"duration_min\": round((last_dt - first_dt) / timedelta(minutes=1), 1),\n            \"grid_ci\": ci,\n            \"grid_rj\": rj,\n            \"grid_center_lon\": round(center_lon, 6),\n            \"grid_center_lat\": round(center_lat, 6),\n            \"dist_home_m\": round(dist_home, 1),\n            \"is_home\": 1 if tag == \"home\" else 0,\n            \"is_sw\":   1 if tag == \"sw\"   else 0,\n            \"is_pv\":   1 if tag == \"pv\"   else 0,\n            \"is_pn\":   1 if tag == \"pn\"   else 0,\n            \"place_id\": pid,          # HOME / SW / Pv# / Pn#\n            \"next_step\": next_tag,    # home / sw / pv / pn / none\n            \"visit_order_in_day\": visit_order,  \n            \"action_order\": action_ord         \n        })\n\n# Final visit-level table\nvisit_table = (\n    pd.DataFrame(visit_rows)\n    .sort_values([\"date\", \"first_time\", \"grid_ci\", \"grid_rj\"])\n    .reset_index(drop=True)\n)\n\nprint(\"Number of visit events:\", len(visit_table))\nvisit_table.head(10)\n\nTotal unique Pv cells: 106\nTotal Pn labels assigned: 294\nNumber of visit events: 1841\n\n\n\n\n\n\n\n\n\nperson\ndate\nfirst_time\nlast_time\nduration_min\ngrid_ci\ngrid_rj\ngrid_center_lon\ngrid_center_lat\ndist_home_m\nis_home\nis_sw\nis_pv\nis_pn\nplace_id\nnext_step\nvisit_order_in_day\naction_order\n\n\n\n\n0\n000\n2008-10-23\n03:00:55\n04:13:32\n72.6\n40\n50\n116.300184\n39.984308\n3201.6\n0\n0\n1\n0\nPv1\nsw\n1\n1\n\n\n1\n000\n2008-10-23\n09:42:25\n09:42:30\n0.1\n43\n55\n116.317527\n40.006935\n500.0\n0\n1\n0\n0\nSW\nhome\n2\n2\n\n\n2\n000\n2008-10-23\n09:42:35\n10:05:29\n22.9\n44\n55\n116.323385\n40.006970\n0.0\n1\n0\n0\n0\nHOME\nsw\n3\n0\n\n\n3\n000\n2008-10-23\n10:05:34\n10:30:15\n24.7\n43\n55\n116.317527\n40.006935\n500.0\n0\n1\n0\n0\nSW\nhome\n4\n1\n\n\n4\n000\n2008-10-23\n10:30:20\n11:10:57\n40.6\n44\n55\n116.323385\n40.006970\n0.0\n1\n0\n0\n0\nHOME\nnone\n5\n0\n\n\n5\n000\n2008-10-24\n02:09:59\n02:10:54\n0.9\n43\n55\n116.317527\n40.006935\n500.0\n0\n1\n0\n0\nSW\nhome\n1\n1\n\n\n6\n000\n2008-10-24\n02:10:59\n02:47:06\n36.1\n44\n55\n116.323385\n40.006970\n0.0\n1\n0\n0\n0\nHOME\nnone\n2\n0\n\n\n7\n000\n2008-10-26\n14:04:27\n14:12:42\n8.2\n55\n33\n116.388704\n39.908225\n12298.4\n0\n0\n1\n0\nPv4\npv\n1\n1\n\n\n8\n000\n2008-10-26\n14:23:42\n14:35:17\n11.6\n56\n31\n116.394633\n39.899246\n13416.4\n0\n0\n1\n0\nPv5\nnone\n2\n2\n\n\n9\n000\n2008-10-27\n12:03:59\n12:05:54\n1.9\n44\n55\n116.323385\n40.006970\n0.0\n1\n0\n0\n0\nHOME\nnone\n1\n0\n\n\n\n\n\n\n\n\n# save visit-level table for later notebooks\nOUT_DIR = \"data\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nout_path = os.path.join(OUT_DIR, f\"visit_level_table_{USER_ID}.csv\")\nvisit_table.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\nprint(\"Saved visit-level table to:\", out_path)\n\nSaved visit-level table to: data\\visit_level_table_000.csv",
    "crumbs": [
      "Analysis",
      "Notebook 1 ‚Äì Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#mapping-inferred-activity-places",
    "href": "Data Cleaning.html#mapping-inferred-activity-places",
    "title": "Notebook 1 ‚Äî From raw Geolife trajectories to visit-level events",
    "section": "8. Mapping inferred activity places",
    "text": "8. Mapping inferred activity places\nTo summarise the cleaned visit-level data, I project the inferred activity locations back onto an interactive map. Starting from the visit table, I collapse repeated visits to the same 500 m grid cell and draw one square for each distinct cell that ever appears as a visit. Each cell is coloured according to its role in the visit history (explored at least once versus only revisited), while the HOME and SW cells are highlighted separately as point markers. Compared with the raw trajectories, this map shows a much simpler picture of the user‚Äôs daily activity space: a small set of recurrent places organised around home, rather than every individual GPS point and transit segment.\n\n# --- 8.1 Aggregate unique places from the visit-level table ---\n\n# We keep only non-home visits (HOME is shown separately as a point),\n# and collapse repeated visits to the same grid cell.\nplaces = (\n    visit_table\n    .groupby([\"grid_ci\", \"grid_rj\"], as_index=False)\n    .agg(\n        any_home=(\"is_home\", \"max\"),\n        any_sw=(\"is_sw\", \"max\"),\n        any_pv=(\"is_pv\", \"max\"),\n        any_pn=(\"is_pn\", \"max\"),\n        grid_center_lon=(\"grid_center_lon\", \"first\"),\n        grid_center_lat=(\"grid_center_lat\", \"first\"),\n        dist_home_m=(\"dist_home_m\", \"min\")\n    )\n)\n\n\nprint(\"Number of unique grid cells appearing in visits:\", len(places))\n\n\n# --- 8.2 Helper: cell polygon in lat/lon ---\n\ndef cell_bounds_lonlat(ci, rj):\n    \"\"\"\n    Return the 4 corners (and closed ring) of grid cell (ci, rj)\n    as (lat, lon) pairs for use in folium.Polygon.\n    \"\"\"\n    x0 = gx0 + ci * GRID_SIZE\n    y0 = gy0 + rj * GRID_SIZE\n    x1 = x0 + GRID_SIZE\n    y1 = y0 + GRID_SIZE\n\n    lon0, lat0 = to_wgs(x0, y0)\n    lon1, lat1 = to_wgs(x1, y0)\n    lon2, lat2 = to_wgs(x1, y1)\n    lon3, lat3 = to_wgs(x0, y1)\n\n    # folium expects (lat, lon)\n    return [\n        (lat0, lon0),\n        (lat1, lon1),\n        (lat2, lon2),\n        (lat3, lon3),\n        (lat0, lon0),\n    ]\n\n\n# --- 8.3 Colour rule for places ---\n\ndef color_for_place(row):\n    cell = (int(row[\"grid_ci\"]), int(row[\"grid_rj\"]))\n    # HOME and SW will be shown as point markers; here we colour only squares\n    if sw_cell is not None and cell == sw_cell:\n        return \"green\"\n    # Pv vs Pn logic\n    if row[\"any_pn\"] == 1:\n        return \"orange\"   # ever seen as Pv at least once\n    if row[\"any_pv\"] == 1:\n        return \"purple\"   # only returns\n    return \"gray\"\n\n\n# --- 8.4 Initialise a folium map centred at HOME ---\n\n# Compute HOME centre in lat/lon (from grid indices)\nhx = gx0 + (home_cell[0] + 0.5) * GRID_SIZE\nhy = gy0 + (home_cell[1] + 0.5) * GRID_SIZE\nhome_lon, home_lat = to_wgs(hx, hy)\n\nm = folium.Map(location=[home_lat, home_lon],\n               zoom_start=12,\n               tiles=\"cartodbpositron\")\n\n\n# --- 8.5 Add grid-cell polygons for activity places ---\n\nfor _, row in places.iterrows():\n    ci = int(row[\"grid_ci\"])\n    rj = int(row[\"grid_rj\"])\n    poly_latlon = cell_bounds_lonlat(ci, rj)\n    col = color_for_place(row)\n\n    popup_html = (\n        f\"Cell: ({ci}, {rj})&lt;br&gt;\"\n        f\"Center: ({row['grid_center_lat']:.5f}, {row['grid_center_lon']:.5f})&lt;br&gt;\"\n        f\"Dist. to HOME: {int(round(row['dist_home_m']))} m&lt;br&gt;\"\n        f\"Any Pv: {int(row['any_pv'])} &nbsp; Any Pn: {int(row['any_pn'])}\"\n    )\n\n    folium.Polygon(\n        locations=poly_latlon,\n        color=col,\n        weight=2,\n        fill=True,\n        fill_opacity=0.35,\n        popup=popup_html\n    ).add_to(m)\n\n\n# --- 8.6 Add HOME and SW as point markers ---\n\nfolium.CircleMarker(\n    location=[home_lat, home_lon],\n    radius=7,\n    color=\"blue\",\n    fill=True,\n    fill_opacity=0.9,\n    popup=\"HOME\"\n).add_to(m)\n\nif sw_cell is not None:\n    sx = gx0 + (sw_cell[0] + 0.5) * GRID_SIZE\n    sy = gy0 + (sw_cell[1] + 0.5) * GRID_SIZE\n    sw_lon, sw_lat = to_wgs(sx, sy)\n\n    folium.CircleMarker(\n        location=[sw_lat, sw_lon],\n        radius=7,\n        color=\"green\",\n        fill=True,\n        fill_opacity=0.9,\n        popup=\"SW\"\n    ).add_to(m)\n\n\nm\n\nNumber of unique grid cells appearing in visits: 106\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Analysis",
      "Notebook 1 ‚Äì Data cleaning"
    ]
  },
  {
    "objectID": "LR.html",
    "href": "LR.html",
    "title": "Literature Review & Project Overview",
    "section": "",
    "text": "This project uses high-resolution Geolife GPS trajectories to examine everyday mobility as a sequence of discrete visits, trips, and exploration decisions.\nThe analysis is structured into five Jupyter notebooks.\n\n1. What is EPR?\nWe examine everyday movement as a balance between revisiting familiar places and exploring new ones. In behavioral terms, most people follow habitual travel routines (e.g., home‚Äìwork‚Äìhome) yet occasionally deviate to novel destinations. This ‚Äúexploit vs.¬†explore‚Äù trade-off can be framed by an Exploration‚ÄìPreferencing Ratio (EPR): a higher EPR means more exploration of new sites relative to returning to known ones. Conceptually, EPR is analogous to the exploration-and-preferential-return model in human mobility research, in which at each move an agent either visits a new location or returns to a past one (Pappalardo, Rinzivillo, & Simini, 2016; Song, Koren, Wang, & Barab√°si, 2010). Large-scale visitation patterns have been shown to arise from EPR-like dynamics in empirical data (Schl√§pfer et al., 2021; Song, Qu, Blumm, & Barab√°si, 2010).\nIn mobility data, we treat EPR operationally as the ratio of novel stops to repeated stops. For example, if a trip includes three previously unvisited stops (exploration) and one repeated stop, the person‚Äôs trip-level EPR would be 3:1, indicating exploratory behavior. In our context, we adapt this idea to individual GPS trajectories by explicitly labelling each stop as either ‚ÄúPv‚Äù (previously visited) or ‚ÄúPn‚Äù (novel), and then modeling the patterns of Pv/Pn occurrences within trips.\nIn our data, we operationalize this by labelling each non-home stop as Pn (novel) if it is the first time the user has stopped at that particular location, or Pv (visited) if it falls at a location the user has visited before. Over a trip (home ‚Üí ‚Ä¶ ‚Üí home), we then summarize the trip‚Äôs exploratory tendency by, for example, the count of Pn stops or the ratio Pn/(Pv + Pn). A person‚Äôs overall EPR can be aggregated from their trips (e.g., average per-trip exploration rate).\nThis trip-focused EPR differs from classic probabilistic models (e.g., the EPR model of Song et al., 2010) in that we measure empirical behavior rather than impose a fixed probability of exploration (Song et al., 2010). Prior literature has examined related metrics: Pappalardo et al.¬†(2015), for instance, show that individuals cluster into ‚Äúexplorers‚Äù (many new locations) or ‚Äúreturners‚Äù (few new locations) based on visit-count ratios. Our approach is similar in spirit but works at the granularity of trips and discrete stops.\nWe also draw on ecological ideas of foraging: just as people navigating information maximize an information-gain rate by choosing to explore ‚Äúnew patches‚Äù only when the expected gain outweighs the cost (Pirolli & Card, 1999; Nielsen, 2019), travelers may implicitly weigh the novelty of a potential stop against its travel or time cost. We do not explicitly model that decision rule, but the EPR encapsulates its outcome. In short, EPR is the key behavioral concept linking individual choices of revisiting vs.¬†exploring, and our goal is to measure it from trajectory data.\n\n\n2. Data Processing: From Raw GPS to Home‚ÄìHome Trips\nWe apply the methodology to the Microsoft Geolife GPS dataset (184 users, multi-year GPS tracks; Zheng, Xie, & Ma, 2010). After loading and cleaning the Geolife .plt trajectories, coordinates are projected to UTM Zone 50N so that distances are measured in meters. The trajectory is then discretized onto a regular 500 m √ó 500 m grid, and consecutive fixes assigned to the same grid cell are treated as a single stay segment. This representation reduces sensitivity to GPS jitter and irregular sampling, while producing visits with explicit arrival/departure times and durations.\nHome location is inferred from night-time (00:00‚Äì06:00) observations rather than overall visit frequency: the grid cell with the strongest night-time presence is labeled HOME, and a secondary frequent night-time cell may be labeled SW. To avoid misclassifying fast in-transit corridors as ‚Äúplaces,‚Äù daily activity locations are identified conservatively using line‚Äìgrid intersection counts: consecutive points are connected into segments, and grid cells with sufficiently many segment intersections are retained, with an adaptive threshold to cap the number of activity cells per day. Visit events are then constructed by collapsing the within-day cell sequence, dropping non-activity blocks as transit, and merging adjacent identical blocks to reduce fragmentation.\nEach non-home visit is labeled as first-time versus return based on the user‚Äôs cumulative visitation history across days: a cell is treated as novel the first day it appears as an activity location and as a return on subsequent days. The resulting visit sequence provides the input for subsequent steps, where home‚Äìhome trips are constructed and the trip-level Pv/Pn patterns are analyzed to quantify exploration behavior.\n\n\n\nNotebook 1 overview\n\n\n\n\n3. Trip-level behaviour within a single user\nGiven the home‚Äìhome trip sequences with first-time/return labels, two step-level aspects of trip behavior are modeled.\nThe first is the hazard of returning home (trip termination). Each trip is treated as a discrete-time survival process: at every stop (time step) prior to termination, there is some probability that the next move is a return to HOME. Following Singer and Willett‚Äôs discrete-time framework, a logistic hazard model is estimated by ‚Äúexploding‚Äù each trip into one row per stop, with a binary outcome indicating whether the next step is HOME (return = 1) or not (0). This outcome is regressed on covariates observed at that step (e.g., stop order, elapsed time since departure, distance from home, and whether the most recent stop was first-time versus a return). The fitted hazard model summarizes how the propensity to end a trip evolves as the trip unfolds.\nThe second component models exploration propensity at each step. Here the binary outcome indicates whether the current stop is a first-time place versus a return (under the notebook‚Äôs Pv/Pn convention). A logistic regression (or similar classifier) is fitted for ‚Äúexplore = 1‚Äù using predictors such as step index, time of day, day of week, and distance from home. This model captures how the probability of making a first-time stop changes with trip progress and context, and can be interpreted as a step-level analogue of EPR.\n\n\n\nNotebook 2 overview\n\n\n\n\n4. Cross-User Comparison: PCA and Clustering of EPR Profiles\nFinally, we compare users in the space of these model-derived behaviors. Each user‚Äôs model yields a feature vector (e.g., [hazard intercept, hazard slope, exploration intercept, exploration slope, ‚Ä¶]) that encodes their tendency to explore and to return. Since this vector may be high-dimensional, we first apply principal components analysis (PCA) to reduce dimensionality and identify the main axes of behavioral variance. PCA reveals whether users vary along, for example, a ‚Äúhigh-exploration vs.¬†high-preference for revisitation‚Äù spectrum or along other blended factors.\nWe then apply k-means clustering (with k chosen by silhouette or cross-validation criteria) to group users into behavioral phenotypes based on their PCA scores. Prior work has shown that clustering mobility patterns often yields well-separated groups defined by exploration intensity and travel range. In our case, the clustering identifies typical EPR/hazard profiles in the population‚Äîfor example, ‚Äúlocal explorers‚Äù (generally short trips but often to new places) versus ‚Äúlong-distance returners‚Äù (travel far but largely between the same hubs). We can then summarize each cluster by its mean EPR, radius of gyration, and similar indicators, and interpret it in behavioral terms.",
    "crumbs": [
      "Analysis",
      "Notebook 0 - Overview"
    ]
  }
]