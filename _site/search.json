[
  {
    "objectID": "verification.html",
    "href": "verification.html",
    "title": "Notebook 3 — Cross-user verification of daily exploration patterns",
    "section": "",
    "text": "Notebook 3 moves from a single-user case study to a cross-user view of mobility behaviour. Building on the visit-level tables created earlier (data/visit_level_table_XXX.csv), it pools a set of long-coverage Geolife users and compares their within-trip “going home” and exploration patterns.\nThe workflow has three components:\n\nSample construction and trip reconstruction. I select users with sufficiently long observation windows. For each user, I re-attach trip structure to the visit-level events by assigning a within-day trip_id and a within-trip stop order (action_order_in_trip) to all visits.\nPer-user mobility summaries. I compress each user’s mobility into a small set of trip-level statistics: number of observed days, number of days with at least one non-home stop, total number of home-based trips, and the distribution of non-home stops per trip (mean, median, max). I also compute simple exploration indicators based on whether non-home stops are first-time places (Pv) or revisits (Pn).\nCross-user model-based comparison. To compare users on a common scale, I fit user-specific discrete-time logit models in the stop-order index. These models provide compact, interpretable parameters describing both the overall level and within-trip change of (i) returning HOME and (ii) visiting a new place.\n\n\n\nShow code\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"DataFrameGroupBy.apply operated on the grouping columns\",\n    category=FutureWarning,\n)\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"Parsing dates in %d/%m/%Y format\",\n    category=UserWarning,\n)\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\nplt.style.use(\"default\")\n\nDATA_DIR = \"data\"  # folder containing visit_level_table_XXX.csv\n\n# Pre-selected long-coverage users (can be updated if needed)\nLONG_USERS = [\n    \"004\", \"003\", \"017\", \"025\", \"030\", \"126\",\n    \"062\", \"084\", \"039\", \"041\", \"022\", \"014\", \"000\",\n    \"002\", \"092\", \"112\", \"104\", \"052\"\n]\n\n\ndef load_visit_table_from_csv(user_id: str,\n                              data_dir: str = DATA_DIR,\n                              verbose: bool = True) -&gt; pd.DataFrame | None:\n    \"\"\"\n    Load visit_level_table_XXX.csv for one user and parse timestamps.\n    Assumes the schema from Notebook 1 / 2:\n      - person\n      - date, first_time, last_time\n      - is_home, next_step, dist_home_m\n      - visit_order_in_day, is_pv, is_pn\n    \"\"\"\n    fname = f\"visit_level_table_{user_id}.csv\"\n    path = os.path.join(data_dir, fname)\n\n    if not os.path.exists(path):\n        if verbose:\n            print(f\"[skip] {fname} not found at {path}\")\n        return None\n\n    vt = pd.read_csv(path)\n    if vt.empty:\n        if verbose:\n            print(f\"[skip] {fname} is empty\")\n        return None\n\n    vt[\"date\"] = pd.to_datetime(vt[\"date\"]).dt.date\n    vt[\"first_dt\"] = pd.to_datetime(vt[\"date\"].astype(str) + \" \" + vt[\"first_time\"])\n    vt[\"last_dt\"] = pd.to_datetime(vt[\"date\"].astype(str) + \" \" + vt[\"last_time\"])\n\n    vt = (\n        vt.sort_values([\"person\", \"date\", \"first_dt\"])\n          .reset_index(drop=True)\n    )\n    return vt\n\n\ndef add_trip_cols_one_day(df_day: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    For a single (person, date), label home–home trip episodes.\n\n    trip_id:\n      0 if no trip yet; 1,2,... for successive trips in that day\n    action_order_in_trip:\n      0 for HOME visits; 1,2,... for non-home stops within a trip\n    \"\"\"\n    df_day = df_day.sort_values(\"first_dt\").copy()\n\n    trip_id = 0\n    action_order = 0\n    out = False  # currently away from HOME?\n\n    trip_ids = []\n    action_orders = []\n\n    for _, row in df_day.iterrows():\n        if row[\"is_home\"] == 1:\n            out = False\n            action_order = 0\n            trip_ids.append(trip_id)\n            action_orders.append(0)\n        else:\n            if not out:\n                trip_id += 1\n                action_order = 1\n                out = True\n            else:\n                action_order += 1\n\n            trip_ids.append(trip_id)\n            action_orders.append(action_order)\n\n    df_day[\"trip_id\"] = trip_ids\n    df_day[\"action_order_in_trip\"] = action_orders\n    return df_day\n\n\ndef attach_trip_structure(vt: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Apply add_trip_cols_one_day to each (person, date).\"\"\"\n    vt_trips = (\n        vt.groupby([\"person\", \"date\"], group_keys=False)\n          .apply(add_trip_cols_one_day)\n          .reset_index(drop=True)\n    )\n    return vt_trips",
    "crumbs": [
      "Analysis",
      "Notebook 3 – Cross-user verification"
    ]
  },
  {
    "objectID": "verification.html#more-geolife-users",
    "href": "verification.html#more-geolife-users",
    "title": "Notebook 3 — Cross-user verification of daily exploration patterns",
    "section": "",
    "text": "Notebook 3 moves from a single-user case study to a cross-user view of mobility behaviour. Building on the visit-level tables created earlier (data/visit_level_table_XXX.csv), it pools a set of long-coverage Geolife users and compares their within-trip “going home” and exploration patterns.\nThe workflow has three components:\n\nSample construction and trip reconstruction. I select users with sufficiently long observation windows. For each user, I re-attach trip structure to the visit-level events by assigning a within-day trip_id and a within-trip stop order (action_order_in_trip) to all visits.\nPer-user mobility summaries. I compress each user’s mobility into a small set of trip-level statistics: number of observed days, number of days with at least one non-home stop, total number of home-based trips, and the distribution of non-home stops per trip (mean, median, max). I also compute simple exploration indicators based on whether non-home stops are first-time places (Pv) or revisits (Pn).\nCross-user model-based comparison. To compare users on a common scale, I fit user-specific discrete-time logit models in the stop-order index. These models provide compact, interpretable parameters describing both the overall level and within-trip change of (i) returning HOME and (ii) visiting a new place.\n\n\n\nShow code\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"DataFrameGroupBy.apply operated on the grouping columns\",\n    category=FutureWarning,\n)\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"Parsing dates in %d/%m/%Y format\",\n    category=UserWarning,\n)\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\nplt.style.use(\"default\")\n\nDATA_DIR = \"data\"  # folder containing visit_level_table_XXX.csv\n\n# Pre-selected long-coverage users (can be updated if needed)\nLONG_USERS = [\n    \"004\", \"003\", \"017\", \"025\", \"030\", \"126\",\n    \"062\", \"084\", \"039\", \"041\", \"022\", \"014\", \"000\",\n    \"002\", \"092\", \"112\", \"104\", \"052\"\n]\n\n\ndef load_visit_table_from_csv(user_id: str,\n                              data_dir: str = DATA_DIR,\n                              verbose: bool = True) -&gt; pd.DataFrame | None:\n    \"\"\"\n    Load visit_level_table_XXX.csv for one user and parse timestamps.\n    Assumes the schema from Notebook 1 / 2:\n      - person\n      - date, first_time, last_time\n      - is_home, next_step, dist_home_m\n      - visit_order_in_day, is_pv, is_pn\n    \"\"\"\n    fname = f\"visit_level_table_{user_id}.csv\"\n    path = os.path.join(data_dir, fname)\n\n    if not os.path.exists(path):\n        if verbose:\n            print(f\"[skip] {fname} not found at {path}\")\n        return None\n\n    vt = pd.read_csv(path)\n    if vt.empty:\n        if verbose:\n            print(f\"[skip] {fname} is empty\")\n        return None\n\n    vt[\"date\"] = pd.to_datetime(vt[\"date\"]).dt.date\n    vt[\"first_dt\"] = pd.to_datetime(vt[\"date\"].astype(str) + \" \" + vt[\"first_time\"])\n    vt[\"last_dt\"] = pd.to_datetime(vt[\"date\"].astype(str) + \" \" + vt[\"last_time\"])\n\n    vt = (\n        vt.sort_values([\"person\", \"date\", \"first_dt\"])\n          .reset_index(drop=True)\n    )\n    return vt\n\n\ndef add_trip_cols_one_day(df_day: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    For a single (person, date), label home–home trip episodes.\n\n    trip_id:\n      0 if no trip yet; 1,2,... for successive trips in that day\n    action_order_in_trip:\n      0 for HOME visits; 1,2,... for non-home stops within a trip\n    \"\"\"\n    df_day = df_day.sort_values(\"first_dt\").copy()\n\n    trip_id = 0\n    action_order = 0\n    out = False  # currently away from HOME?\n\n    trip_ids = []\n    action_orders = []\n\n    for _, row in df_day.iterrows():\n        if row[\"is_home\"] == 1:\n            out = False\n            action_order = 0\n            trip_ids.append(trip_id)\n            action_orders.append(0)\n        else:\n            if not out:\n                trip_id += 1\n                action_order = 1\n                out = True\n            else:\n                action_order += 1\n\n            trip_ids.append(trip_id)\n            action_orders.append(action_order)\n\n    df_day[\"trip_id\"] = trip_ids\n    df_day[\"action_order_in_trip\"] = action_orders\n    return df_day\n\n\ndef attach_trip_structure(vt: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Apply add_trip_cols_one_day to each (person, date).\"\"\"\n    vt_trips = (\n        vt.groupby([\"person\", \"date\"], group_keys=False)\n          .apply(add_trip_cols_one_day)\n          .reset_index(drop=True)\n    )\n    return vt_trips",
    "crumbs": [
      "Analysis",
      "Notebook 3 – Cross-user verification"
    ]
  },
  {
    "objectID": "verification.html#per-user-trip-level-summary",
    "href": "verification.html#per-user-trip-level-summary",
    "title": "Notebook 3 — Cross-user verification of daily exploration patterns",
    "section": "2. Per-user trip-level summary",
    "text": "2. Per-user trip-level summary\nThis section constructs a compact trip-level “fingerprint” for each long-coverage user. Starting from the visit-level tables, I first reconstruct home-based trips and then compute:\n\nActivity coverage: total observed days (n_days_total) and days with ≥1 non-home stop (n_days_nonhome).\nTrip volume and complexity: number of trips (n_trips) and stops per trip (mean/median/max).\n\nI then summarize within-trip decisions and exploration with two simple user-level models:\n\nGoing-home behaviour (hazard model). For each user, I estimate:\n\nhazard_h1: the empirical probability of going HOME immediately after the first non-home stop.\nhazard_h2_const: a pooled baseline hazard for stops k ≥ 2` (a constant level for later stops).\nhazard_beta_k: a logit slope in stop order capturing whether the later-stop hazard increases or decreases with k (estimated using k ≥ 2 and restricting to stop orders with sufficient data).\n\nWithin-trip exploration (Pv model). For each user, I record:\n\noverall_pv_share: the overall share of first-time places (Pv) among non-home stops within trips.\npv_odds_ratio_per_stop: an odds ratio from a logit model of Pv on stop order, summarizing how the odds of visiting a new place change with each additional stop (estimated only where each stop order has sufficient observations).\n\n\nThe resulting summary_df contains one row per user and combines trip statistics, going-home parameters, and exploration parameters for cross-user comparisons.\n\n\nShow code\ndef summarise_trip_structure(vt_trips: pd.DataFrame) -&gt; dict:\n    \"\"\"\n    Basic trip-level stats:\n      - n_days_total: days with any record\n      - n_days_nonhome: days with at least one non-home stop\n      - n_trips: number of home–home trips\n      - mean / median / max non-home stops per trip\n    \"\"\"\n    nonhome = vt_trips[\n        (vt_trips[\"trip_id\"] &gt; 0) &\n        (vt_trips[\"is_home\"] == 0) &\n        (vt_trips[\"action_order_in_trip\"] &gt; 0)\n    ].copy()\n\n    n_days_total = vt_trips[\"date\"].nunique()\n    n_days_nonhome = nonhome[\"date\"].nunique()\n\n    trips = (\n        nonhome[[\"person\", \"date\", \"trip_id\", \"action_order_in_trip\"]]\n        .groupby([\"person\", \"date\", \"trip_id\"], as_index=False)\n        .agg(n_stops=(\"action_order_in_trip\", \"max\"))\n    )\n\n    if trips.empty:\n        return {\n            \"n_days_total\": n_days_total,\n            \"n_days_nonhome\": n_days_nonhome,\n            \"n_trips\": 0,\n            \"mean_stops_per_trip\": np.nan,\n            \"median_stops_per_trip\": np.nan,\n            \"max_stops_per_trip\": np.nan,\n        }\n\n    return {\n        \"n_days_total\": n_days_total,\n        \"n_days_nonhome\": n_days_nonhome,\n        \"n_trips\": len(trips),\n        \"mean_stops_per_trip\": trips[\"n_stops\"].mean(),\n        \"median_stops_per_trip\": trips[\"n_stops\"].median(),\n        \"max_stops_per_trip\": trips[\"n_stops\"].max(),\n    }\n\n\ndef fit_hazard_trip(vt_trips: pd.DataFrame,\n                    maxk: int = 30,\n                    night_hour: int = 23,\n                    near_home_m: float = 750.0,\n                    min_risk: int = 20) -&gt; dict:\n    \"\"\"\n    Trip-level discrete-time hazard of going home:\n      - hazard_h1: empirical hazard at k=1\n      - hazard_h2_const: pooled hazard over k&gt;=2\n      - hazard_beta_k: logit slope of hazard vs k for k&gt;=2\n    \"\"\"\n    vis = vt_trips[\n        (vt_trips[\"is_home\"] == 0) &\n        (vt_trips[\"trip_id\"] &gt; 0) &\n        (vt_trips[\"action_order_in_trip\"] &gt; 0)\n    ].copy()\n    vis = vis[vis[\"action_order_in_trip\"] &lt;= maxk].copy()\n\n    if vis.empty:\n        return {\n            \"hazard_h1\": np.nan,\n            \"hazard_h2_const\": np.nan,\n            \"hazard_beta_k\": np.nan,\n        }\n\n    vis[\"outcome\"] = np.where(\n        vis[\"next_step\"] == \"home\", \"home\",\n        np.where(vis[\"next_step\"].isin([\"sw\", \"pv\", \"pn\"]), \"explore\", \"end\")\n    )\n\n    # Last non-home stop in each trip\n    last_mask = vis.groupby(\n        [\"person\", \"date\", \"trip_id\"]\n    ).cumcount(ascending=False).eq(0)\n\n    last_time_dt = pd.to_datetime(\n        vis[\"last_time\"], format=\"%H:%M:%S\", errors=\"coerce\"\n    )\n    late = last_time_dt.dt.hour.ge(night_hour)\n    near = vis[\"dist_home_m\"].le(near_home_m)\n\n    vis.loc[\n        last_mask & (vis[\"outcome\"] == \"end\") & (late | near),\n        \"outcome\"\n    ] = \"home\"\n\n    pertrip_maxk = (\n        vis.groupby([\"person\", \"date\", \"trip_id\"])[\"action_order_in_trip\"]\n           .max()\n    )\n\n    at_risk = pd.Series(\n        {k: int((pertrip_maxk &gt;= k).sum()) for k in range(1, maxk + 1)},\n        name=\"at_risk\"\n    )\n\n    home_k = (\n        vis[vis[\"outcome\"] == \"home\"]\n        .groupby(\"action_order_in_trip\")\n        .size()\n        .reindex(range(1, maxk + 1), fill_value=0)\n        .rename(\"home_k\")\n    )\n\n    hazard = (home_k / at_risk.replace(0, np.nan)).rename(\"hazard\")\n    haz_df = pd.concat([home_k, at_risk, hazard], axis=1)\n\n    # k = 1\n    hazard_h1 = haz_df.loc[1, \"hazard\"] if at_risk.loc[1] &gt; 0 else np.nan\n\n    # pooled hazard for k &gt;= 2\n    mask_ge2 = (haz_df.index &gt;= 2) & (haz_df[\"at_risk\"] &gt; 0)\n    if mask_ge2.any():\n        H_num = haz_df.loc[mask_ge2, \"home_k\"].sum()\n        H_den = haz_df.loc[mask_ge2, \"at_risk\"].sum()\n        hazard_h2_const = H_num / H_den if H_den &gt; 0 else np.nan\n    else:\n        hazard_h2_const = np.nan\n\n    # logit slope for k &gt;= 2\n    haz_fit = (\n        haz_df\n        .loc[haz_df.index &gt;= 2]\n        .loc[lambda df: df[\"at_risk\"] &gt;= min_risk]\n        .copy()\n    )\n\n    if haz_fit.empty:\n        hazard_beta_k = np.nan\n    else:\n        k_fit = haz_fit.index.values.astype(float)\n        X = sm.add_constant(k_fit)\n        y = np.column_stack([\n            haz_fit[\"home_k\"].values,\n            (haz_fit[\"at_risk\"] - haz_fit[\"home_k\"]).values\n        ])\n        glm = sm.GLM(y, X, family=sm.families.Binomial())\n        res_haz = glm.fit()\n        hazard_beta_k = res_haz.params[1]\n\n    return {\n        \"hazard_h1\": hazard_h1,\n        \"hazard_h2_const\": hazard_h2_const,\n        \"hazard_beta_k\": hazard_beta_k,\n    }\n\n\ndef fit_pv_trip(vt_trips: pd.DataFrame,\n                maxk: int = 30,\n                min_n: int = 30) -&gt; dict:\n    \"\"\"\n    Within-trip exploration model:\n      - overall_pv_share: mean(is_pv) among non-home trip stops\n      - pv_odds_ratio_per_stop: odds ratio per +1 stop from logit(P(Pv) ~ k_centered)\n    \"\"\"\n    vis_places = vt_trips[\n        (vt_trips[\"is_home\"] == 0) &\n        (vt_trips[\"trip_id\"] &gt; 0) &\n        (vt_trips[\"action_order_in_trip\"] &gt; 0)\n    ].copy()\n    vis_places = vis_places[vis_places[\"action_order_in_trip\"] &lt;= maxk].copy()\n\n    if vis_places.empty:\n        return {\n            \"overall_pv_share\": np.nan,\n            \"pv_odds_ratio_per_stop\": np.nan,\n        }\n\n    overall_pv = vis_places[\"is_pv\"].mean()\n\n    pv_by_k = (\n        vis_places\n        .groupby(\"action_order_in_trip\")[[\"is_pv\", \"is_pn\"]]\n        .agg(\n            pv_cnt=(\"is_pv\", \"sum\"),\n            pn_cnt=(\"is_pn\", \"sum\"),\n            n=(\"is_pv\", \"size\")\n        )\n        .reset_index()\n    )\n\n    valid_k = pv_by_k.loc[pv_by_k[\"n\"] &gt;= min_n, \"action_order_in_trip\"]\n    vis_lr_sub = vis_places[\n        vis_places[\"action_order_in_trip\"].isin(valid_k)\n    ].copy()\n\n    if vis_lr_sub.empty:\n        return {\n            \"overall_pv_share\": overall_pv,\n            \"pv_odds_ratio_per_stop\": np.nan,\n        }\n\n    vis_lr_sub[\"k_centered\"] = (\n        vis_lr_sub[\"action_order_in_trip\"]\n        - vis_lr_sub[\"action_order_in_trip\"].mean()\n    )\n\n    X = sm.add_constant(vis_lr_sub[\"k_centered\"])\n    y = vis_lr_sub[\"is_pv\"]\n\n    logit_model = sm.Logit(y, X)\n    res = logit_model.fit(disp=False)\n\n    beta1 = res.params[\"k_centered\"]\n    odds_ratio = float(np.exp(beta1))\n\n    return {\n        \"overall_pv_share\": overall_pv,\n        \"pv_odds_ratio_per_stop\": odds_ratio,\n    }\n\n\ndef summarise_user_trip_behaviour(uid: str) -&gt; pd.Series | None:\n    \"\"\"\n    Wrapper:\n      - load visit table\n      - attach trip structure\n      - compute trip stats, hazard stats, Pv stats\n      - return as a single-row Series\n    \"\"\"\n    vt = load_visit_table_from_csv(uid)\n    if vt is None or vt.empty:\n        print(f\"[skip] user {uid}: no usable visit table.\")\n        return None\n\n    vt_trips = attach_trip_structure(vt)\n\n    trip_stats = summarise_trip_structure(vt_trips)\n    haz_stats = fit_hazard_trip(vt_trips)\n    pv_stats = fit_pv_trip(vt_trips)\n\n    all_stats = {\n        \"uid\": uid,\n        **trip_stats,\n        **haz_stats,\n        **pv_stats,\n    }\n    return pd.Series(all_stats)\n\n\n# run over all long users and build summary_df\nsummary_rows = []\nfor uid in LONG_USERS:\n    s = summarise_user_trip_behaviour(uid)\n    if s is not None:\n        summary_rows.append(s)\n\nsummary_df = pd.DataFrame(summary_rows).sort_values(\"uid\").reset_index(drop=True)\nsummary_df\n\n\n\n\n\n\n\n\n\nuid\nn_days_total\nn_days_nonhome\nn_trips\nmean_stops_per_trip\nmedian_stops_per_trip\nmax_stops_per_trip\nhazard_h1\nhazard_h2_const\nhazard_beta_k\noverall_pv_share\npv_odds_ratio_per_stop\n\n\n\n\n0\n000\n115\n108\n429\n3.412587\n1.0\n98\n0.657343\n0.137970\n-0.092337\n0.180704\n1.266709\n\n\n1\n002\n110\n108\n146\n5.794521\n4.5\n30\n0.171233\n0.147143\n0.039253\n0.203310\n1.131430\n\n\n2\n003\n220\n218\n327\n8.284404\n6.0\n50\n0.131498\n0.068045\n-0.029529\n0.122688\n1.093674\n\n\n3\n004\n242\n242\n532\n4.748120\n2.0\n33\n0.218045\n0.132663\n-0.092148\n0.113799\n1.108161\n\n\n4\n014\n118\n117\n162\n3.833333\n3.0\n14\n0.111111\n0.115468\n-0.209144\n0.227053\n1.060749\n\n\n5\n017\n187\n187\n335\n5.776119\n4.0\n28\n0.125373\n0.088125\n-0.046627\n0.193798\n1.022117\n\n\n6\n022\n120\n116\n193\n5.839378\n3.0\n28\n0.207254\n0.064240\n-0.099730\n0.167702\n1.026406\n\n\n7\n025\n143\n135\n245\n5.028571\n4.0\n29\n0.191837\n0.177305\n0.049723\n0.191558\n1.110949\n\n\n8\n030\n179\n179\n285\n7.789474\n6.0\n41\n0.070175\n0.072025\n-0.069146\n0.098137\n1.068854\n\n\n9\n039\n140\n139\n163\n7.312883\n6.0\n30\n0.079755\n0.083576\n0.012538\n0.215604\n1.067058\n\n\n10\n041\n137\n137\n287\n4.867596\n4.0\n29\n0.156794\n0.168468\n-0.002936\n0.350036\n0.999954\n\n\n11\n052\n96\n94\n120\n8.175000\n7.0\n31\n0.058333\n0.079070\n0.013165\n0.193878\n1.026394\n\n\n12\n062\n140\n138\n234\n4.286325\n3.0\n32\n0.243590\n0.177314\n-0.047536\n0.240759\n1.113801\n\n\n13\n084\n138\n137\n194\n5.659794\n3.0\n35\n0.149485\n0.090100\n-0.121527\n0.305581\n1.062512\n\n\n14\n092\n107\n106\n135\n3.170370\n2.0\n25\n0.177778\n0.058020\n-0.442773\n0.228972\n1.247307\n\n\n15\n104\n102\n93\n117\n3.726496\n2.0\n15\n0.239316\n0.134796\n0.004196\n0.295872\n1.200896\n\n\n16\n112\n101\n81\n91\n3.912088\n2.0\n22\n0.395604\n0.166038\n-0.226320\n0.303371\n2.364714\n\n\n17\n126\n163\n154\n199\n5.587940\n4.0\n29\n0.090452\n0.067908\n0.025523\n0.231115\n1.066412",
    "crumbs": [
      "Analysis",
      "Notebook 3 – Cross-user verification"
    ]
  },
  {
    "objectID": "verification.html#cross-user-trip-level-going-home-hazards",
    "href": "verification.html#cross-user-trip-level-going-home-hazards",
    "title": "Notebook 3 — Cross-user verification of daily exploration patterns",
    "section": "3. Cross-user trip-level going-home hazards",
    "text": "3. Cross-user trip-level going-home hazards\nIn this section, I treat “going home during a trip” as a discrete-time stopping process and estimate a separate going-home hazard curve for each user. For each stop order k, I count (1) how many trips survive to at least stop k (the risk set) and (2) among those, how many return HOME immediately after stop k. Their ratio yields an empirical hazard at each k. To smooth and compare patterns across users, I keep the first-stop hazard fixed at its empirical value—since it is typically much higher and reflects many short, errand-like trips—and fit a simple logit trend in k for later stops (k≥2). Overlaying the fitted curves highlights both level differences (some users are generally more likely to go HOME during trips) and shape differences (whether the going-home tendency declines, stabilizes, or rises as trips accumulate more stops). Finally, because the risk set becomes small at large k, the empirical hazards for long trips can be noisy; apparent “upward tails” at high stop orders may therefore reflect sparse data rather than a robust behavioral trend.\n\n\nShow code\nMAXK = 12\nMIN_RISK = 20\n\n\ndef compute_trip_hazard_curve(vt_trips: pd.DataFrame,\n                              maxk: int = MAXK,\n                              min_risk: int = MIN_RISK) -&gt; dict | None:\n    \"\"\"\n    Given a visit table with trip_id and action_order_in_trip,\n    compute:\n      - empirical hazard h_k\n      - fitted hazard curve via logit GLM\n    Returns dict with:\n      - k_axis, haz_emp, haz_hat, haz_df\n    \"\"\"\n    vis = vt_trips[\n        (vt_trips[\"is_home\"] == 0) &\n        (vt_trips[\"trip_id\"] &gt; 0) &\n        (vt_trips[\"action_order_in_trip\"] &gt; 0) &\n        (vt_trips[\"action_order_in_trip\"] &lt;= maxk)\n    ].copy()\n\n    if vis.empty:\n        return None\n\n    vis[\"outcome\"] = np.where(\n        vis[\"next_step\"] == \"home\", \"home\",\n        np.where(vis[\"next_step\"].isin([\"sw\", \"pv\", \"pn\"]), \"explore\", \"end\")\n    )\n\n    last_mask = vis.groupby(\n        [\"person\", \"date\", \"trip_id\"]\n    ).cumcount(ascending=False).eq(0)\n\n    NIGHT_HOUR = 23\n    NEAR_HOME_M = 750\n\n    last_time_dt = pd.to_datetime(\n        vis[\"last_time\"], format=\"%H:%M:%S\", errors=\"coerce\"\n    )\n    late = last_time_dt.dt.hour.ge(NIGHT_HOUR)\n    near = vis[\"dist_home_m\"].le(NEAR_HOME_M)\n\n    vis.loc[\n        last_mask & (vis[\"outcome\"] == \"end\") & (late | near),\n        \"outcome\"\n    ] = \"home\"\n\n    pertrip_maxk = (\n        vis.groupby([\"person\", \"date\", \"trip_id\"])[\"action_order_in_trip\"]\n           .max()\n    )\n\n    at_risk = pd.Series(\n        {k: int((pertrip_maxk &gt;= k).sum()) for k in range(1, maxk + 1)},\n        name=\"at_risk\"\n    )\n\n    home_k = (\n        vis[vis[\"outcome\"] == \"home\"]\n        .groupby(\"action_order_in_trip\")\n        .size()\n        .reindex(range(1, maxk + 1), fill_value=0)\n        .rename(\"home_k\")\n    )\n\n    hazard = (home_k / at_risk.replace(0, np.nan)).rename(\"hazard\")\n    haz_df = pd.concat([home_k, at_risk, hazard], axis=1)\n\n    k_axis = np.arange(1, maxk + 1)\n    haz_emp = hazard.reindex(k_axis).to_numpy()\n\n    if np.all(np.isnan(haz_emp)):\n        return None\n\n    # logit fit on k &gt;= 2\n    haz_fit = (\n        haz_df\n        .loc[haz_df.index &gt;= 2]\n        .loc[lambda df: df[\"at_risk\"] &gt;= min_risk]\n        .copy()\n    )\n\n    if haz_fit.empty:\n        haz_hat = haz_emp.copy()\n    else:\n        k_fit = haz_fit.index.values.astype(float)\n        X = sm.add_constant(k_fit)\n        y = np.column_stack([\n            haz_fit[\"home_k\"].values,\n            (haz_fit[\"at_risk\"] - haz_fit[\"home_k\"]).values\n        ])\n        glm = sm.GLM(y, X, family=sm.families.Binomial())\n        res_haz = glm.fit()\n\n        alpha, beta = res_haz.params[0], res_haz.params[1]\n        X_pred = sm.add_constant(k_axis[1:])\n        haz_hat_tail = res_haz.predict(X_pred)\n        h1_emp = haz_df.loc[1, \"hazard\"]\n        haz_hat = np.concatenate([[h1_emp], haz_hat_tail])\n\n    return {\n        \"k_axis\": k_axis,\n        \"haz_emp\": haz_emp,\n        \"haz_hat\": haz_hat,\n        \"haz_df\": haz_df,\n    }\n\n\n# compute hazard curves for all users\nhazard_curves = {}\n\nfor uid in summary_df[\"uid\"]:\n    vt = load_visit_table_from_csv(uid, verbose=False)\n    if vt is None or vt.empty:\n        continue\n    vt_trips = attach_trip_structure(vt)\n    curve = compute_trip_hazard_curve(vt_trips, maxk=MAXK, min_risk=MIN_RISK)\n    if curve is not None:\n        hazard_curves[uid] = curve\n\nprint(\"Users with usable hazard curves:\", len(hazard_curves))\n\n\n# overlay fitted hazards across users\nplt.figure(figsize=(8, 5))\n\nfor uid, hc in hazard_curves.items():\n    plt.plot(hc[\"k_axis\"], hc[\"haz_hat\"], alpha=0.6, linewidth=1, label=uid)\n\nplt.xlabel(\"Stop order k in trip\")\nplt.ylabel(\"P(go home | trip has reached k)\")\nplt.ylim(0, 1)\nplt.xlim(1, MAXK)\nplt.title(\"Trip-level going-home hazard — fitted curves across users\")\nplt.legend(ncol=3, fontsize=8, frameon=False)\nplt.tight_layout()\nplt.show()\n\n\nUsers with usable hazard curves: 18",
    "crumbs": [
      "Analysis",
      "Notebook 3 – Cross-user verification"
    ]
  },
  {
    "objectID": "verification.html#cross-user-within-trip-exploration-pv",
    "href": "verification.html#cross-user-within-trip-exploration-pv",
    "title": "Notebook 3 — Cross-user verification of daily exploration patterns",
    "section": "4. Cross-user within-trip exploration (Pv)",
    "text": "4. Cross-user within-trip exploration (Pv)\nThis section summarizes exploration behaviour across users using the Pv logit model. For each user, I restrict to non-home stops within trips and, by stop order k, count how many stops are first-time places (Pv) versus revisits (Pn). I fit a user-specific logit model where stop order enters linearly (after recentring), producing a smooth fitted curve for the probability that a stop is Pv.\nI overlay these fitted curves across users (truncated to the first ten non-home stops, and only shown where each stop order has enough observations). Vertical differences reflect baseline exploratory tendency, while curve slope reflects how exploration changes as a trip progresses. Many users show gently increasing exploration with stop order, while some have near-flat curves consistent with a roughly constant exploration rate.\n\n\nShow code\nMAXK_PV = 12      # maximum stop order for Pv curves\nMIN_N_PV = 20     # minimum n per k for fitting / error\n\n\ndef compute_pv_curve(vt_trips: pd.DataFrame,\n                     maxk: int = MAXK_PV,\n                     min_n: int = MIN_N_PV) -&gt; dict | None:\n    \"\"\"\n    Given a visit table with trip_id and action_order_in_trip,\n    compute:\n      - empirical Pv share by stop order k\n      - fitted Pv probability via logit in k\n    Returns dict with:\n      - k_axis, pv_emp, pv_hat, pv_df\n    \"\"\"\n    vis_places = vt_trips[\n        (vt_trips[\"is_home\"] == 0) &\n        (vt_trips[\"trip_id\"] &gt; 0) &\n        (vt_trips[\"action_order_in_trip\"] &gt; 0)\n    ].copy()\n    vis_places = vis_places[vis_places[\"action_order_in_trip\"] &lt;= maxk].copy()\n\n    if vis_places.empty:\n        return None\n\n    pv_by_k = (\n        vis_places\n        .groupby(\"action_order_in_trip\")[[\"is_pv\", \"is_pn\"]]\n        .agg(\n            pv_cnt=(\"is_pv\", \"sum\"),\n            pn_cnt=(\"is_pn\", \"sum\"),\n            n=(\"is_pv\", \"size\")\n        )\n    )\n\n    pv_by_k = pv_by_k.reindex(range(1, maxk + 1), fill_value=0)\n\n    denom = pv_by_k[\"pv_cnt\"] + pv_by_k[\"pn_cnt\"]\n    pv_emp = (pv_by_k[\"pv_cnt\"] / denom.where(denom &gt; 0, np.nan)).to_numpy()\n\n    k_axis = np.arange(1, maxk + 1)\n\n    valid_k = pv_by_k.index[pv_by_k[\"n\"] &gt;= min_n]\n    if len(valid_k) == 0:\n        return {\n            \"k_axis\": k_axis,\n            \"pv_emp\": pv_emp,\n            \"pv_hat\": pv_emp.copy(),\n            \"pv_df\": pv_by_k.reset_index().rename(columns={\"index\": \"k\"})\n        }\n\n    vis_lr_sub = vis_places[\n        vis_places[\"action_order_in_trip\"].isin(valid_k)\n    ].copy()\n\n    mean_k = vis_lr_sub[\"action_order_in_trip\"].mean()\n    vis_lr_sub[\"k_centered\"] = vis_lr_sub[\"action_order_in_trip\"] - mean_k\n\n    X = sm.add_constant(vis_lr_sub[\"k_centered\"])\n    y = vis_lr_sub[\"is_pv\"]\n\n    logit_model = sm.Logit(y, X)\n    res = logit_model.fit(disp=False)\n\n    beta0 = res.params[\"const\"]\n    beta1 = res.params[\"k_centered\"]\n\n    k_centered_grid = k_axis - mean_k\n    lin_pred = beta0 + beta1 * k_centered_grid\n    pv_hat = 1.0 / (1.0 + np.exp(-lin_pred))\n\n    pv_df = pv_by_k.reset_index().rename(columns={\"index\": \"k\"})\n\n    return {\n        \"k_axis\": k_axis,\n        \"pv_emp\": pv_emp,\n        \"pv_hat\": pv_hat,\n        \"pv_df\": pv_df,\n    }\n\n\n# compute Pv curves for all users\npv_curves = {}\n\nfor uid in summary_df[\"uid\"]:\n    vt = load_visit_table_from_csv(uid, verbose=False)\n    if vt is None or vt.empty:\n        continue\n\n    vt_trips = attach_trip_structure(vt)\n    curve = compute_pv_curve(vt_trips, maxk=MAXK_PV, min_n=MIN_N_PV)\n    if curve is not None:\n        pv_curves[uid] = curve\n\nprint(\"Users with usable Pv curves:\", len(pv_curves))\n\n\n# overlay fitted Pv curves across users\nplt.figure(figsize=(8, 5))\n\nfor uid, curve in pv_curves.items():\n    k_axis = curve[\"k_axis\"]\n    pv_hat = curve[\"pv_hat\"]\n    n_k = curve[\"pv_df\"][\"n\"].to_numpy()\n\n    mask_fit = n_k &gt;= MIN_N_PV\n    if not mask_fit.any():\n        continue\n\n    plt.plot(\n        k_axis[mask_fit],\n        pv_hat[mask_fit],\n        alpha=0.6,\n        linewidth=1,\n        label=uid\n    )\n\nplt.ylim(0, 0.5)\nplt.xlim(1, MAXK_PV)\nplt.xlabel(\"Stop order k in trip (non-home)\")\nplt.ylabel(\"P(stop is Pv)\")\nplt.title(\"Within-trip Pv probability — fitted curves across users\")\nplt.legend(ncol=3, fontsize=8, frameon=False)\nplt.tight_layout()\nplt.show()\n\n\nUsers with usable Pv curves: 18",
    "crumbs": [
      "Analysis",
      "Notebook 3 – Cross-user verification"
    ]
  },
  {
    "objectID": "verification.html#model-fit-diagnostics-for-hazard-and-pv-curves",
    "href": "verification.html#model-fit-diagnostics-for-hazard-and-pv-curves",
    "title": "Notebook 3 — Cross-user verification of daily exploration patterns",
    "section": "5. Model fit diagnostics for hazard and Pv curves",
    "text": "5. Model fit diagnostics for hazard and Pv curves\nTo evaluate how well the simple logit specifications approximate the empirical profiles, I compute a per-user mean squared error (MSE) between empirical probabilities and fitted values:\n\nFor going-home hazards, MSE is computed over all stop orders with non-missing hazards.\nFor Pv exploration, MSE is computed only over stop orders meeting the minimum sample-size threshold used for fitting.\n\nThese MSE values are merged back into the user summary table to provide a compact goodness-of-fit diagnostic. In general, the largest deviations occur at high stop orders where long trips are rare and empirical estimates are noisy.\n\n\nShow code\n# === 5.0 Compute per-user MSE for hazard and Pv ===\n\n# 5.0.1 Hazard MSE for each user\nhaz_fit_rows = []\nfor uid, hc in hazard_curves.items():\n    emp = hc[\"haz_emp\"]      # empirical hazard h_k\n    hat = hc[\"haz_hat\"]      # fitted hazard \\hat{h}_k\n\n    mask = ~np.isnan(emp)\n    if not mask.any():\n        continue\n\n    mse = np.mean((emp[mask] - hat[mask])**2)\n    haz_fit_rows.append({\n        \"uid\": uid,\n        \"hazard_mse\": mse,\n    })\n\nhaz_fit_df = (\n    pd.DataFrame(haz_fit_rows)\n      .sort_values(\"uid\")\n      .reset_index(drop=True)\n)\n\n\n# 5.0.2 Pv MSE for each user\npv_fit_rows = []\nfor uid, curve in pv_curves.items():\n    emp = curve[\"pv_emp\"]    # empirical Pv share by k\n    hat = curve[\"pv_hat\"]    # fitted Pv probability by k\n    n_k = curve[\"pv_df\"][\"n\"].to_numpy()  # sample size per k\n\n    mask = (~np.isnan(emp)) & (n_k &gt;= MIN_N_PV)\n    if not mask.any():\n        continue\n\n    mse = np.mean((emp[mask] - hat[mask])**2)\n    pv_fit_rows.append({\n        \"uid\": uid,\n        \"pv_mse\": mse,\n    })\n\npv_fit_df = (\n    pd.DataFrame(pv_fit_rows)\n      .sort_values(\"uid\")\n      .reset_index(drop=True)\n)\n\n\n\n\nShow code\n# 5.1 Hazard MSE summary\nh_min   = haz_fit_df[\"hazard_mse\"].min()\nh_med   = haz_fit_df[\"hazard_mse\"].median()\nh_max   = haz_fit_df[\"hazard_mse\"].max()\n\nbest_haz  = haz_fit_df.nsmallest(2, \"hazard_mse\")\nworst_haz = haz_fit_df.nlargest(2, \"hazard_mse\")\n\nprint(f\"Hazard MSE range: {h_min:.4f} – {h_max:.4f}, median {h_med:.4f}\")\nprint(\"Best hazard fits:\")\nprint(best_haz.to_string(index=False))\n\nprint(\"Worst hazard fits:\")\nprint(worst_haz.to_string(index=False))\n\n\n# 5.2 Pv MSE summary\np_min   = pv_fit_df[\"pv_mse\"].min()\np_med   = pv_fit_df[\"pv_mse\"].median()\np_max   = pv_fit_df[\"pv_mse\"].max()\n\nbest_pv  = pv_fit_df.nsmallest(2, \"pv_mse\")\nworst_pv = pv_fit_df.nlargest(2, \"pv_mse\")\n\nprint(f\"\\nPv MSE range: {p_min:.4f} – {p_max:.4f}, median {p_med:.4f}\")\nprint(\"Best Pv fits:\")\nprint(best_pv.to_string(index=False))\n\nprint(\"Worst Pv fits:\")\nprint(worst_pv.to_string(index=False))\n\n# 5.3 Merge fit stats into summary_df and save\nsummary_fit = (\n    summary_df\n    .merge(haz_fit_df, on=\"uid\", how=\"left\")\n    .merge(pv_fit_df,  on=\"uid\", how=\"left\")\n)\nsummary_fit.to_csv(\"data/user_trip_summary_with_fit.csv\", index=False)\n\n\nHazard MSE range: 0.0002 – 0.0138, median 0.0013\nBest hazard fits:\nuid  hazard_mse\n003    0.000184\n030    0.000259\nWorst hazard fits:\nuid  hazard_mse\n112    0.013826\n104    0.009189\n\nPv MSE range: 0.0008 – 0.0091, median 0.0022\nBest Pv fits:\nuid   pv_mse\n030 0.000807\n084 0.001045\nWorst Pv fits:\nuid   pv_mse\n112 0.009124\n041 0.003860\n\n\n\n5.a Hazard fit: best and worst users\nFigure 5a compares the empirical trip-level going-home hazard with the fitted logit curves for the two best-fitting users (003, 030) and the two worst-fitting users (112, 104). For Users 003 and 030, the points lie almost exactly on the fitted line across stop orders 1–12, and the MSE is essentially zero, which means the simple logit trend in k captures their going-home probabilities very well. For Users 112 and 104 the overall MSE is still small (≈0.014 and ≈0.009), but the last few stops show visible deviations: with few long trips, individual “go home” events at high k create noisy empirical hazards that the smooth logit curve only approximates.\n\n\nShow code\n# 5.a Hazard fit: 2×2 panels for best and worst users\n\nhaz_best  = [\"003\", \"030\"]\nhaz_worst = [\"112\", \"104\"]\nhaz_order = haz_best + haz_worst   # top row: best, bottom row: worst\n\nhaz_mse_dict = haz_fit_df.set_index(\"uid\")[\"hazard_mse\"].to_dict()\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 6), sharex=True, sharey=True)\n\nfor ax, uid in zip(axes.ravel(), haz_order):\n    hc  = hazard_curves[uid]\n    k   = hc[\"k_axis\"]\n    emp = hc[\"haz_emp\"]\n    hat = hc[\"haz_hat\"]\n\n    mask_emp = ~np.isnan(emp)\n\n    ax.scatter(k[mask_emp], emp[mask_emp], s=25, label=\"Empirical hazard\")\n    ax.plot(k, hat, label=\"Fitted hazard\")\n\n    ax.set_title(f\"User {uid} (MSE={haz_mse_dict[uid]:.3f})\")\n    ax.set_xlim(1, MAXK)\n    ax.set_ylim(0, 1)\n\naxes[0, 0].legend()\n\nfig.suptitle(\"Trip-level going-home hazard — best and worst fits\", y=0.98)\nfig.text(0.5, 0.04, \"Stop order k in trip\", ha=\"center\")\nfig.text(0.04, 0.5, \"P(go home | trip has reached k)\", va=\"center\", rotation=\"vertical\")\n\nplt.tight_layout(rect=[0.06, 0.06, 1, 0.94])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n5.b Pv fit: best and worst users\nFigure 5b repeats the same exercise for within-trip exploration. For Users 030 and 084 (best fits), the fitted Pv probability is almost indistinguishable from the empirical Pv share for k up to 10, and the MSE stays around 0.001, indicating that a linear logit trend in stop order is an adequate summary of how exploration evolves within their trips. For Users 112 and 041 (worst fits), the fitted line still gets the general level and slope roughly right, but the empirical points at higher k fluctuate more strongly (especially for 112), reflecting sparse data for long trips rather than a systematic failure of the model.\n\n\nShow code\n# 5.b Pv fit: 2×2 panels for best and worst users\n\npv_best  = [\"030\", \"084\"]\npv_worst = [\"112\", \"041\"]\npv_order = pv_best + pv_worst\n\npv_mse_dict = pv_fit_df.set_index(\"uid\")[\"pv_mse\"].to_dict()\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 6), sharex=True, sharey=True)\n\nfor ax, uid in zip(axes.ravel(), pv_order):\n    curve = pv_curves[uid]\n    k     = curve[\"k_axis\"]\n    emp   = curve[\"pv_emp\"]\n    hat   = curve[\"pv_hat\"]\n    n_k   = curve[\"pv_df\"][\"n\"].to_numpy()\n\n    mask_emp = ~np.isnan(emp)\n    mask_fit = (n_k &gt;= MIN_N_PV)\n\n    ax.scatter(k[mask_emp], emp[mask_emp], s=25, label=\"Empirical Pv share\")\n    ax.plot(k[mask_fit], hat[mask_fit], label=\"Fitted Pv probability\")\n\n    ax.set_title(f\"User {uid} (MSE={pv_mse_dict[uid]:.3f})\")\n    ax.set_xlim(1, MAXK_PV)\n    ax.set_ylim(0, 0.5)\n\naxes[0, 0].legend()\n\nfig.suptitle(\"Within-trip Pv probability — best and worst fits\", y=0.98)\nfig.text(0.5, 0.04, \"Stop order k in trip (non-home)\", ha=\"center\")\nfig.text(0.04, 0.5, \"P(stop is Pv)\", va=\"center\", rotation=\"vertical\")\n\nplt.tight_layout(rect=[0.06, 0.06, 1, 0.94])\nplt.show()",
    "crumbs": [
      "Analysis",
      "Notebook 3 – Cross-user verification"
    ]
  },
  {
    "objectID": "verification.html#user-typology-in-multivariate-trip-space",
    "href": "verification.html#user-typology-in-multivariate-trip-space",
    "title": "Notebook 3 — Cross-user verification of daily exploration patterns",
    "section": "6. User typology in multivariate trip space",
    "text": "6. User typology in multivariate trip space\nFor the selected long-coverage users, I build a multi-dimensional mobility profile capturing trip frequency, trip complexity, going-home behaviour, and within-trip exploration. After standardising these features, I apply k-means clustering (k = 3) to identify broad user types:\n\n\nShow code\nfeat_cols = [\n    \"mean_stops_per_trip\",     \n    \"median_stops_per_trip\",\n    \"hazard_h1\",              \n    \"hazard_h2_const\",        \n    \"hazard_beta_k\",          \n    \"overall_pv_share\",       \n    \"pv_odds_ratio_per_stop\",  \n    \"n_trips\",                \n    \"n_days_nonhome\",        \n]\n\nX = summary_fit[feat_cols].dropna()\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nkmeans = KMeans(n_clusters=3, random_state=0)\nsummary_fit[\"cluster3\"] = kmeans.fit_predict(X_scaled)\n\n\npca = PCA(n_components=3)\nPCs = pca.fit_transform(X_scaled)\nsummary_fit[[\"PC1\", \"PC2\", \"PC3\"]] = PCs\n\nloadings = pd.DataFrame(\n    pca.components_.T,\n    index=feat_cols,\n    columns=[\"PC1\", \"PC2\", \"PC3\"]\n)\ndisplay(loadings.round(2))\n\n\nc:\\Users\\Jingqi\\anaconda3\\envs\\geospatial\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\n\n\n\n\nmean_stops_per_trip\n0.46\n-0.16\n0.06\n\n\nmedian_stops_per_trip\n0.43\n-0.29\n0.14\n\n\nhazard_h1\n-0.36\n0.33\n0.08\n\n\nhazard_h2_const\n-0.29\n0.14\n0.62\n\n\nhazard_beta_k\n0.25\n-0.01\n0.71\n\n\noverall_pv_share\n-0.31\n-0.35\n0.25\n\n\npv_odds_ratio_per_stop\n-0.34\n-0.07\n-0.09\n\n\nn_trips\n0.12\n0.66\n0.07\n\n\nn_days_nonhome\n0.32\n0.45\n-0.05\n\n\n\n\n\n\n\n\n\nShow code\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D \n\nfig = plt.figure(figsize=(7, 6))\nax = fig.add_subplot(111, projection=\"3d\")\n\ncolors = {0: \"tab:orange\", 1: \"tab:blue\", 2: \"tab:green\"}\n\nfor c in sorted(summary_fit[\"cluster3\"].unique()):\n    sub = summary_fit[summary_fit[\"cluster3\"] == c]\n    ax.scatter(\n        sub[\"PC1\"], sub[\"PC2\"], sub[\"PC3\"],\n        c=colors[c], label=f\"Cluster {c}\", s=40\n    )\n    for _, row in sub.iterrows():\n        ax.text(\n            row[\"PC1\"], row[\"PC2\"], row[\"PC3\"],\n            row[\"uid\"],\n            fontsize=8\n        )\n\nax.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.0%})\")\nax.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.0%})\")\nax.set_zlabel(f\"PC3 ({pca.explained_variance_ratio_[2]:.0%})\")\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCluster 0 (orange) — multi-stop revisitors These users make longer, multi-stop trips and often string together several non-home stops in one outing. Their overall going-out frequency is fairly normal (not especially high or low). Once they are out, their chance of returning home at later stops tends to stay low, so trips look like extended chains of mostly familiar places.\nCluster 1 (blue) — errand-homebodies These users go out more frequently, but their trips are generally short to medium in length. A defining feature is that they often return home quickly—many outings end after the first one or two non-home stops. Overall, their mobility looks like frequent, errand-style trips that wrap up fast.\nCluster 2 (green) — exploratory roamers These users go out less often and their trips are shorter on average, but their behaviour while out is more exploratory. If they don’t return home immediately, they tend to keep going rather than heading back, with a low tendency to go home at later stops. In practice, this looks like fewer outings, but relatively more exploration when outings happen.",
    "crumbs": [
      "Analysis",
      "Notebook 3 – Cross-user verification"
    ]
  },
  {
    "objectID": "LR.html",
    "href": "LR.html",
    "title": "Project Overview",
    "section": "",
    "text": "This project analyzes high-resolution Geolife GPS trajectories by representing everyday mobility as a sequence of discrete visit events linked into home-based trips. Each non-home stop is labeled as either a first-time place or a return place to quantify exploration versus revisitation within and across trips. Using these labeled sequences, I estimate how the likelihood of returning home and the propensity to explore evolve with stop order and compare these patterns across users.",
    "crumbs": [
      "Analysis",
      "Notebook 0 - Overview"
    ]
  },
  {
    "objectID": "LR.html#what-is-epr",
    "href": "LR.html#what-is-epr",
    "title": "Project Overview",
    "section": "1. What is EPR?",
    "text": "1. What is EPR?\nWe examine everyday movement as a balance between revisiting familiar places and exploring new ones. In behavioral terms, most people follow habitual travel routines (e.g., home–work–home) yet occasionally deviate to novel destinations. This “exploit vs. explore” trade-off can be framed by an Exploration–Preferencing Ratio (EPR): a higher EPR means more exploration of new sites relative to returning to known ones. Conceptually, EPR is analogous to the exploration-and-preferential-return model in human mobility research, in which at each move an agent either visits a new location or returns to a past one (Pappalardo, Rinzivillo, & Simini, 2016; Song, Koren, Wang, & Barabási, 2010). Large-scale visitation patterns have been shown to arise from EPR-like dynamics in empirical data (Schläpfer et al., 2021; Song, Qu, Blumm, & Barabási, 2010).\nIn mobility data, we treat EPR operationally as the ratio of novel stops to repeated stops. For example, if a trip includes three previously unvisited stops (exploration) and one repeated stop, the person’s trip-level EPR would be 3:1, indicating exploratory behavior. In our context, we adapt this idea to individual GPS trajectories by explicitly labelling each stop as either “Pv” (previously visited) or “Pn” (novel), and then modeling the patterns of Pv/Pn occurrences within trips.\nIn our data, we operationalize this by labelling each non-home stop as Pn (novel) if it is the first time the user has stopped at that particular location, or Pv (visited) if it falls at a location the user has visited before. Over a trip (home → … → home), we then summarize the trip’s exploratory tendency by, for example, the count of Pn stops or the ratio Pn/(Pv + Pn). A person’s overall EPR can be aggregated from their trips (e.g., average per-trip exploration rate).\nThis trip-focused EPR differs from classic probabilistic models (e.g., the EPR model of Song et al., 2010) in that we measure empirical behavior rather than impose a fixed probability of exploration (Song et al., 2010). Prior literature has examined related metrics: Pappalardo et al. (2015), for instance, show that individuals cluster into “explorers” (many new locations) or “returners” (few new locations) based on visit-count ratios. Our approach is similar in spirit but works at the granularity of trips and discrete stops.\nWe also draw on ecological ideas of foraging: just as people navigating information maximize an information-gain rate by choosing to explore “new patches” only when the expected gain outweighs the cost (Pirolli & Card, 1999; Nielsen, 2019), travelers may implicitly weigh the novelty of a potential stop against its travel or time cost. We do not explicitly model that decision rule, but the EPR encapsulates its outcome. In short, EPR is the key behavioral concept linking individual choices of revisiting vs. exploring, and our goal is to measure it from trajectory data.",
    "crumbs": [
      "Analysis",
      "Notebook 0 - Overview"
    ]
  },
  {
    "objectID": "LR.html#from-epr-concept-to-empirical-patterns",
    "href": "LR.html#from-epr-concept-to-empirical-patterns",
    "title": "Project Overview",
    "section": "2. From EPR Concept to Empirical Patterns",
    "text": "2. From EPR Concept to Empirical Patterns\nBuilding on the EPR idea, the remainder of the project focuses on how exploration and returning behaviour unfold within trips and how these patterns vary across individuals. Rather than treating mobility purely as a set of locations, I treat it as an ordered sequence of stops and decisions, which makes it possible to ask step-by-step questions such as: When do trips tend to end? and At what point in a trip do new places become more likely?\nAt the trip level, I summarize outings by their length (number of non-home stops) and spatial reach (distance from HOME), and then examine the next-step dynamics after each stop. This enables a clear separation between short, errand-like trips (often ending quickly) and longer outings in which individuals remain out and continue chaining stops. In parallel, I quantify exploration by tracking the share of first-time stops within trips and how that share changes as a trip progresses.\nFinally, the analysis scales up to a cross-user setting. For a set of long-coverage users, I compute comparable trip and exploration summaries and estimate user-specific parameters that capture both (i) overall levels of going home and exploration and (ii) how these tendencies change with stop order. These user-level profiles support systematic cross-user comparisons and allow the identification of broad behavioural types (e.g., frequent errand trips versus fewer but more exploratory outings).",
    "crumbs": [
      "Analysis",
      "Notebook 0 - Overview"
    ]
  },
  {
    "objectID": "Data Cleaning.html",
    "href": "Data Cleaning.html",
    "title": "Notebook 1 — From raw Geolife trajectories to visit-level events",
    "section": "",
    "text": "This notebook shows how raw Geolife GPS trajectories for one user are turned into a cleaned visit-level dataset. I load and filter the original .plt files, restrict the data to Beijing, and project all points onto a 500 m grid. Night-time points are used to infer the user’s home (HOME) and a secondary frequently visited place (SW), and a simple movement-based rule selects additional grid cells as daily activity locations. Consecutive points within the same activity cell are then collapsed into single visit events and labelled as HOME, SW, first-time place (Pv), or return place (Pn). The resulting visit-level table is the starting point for all subsequent analyses.",
    "crumbs": [
      "Analysis",
      "Notebook 1 – Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#environment-and-data-paths",
    "href": "Data Cleaning.html#environment-and-data-paths",
    "title": "Notebook 1 — From raw Geolife trajectories to visit-level events",
    "section": "1. Environment and data paths",
    "text": "1. Environment and data paths\nThe Geolife data folder Geolife Trajectories 1.3 is stored in the same directory as this notebook. Below I set the base directory and import the Python packages used throughout the analysis.\n\n\nShow code\nimport os\nimport glob\nimport math\n\nimport numpy as np\nimport pandas as pd\nimport pyproj\n\n# Path to the Geolife data (relative to the notebook)\nBASE_DIR = os.path.join(\"Geolife Trajectories 1.3\", \"Data\")\n\n# Example user (we start with user 000)\nUSER_ID = \"000\"\n\n# Optional: restrict to Beijing area\nFILTER_BEIJING = True\nBEIJING_BBOX = (115.42, 39.44, 117.50, 41.06)   # (minLon, minLat, maxLon, maxLat)\n\n# Column names for Geolife .plt files (after skipping the 6-line header)\nPLT_COLS = [\"lat\", \"lon\", \"unused\", \"altitude_feet\", \"days\", \"date\", \"time\"]",
    "crumbs": [
      "Analysis",
      "Notebook 1 – Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#inspecting-raw-gps-trajectories",
    "href": "Data Cleaning.html#inspecting-raw-gps-trajectories",
    "title": "Notebook 1 — From raw Geolife trajectories to visit-level events",
    "section": "2. Inspecting raw GPS trajectories",
    "text": "2. Inspecting raw GPS trajectories\nBefore any spatial filtering or grid construction, the raw Geolife records for user 000 are inspected. Table 1 previews the cleaned point-level data: each row corresponds to a single GPS fix with a timestamp, derived calendar date and hour, latitude/longitude, and altitude.\nThe spatial footprint of these records is visualised by plotting the raw trajectories as daily polylines in geographic coordinates (Fig. 2). Connecting consecutive points into lines highlights dense clusters of movement as well as the long-distance corridor between Beijing and Shanghai, and motivates the later projection to UTM and discretisation onto a 500 m grid.\nTemporal coverage is assessed by aggregating the data by date and counting the number of recorded GPS points per day (Fig. 3). The resulting time series clearly shows highly irregular tracking: some days contain several thousand points, others only a few, and there are extended gaps with no data at all. This pattern reflects the device being switched on only intermittently and at changing sampling rates, a key limitation that subsequent behavioural analyses must be interpreted conditional on.\n\n\nShow code\ndef read_one_plt(path, user_id=\"000\"):\n    \"\"\"Read a single Geolife .plt file, clean basic issues, and return a DataFrame.\"\"\"\n    df = pd.read_csv(path, skiprows=6, names=PLT_COLS)\n\n    # Basic cleaning: drop rows with missing coords or time\n    df = df[\n        pd.notnull(df[\"lat\"]) &\n        pd.notnull(df[\"lon\"]) &\n        pd.notnull(df[\"date\"]) &\n        pd.notnull(df[\"time\"])\n    ].copy()\n\n    # Build timestamp\n    df[\"datetime\"] = pd.to_datetime(\n        df[\"date\"].astype(str) + \" \" + df[\"time\"].astype(str),\n        errors=\"coerce\"\n    )\n    df = df[pd.notnull(df[\"datetime\"])].copy()\n\n    # Derive date and hour\n    df[\"user\"] = user_id\n    df[\"file\"] = os.path.basename(path)\n    df[\"date_only\"] = df[\"datetime\"].dt.date\n    df[\"hour\"] = df[\"datetime\"].dt.hour\n\n    # Keep core columns\n    df = df[[\"user\", \"file\", \"datetime\", \"date_only\", \"hour\",\n             \"lat\", \"lon\", \"altitude_feet\"]]\n    return df\n\n\n# Scan all trajectory files for the chosen user\ntraj_glob = os.path.join(BASE_DIR, USER_ID, \"Trajectory\", \"*.plt\")\nfiles = sorted(glob.glob(traj_glob))\n\n\n\n\nShow code\ndfs = []\nfor fp in files:\n    try:\n        dfs.append(read_one_plt(fp, user_id=USER_ID))\n    except Exception as e:\n        print(f\"[warning] failed to read {fp}: {e}\")\n\nif not dfs:\n    raise RuntimeError(\"No trajectories were successfully read. Check the data path.\")\n\ntraj = pd.concat(dfs, ignore_index=True)\ntraj.sort_values(\"datetime\", inplace=True)\ntraj.reset_index(drop=True, inplace=True)\ntraj.head(10)\n\n\n\n\n\n\n\n\n\nuser\nfile\ndatetime\ndate_only\nhour\nlat\nlon\naltitude_feet\n\n\n\n\n0\n000\n20081023025304.plt\n2008-10-23 02:53:04\n2008-10-23\n2\n39.984702\n116.318417\n492\n\n\n1\n000\n20081023025304.plt\n2008-10-23 02:53:10\n2008-10-23\n2\n39.984683\n116.318450\n492\n\n\n2\n000\n20081023025304.plt\n2008-10-23 02:53:15\n2008-10-23\n2\n39.984686\n116.318417\n492\n\n\n3\n000\n20081023025304.plt\n2008-10-23 02:53:20\n2008-10-23\n2\n39.984688\n116.318385\n492\n\n\n4\n000\n20081023025304.plt\n2008-10-23 02:53:25\n2008-10-23\n2\n39.984655\n116.318263\n492\n\n\n5\n000\n20081023025304.plt\n2008-10-23 02:53:30\n2008-10-23\n2\n39.984611\n116.318026\n493\n\n\n6\n000\n20081023025304.plt\n2008-10-23 02:53:35\n2008-10-23\n2\n39.984608\n116.317761\n493\n\n\n7\n000\n20081023025304.plt\n2008-10-23 02:53:40\n2008-10-23\n2\n39.984563\n116.317517\n496\n\n\n8\n000\n20081023025304.plt\n2008-10-23 02:53:45\n2008-10-23\n2\n39.984539\n116.317294\n500\n\n\n9\n000\n20081023025304.plt\n2008-10-23 02:53:50\n2008-10-23\n2\n39.984606\n116.317065\n505\n\n\n\n\n\n\n\n\n\nShow code\nimport folium\ntraj_for_map = traj.copy()\n\n# Center the map at the median lat/lon of all points\ncenter_lat = traj_for_map[\"lat\"].median()\ncenter_lon = traj_for_map[\"lon\"].median()\n\nm_raw = folium.Map(\n    location=[center_lat, center_lon],\n    zoom_start=8,\n    tiles=\"cartodbpositron\"\n)\n\n# Draw one polyline per day to show daily trajectories\nfor d, sub in traj_for_map.groupby(\"date_only\"):\n    sub = sub.sort_values(\"datetime\")\n    if len(sub) &lt; 2:\n        continue\n\n    coords = list(zip(sub[\"lat\"].values, sub[\"lon\"].values))  # (lat, lon) pairs\n    folium.PolyLine(\n        locations=coords,\n        weight=1,\n        opacity=0.4\n    ).add_to(m_raw)\n\nm_raw\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nShow code\n# Daily number of GPS points\nday_counts = (\n    traj\n    .groupby(\"date_only\")\n    .size()\n    .rename(\"n_points\")\n    .reset_index()\n)\n\n# Simple time-series style plot of counts per day\nday_counts_plot = day_counts.copy()\nday_counts_plot[\"date_only\"] = pd.to_datetime(day_counts_plot[\"date_only\"])\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(7, 3.5))\nplt.plot(\n    day_counts_plot[\"date_only\"],\n    day_counts_plot[\"n_points\"],\n    marker=\"o\",\n    linewidth=1\n)\nplt.xlabel(\"Date\")\nplt.ylabel(\"Number of GPS points\")\nplt.title(\"Daily number of recorded GPS points for user 000\")\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Analysis",
      "Notebook 1 – Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#spatial-filter-restricting-to-the-beijing-area",
    "href": "Data Cleaning.html#spatial-filter-restricting-to-the-beijing-area",
    "title": "Notebook 1 — From raw Geolife trajectories to visit-level events",
    "section": "3. Spatial filter: restricting to the Beijing area",
    "text": "3. Spatial filter: restricting to the Beijing area\nGeolife users may have trajectories in multiple cities. In this project we focus on the Beijing region. We therefore apply a simple bounding box filter on longitude and latitude.\n\n\nShow code\nif FILTER_BEIJING:\n    minLon, minLat, maxLon, maxLat = BEIJING_BBOX\n    before = len(traj)\n    traj = traj[\n        (traj[\"lon\"] &gt;= minLon) & (traj[\"lon\"] &lt;= maxLon) &\n        (traj[\"lat\"] &gt;= minLat) & (traj[\"lat\"] &lt;= maxLat)\n    ].copy()\n    after = len(traj)\n    print(f\"Beijing filter: {before} -&gt; {after} points\")\nelse:\n    print(\"Beijing filter is disabled.\")\n\n\nBeijing filter: 173870 -&gt; 157646 points",
    "crumbs": [
      "Analysis",
      "Notebook 1 – Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#projection-to-utm-and-construction-of-a-500-m-grid",
    "href": "Data Cleaning.html#projection-to-utm-and-construction-of-a-500-m-grid",
    "title": "Notebook 1 — From raw Geolife trajectories to visit-level events",
    "section": "4. Projection to UTM and construction of a 500 m grid",
    "text": "4. Projection to UTM and construction of a 500 m grid\nAll GPS points are transformed from WGS84 geographic coordinates (lat/lon) to UTM Zone 50N so that distances can be measured in metres. In this projected space, a regular 500 m × 500 m grid is built to fully cover the user’s trajectory. Each GPS fix is then mapped to its corresponding grid cell (ci, rj).\nThis discretisation step reduces the influence of GPS jitter and avoids tracing every minor wiggle of the raw trajectories. Subsequent analyses operate at the cell level—using visits to grid cells instead of raw points—which stabilises behaviour patterns and prevents over-interpreting micro-movements that arise from measurement error or inconsistent sampling.\nUsing a fixed grid inevitably introduces a mild modifiable areal unit problem (MAUP): the exact boundaries and grid size may slightly influence which cells a point falls into. In this context, however, the 500 m resolution strikes a balance between reducing noise and retaining meaningful activity locations, making the grid a practical spatial unit for behavioural modelling.\n\n\nShow code\ndef cell_center_lonlat(ci, rj):\n    cx = gx0 + (ci + 0.5) * GRID_SIZE\n    cy = gy0 + (rj + 0.5) * GRID_SIZE\n    lon, lat = to_wgs(cx, cy)\n    return lon, lat\n\n# Coordinate reference systems\nCRS_WGS84 = pyproj.CRS(\"EPSG:4326\")\nCRS_UTM50 = pyproj.CRS(\"EPSG:32650\")\nto_utm = pyproj.Transformer.from_crs(CRS_WGS84, CRS_UTM50, always_xy=True).transform\nto_wgs = pyproj.Transformer.from_crs(CRS_UTM50, CRS_WGS84, always_xy=True).transform\n\n# Lat/lon -&gt; projected (metres)\nx, y = to_utm(traj[\"lon\"].values, traj[\"lat\"].values)\ntraj[\"x\"] = x\ntraj[\"y\"] = y\n\nGRID_SIZE = 500  # metres\n\n# Grid extent with one extra cell of padding on each side\nminx, miny = traj[\"x\"].min(), traj[\"y\"].min()\nmaxx, maxy = traj[\"x\"].max(), traj[\"y\"].max()\n\ngx0 = math.floor(minx / GRID_SIZE) * GRID_SIZE - GRID_SIZE\ngy0 = math.floor(miny / GRID_SIZE) * GRID_SIZE - GRID_SIZE\ngx1 = math.ceil (maxx / GRID_SIZE) * GRID_SIZE + GRID_SIZE\ngy1 = math.ceil (maxy / GRID_SIZE) * GRID_SIZE + GRID_SIZE\n\nncol = int((gx1 - gx0) / GRID_SIZE)\nnrow = int((gy1 - gy0) / GRID_SIZE)\nprint(f\"Grid columns × rows: {ncol} × {nrow} (total {ncol*nrow:,} cells)\")\n\n# Assign each point to a grid cell\ntraj[\"ci\"] = ((traj[\"x\"] - gx0) // GRID_SIZE).astype(int)\ntraj[\"rj\"] = ((traj[\"y\"] - gy0) // GRID_SIZE).astype(int)\n\ntraj[[\"datetime\", \"lat\", \"lon\", \"x\", \"y\", \"ci\", \"rj\"]].head()\n\n\nGrid columns × rows: 128 × 102 (total 13,056 cells)\n\n\n\n\n\n\n\n\n\ndatetime\nlat\nlon\nx\ny\nci\nrj\n\n\n\n\n0\n2008-10-23 02:53:04\n39.984702\n116.318417\n441807.056623\n4.426282e+06\n43\n50\n\n\n1\n2008-10-23 02:53:10\n39.984683\n116.318450\n441809.858037\n4.426280e+06\n43\n50\n\n\n2\n2008-10-23 02:53:15\n39.984686\n116.318417\n441807.043048\n4.426280e+06\n43\n50\n\n\n3\n2008-10-23 02:53:20\n39.984688\n116.318385\n441804.312590\n4.426280e+06\n43\n50\n\n\n4\n2008-10-23 02:53:25\n39.984655\n116.318263\n441793.868245\n4.426277e+06\n43\n50",
    "crumbs": [
      "Analysis",
      "Notebook 1 – Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#detecting-home-and-sw-from-night-time-points",
    "href": "Data Cleaning.html#detecting-home-and-sw-from-night-time-points",
    "title": "Notebook 1 — From raw Geolife trajectories to visit-level events",
    "section": "5. Detecting HOME and SW from night-time points",
    "text": "5. Detecting HOME and SW from night-time points\nI infer HOME and a secondary frequent place (SW) from points observed between 00:00–06:00 on a 500 m grid, using night-time frequency as a proxy for home-like locations. HOME is the cell with the most night points (ties broken by longer time span), and SW is the next strongest night-time cell after removing HOME, if available.\n\n\nShow code\nnight = traj[(traj[\"hour\"] &gt;= 0) & (traj[\"hour\"] &lt; 6)].copy()\nnight[\"cell\"] = list(zip(night[\"ci\"], night[\"rj\"]))\nprint(\"Number of night-time points:\", len(night))\n\ncell_counts = night[\"cell\"].value_counts()\n\n\nNumber of night-time points: 47296\n\n\n\n\nShow code\ndef night_span_seconds(cell):\n    sub = night[night[\"cell\"] == cell][\"datetime\"]\n    return (sub.max() - sub.min()).total_seconds() if not sub.empty else 0\n\n# HOME: cell with the largest night-time count; if tied, pick the one with largest time span\ncandidates = cell_counts[cell_counts == cell_counts.max()].index.tolist()\nhome_cell = max(candidates, key=night_span_seconds) if len(candidates) &gt; 1 else cell_counts.index[0]\n\n# SW: second-strongest night-time cell after removing HOME\ncell_counts_wo_home = cell_counts[cell_counts.index != home_cell]\nsw_cell = None\nif not cell_counts_wo_home.empty:\n    cand2 = cell_counts_wo_home[cell_counts_wo_home == cell_counts_wo_home.max()].index.tolist()\n    sw_cell = max(cand2, key=night_span_seconds) if len(cand2) &gt; 1 else cell_counts_wo_home.index[0]\n\nhome_lon, home_lat = cell_center_lonlat(*home_cell)\nprint(f\"HOME cell = {home_cell} @ ({home_lon:.6f}, {home_lat:.6f})\")\n\nif sw_cell is not None:\n    sw_lon, sw_lat = cell_center_lonlat(*sw_cell)\n    print(f\"SW   cell = {sw_cell} @ ({sw_lon:.6f}, {sw_lat:.6f})\")\nelse:\n    print(\"SW   cell = None\")\n\n\nHOME cell = (44, 55) @ (116.323385, 40.006970)\nSW   cell = (43, 55) @ (116.317527, 40.006935)",
    "crumbs": [
      "Analysis",
      "Notebook 1 – Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#daily-activity-cells-via-linegrid-intersections",
    "href": "Data Cleaning.html#daily-activity-cells-via-linegrid-intersections",
    "title": "Notebook 1 — From raw Geolife trajectories to visit-level events",
    "section": "6. Daily activity cells via line–grid intersections",
    "text": "6. Daily activity cells via line–grid intersections\nNot every grid cell that contains a raw GPS point should be treated as an “activity location.” Points recorded along fast road segments are usually in transit rather than true stops. To produce a more conservative set of daily activity cells, I work with line segments instead of points: for each day, I connect consecutive projected points into segments in the (x, y) plane and count, for each grid cell, how many segments intersect its polygon.\nA cell is labeled an activity location if its intersection count exceeds a daily threshold. I start with a baseline of 100 crossings, which filters most purely in-transit cells while retaining dense clusters around home and other frequently visited areas. This value is chosen empirically by inspecting multiple days: lower thresholds misclassify long road stretches as “places,” while higher thresholds begin to drop plausible stops. The threshold is then adapted day by day to avoid unrealistically “busy” days: if more than 30 cells exceed the threshold on a given day, I increase the threshold and recompute the classification.\n\n\nShow code\nfrom shapely.geometry import LineString, box\nfrom shapely.strtree import STRtree\n\n# Parameters for the line–grid crossing rule\nTHRESH_START = 100          # initial threshold for \"enough crossings\"\nTHRESH_STEP  = 25          # how much to increase the threshold if a day has too many cells\nTHRESH_MAX   = 1000        # upper bound on the threshold\n\nMAX_PLACES_PER_DAY = 30    # per day: at most this many DISTINCT activity cells (places)\n\n# Cache for grid-cell polygons (speeds things up)\n_poly_cache = {}\n\ndef cell_poly(ci, rj):\n    \"\"\"Return the shapely Polygon for grid cell (ci, rj).\"\"\"\n    key = (ci, rj)\n    if key not in _poly_cache:\n        x0 = gx0 + ci * GRID_SIZE\n        y0 = gy0 + rj * GRID_SIZE\n        _poly_cache[key] = box(x0, y0, x0 + GRID_SIZE, y0 + GRID_SIZE)\n    return _poly_cache[key]\n\ndef query_segments(tree, poly, segs):\n    \"\"\"\n    Wrapper around STRtree.query to handle both 'index' and 'geometry' return types,\n    depending on Shapely version.\n    \"\"\"\n    res = tree.query(poly)\n    if len(res) == 0:\n        return []\n    first = res[0]\n    # Some Shapely versions return indices, some return geometries\n    if isinstance(first, (int, np.integer)):\n        return [segs[i] for i in res]\n    return res\n\n# Containers for results\nvisited_cells_by_day = {}   # date -&gt; set of (ci, rj) cells considered \"activity locations\"\ncross_threshold_used = {}   # date -&gt; threshold value actually used for that day\n\n# We iterate over each date present in the trajectory\ndates = sorted(traj[\"date_only\"].unique().tolist())\nprint(f\"Number of days with data for user {USER_ID}: {len(dates)}\")\n\nfor d in dates:\n    # All points for this day, ordered in time\n    day = traj[traj[\"date_only\"] == d].sort_values(\"datetime\").copy()\n    if len(day) &lt; 2:\n        # Not enough points to form line segments\n        visited_cells_by_day[d] = set()\n        cross_threshold_used[d] = THRESH_START\n        continue\n\n    # Build line segments between consecutive points (in projected coordinates)\n    pts = list(zip(day[\"x\"].values, day[\"y\"].values))\n    segs = [LineString([pts[i], pts[i+1]]) for i in range(len(pts) - 1)]\n\n    # Drop empty / zero-length segments\n    segs = [s for s in segs if (not s.is_empty) and (s.length &gt; 0)]\n    if not segs:\n        visited_cells_by_day[d] = set()\n        cross_threshold_used[d] = THRESH_START\n        continue\n\n    tree = STRtree(segs)\n\n    # Only check the grid cells that actually appear for this day\n    cmin, cmax = int(day[\"ci\"].min()), int(day[\"ci\"].max())\n    rmin, rmax = int(day[\"rj\"].min()), int(day[\"rj\"].max())\n\n    # Start from the base threshold and adapt if needed\n    thr = THRESH_START\n    while True:\n        today_cells = set()\n\n        # Loop over all relevant grid cells for that day\n        for ci in range(cmin, cmax + 1):\n            for rj in range(rmin, rmax + 1):\n                poly = cell_poly(ci, rj)\n                cnt = 0\n\n                # Candidate segments intersecting this cell\n                for seg in query_segments(tree, poly, segs):\n                    if seg.intersects(poly):\n                        cnt += 1\n                        if cnt &gt;= thr:\n                            today_cells.add((ci, rj))\n                            break   # no need to count further for this cell\n\n        # If the day has too many activity CELLS, raise the threshold and try again\n        if len(today_cells) &gt; MAX_PLACES_PER_DAY and thr &lt; THRESH_MAX:\n            thr = min(thr + THRESH_STEP, THRESH_MAX)\n        else:\n            visited_cells_by_day[d] = today_cells\n            cross_threshold_used[d] = thr\n            break\n\n# Summarise: how many activity cells per day, and what threshold was used\nperday_summary = (\n    pd.DataFrame({\n        \"date\": [str(d) for d in dates],\n        \"visited_cells\": [len(visited_cells_by_day[d]) for d in dates],\n        \"threshold_used\": [cross_threshold_used[d] for d in dates],\n    })\n    .sort_values(\"date\")\n    .reset_index(drop=True)\n)\n\nprint(\"Daily activity-cell counts (first 10 days):\")\nperday_summary.head(10)\n\n\nNumber of days with data for user 000: 122\nDaily activity-cell counts (first 10 days):\n\n\n\n\n\n\n\n\n\ndate\nvisited_cells\nthreshold_used\n\n\n\n\n0\n2008-10-23\n3\n100\n\n\n1\n2008-10-24\n1\n100\n\n\n2\n2008-10-26\n2\n100\n\n\n3\n2008-10-27\n0\n100\n\n\n4\n2008-10-28\n3\n100\n\n\n5\n2008-10-29\n0\n100\n\n\n6\n2008-11-03\n0\n100\n\n\n7\n2008-11-04\n4\n100\n\n\n8\n2008-11-10\n0\n100\n\n\n9\n2008-11-11\n4\n100",
    "crumbs": [
      "Analysis",
      "Notebook 1 – Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#from-daily-activity-cells-to-visit-level-events",
    "href": "Data Cleaning.html#from-daily-activity-cells-to-visit-level-events",
    "title": "Notebook 1 — From raw Geolife trajectories to visit-level events",
    "section": "7. From daily activity cells to visit-level events",
    "text": "7. From daily activity cells to visit-level events\nGiven the daily activity cells, I next separate first-time places from returns and compress trajectories into visit-level events. Over the full observation window, a grid cell is labeled a first-time place (Pv) on the first date it appears as an activity cell; on any later day when it is active again, it is treated as a return place (Pn). HOME and SW remain separate categories and override Pv/Pn labels.\nUsing these labels, I build the visit-level table day by day. For each calendar day, I first compress the sequence of visited grid cells using simple run-length encoding, so consecutive points in the same cell become a single block. Each block is matched to that day’s activity-cell set and tagged as HOME, SW, Pv, or Pn; blocks not classified as activity locations are dropped as in-transit segments. Adjacent blocks with the same cell and tag are then merged so brief GPS gaps do not split a single stay into multiple visits.\nThe remaining blocks define visit events. For each visit, I record its start/end time, duration, grid indices and cell center (in both projected and geographic coordinates), distance to HOME, and a place identifier (HOME, SW, Pv#, Pn#). I also keep two within-day order measures: a running visit order across the day, and an action-order index that restarts at 1 when the user leaves HOME and resets to 0 upon returning. Finally, each visit receives a next_step label indicating whether the next visit is to HOME, SW, a Pv, a Pn, or none (end of day). Stacking all days yields the final visit-level table, saved as visit_level_table_000.csv and used as input for the subsequent notebooks.\n\n\nShow code\nfrom datetime import timedelta\n\npv_label = {}              # (ci, rj) -&gt; \"Pv#\"\npn_label = {}              # ((ci, rj), date) -&gt; \"Pn#\"\nfirst_visit_date = {}      # (ci, rj) -&gt; first date when this cell is an activity cell\n\npv_counter = 0\npn_counter = 0\n\nfor d in sorted(visited_cells_by_day.keys()):\n    cells = visited_cells_by_day[d]\n    for cell in sorted(cells):\n        if cell not in first_visit_date:\n            # First time this activity cell appears in the whole sample -&gt; Pv\n            pv_counter += 1\n            first_visit_date[cell] = d\n            pv_label[cell] = f\"Pv{pv_counter}\"\n        else:\n            # Subsequent days when the same activity cell is active -&gt; Pn\n            pn_counter += 1\n            pn_label[(cell, d)] = f\"Pn{pn_counter}\"\n\nprint(f\"Total unique Pv cells: {len(pv_label)}\")\nprint(f\"Total Pn labels assigned: {len(pn_label)}\")\n\n\n# --- 7.2 Helper: classify a cell on a given day ---\n\ndef classify_cell(cell, day_date):\n    \"\"\"\n    Classify a grid cell on a given date into:\n      - ('home', 'HOME')\n      - ('sw', 'SW')\n      - ('pv', 'Pv#')\n      - ('pn', 'Pn#')\n      - ('none', '')  for cells that are not treated as activity locations on that day.\n    \"\"\"\n    if cell == home_cell:\n        return \"home\", \"HOME\"\n    if (sw_cell is not None) and (cell == sw_cell):\n        return \"sw\", \"SW\"\n\n    visited_today = visited_cells_by_day.get(day_date, set())\n    if cell in visited_today:\n        if first_visit_date.get(cell) == day_date:\n            return \"pv\", pv_label[cell]\n        else:\n            return \"pn\", pn_label.get((cell, day_date), \"PN\")\n\n    return \"none\", \"\"\n\n\ndef cell_center_xy(ci, rj):\n    \"\"\"Return the projected (x, y) centre of grid cell (ci, rj).\"\"\"\n    cx = gx0 + (ci + 0.5) * GRID_SIZE\n    cy = gy0 + (rj + 0.5) * GRID_SIZE\n    return cx, cy\n\n\n# --- 7.3 Build the merged visit-level table ---\n\nvisit_rows = []\nall_dates = sorted(traj[\"date_only\"].unique().tolist())\n\nfor d in all_dates:\n    day = traj[traj[\"date_only\"] == d].sort_values(\"datetime\").copy()\n    if day.empty:\n        continue\n\n    # Sequence of grid cells and timestamps for this day\n    cells = list(zip(day[\"ci\"].astype(int), day[\"rj\"].astype(int)))\n    times = day[\"datetime\"].tolist()\n\n    # 1) Raw run-length encoding by grid cell:\n    #    consecutive identical (ci, rj) are grouped into one block.\n    runs = []  # list of (cell, i0, i1) with indices into 'times'\n    if cells:\n        start = 0\n        for i in range(1, len(cells)):\n            if cells[i] != cells[i-1]:\n                runs.append((cells[i-1], start, i-1))\n                start = i\n        runs.append((cells[-1], start, len(cells) - 1))\n\n    # 2) Attach labels and time bounds; drop blocks that are not activity cells\n    tagged = []\n    for (cell, i0, i1) in runs:\n        tag, pid = classify_cell(cell, d)\n        if tag == \"none\":\n            continue  # ignore transit-only cells\n        tagged.append({\n            \"cell\": cell,\n            \"ci\": cell[0],\n            \"rj\": cell[1],\n            \"tag\": tag,\n            \"place_id\": pid,\n            \"i0\": i0,\n            \"i1\": i1,\n            \"first_dt\": times[i0],\n            \"last_dt\": times[i1],\n        })\n\n    if not tagged:\n        continue\n\n    # 3) Merge adjacent blocks with the same (ci, rj, tag)\n    merged = []\n    current = tagged[0]\n    for nxt in tagged[1:]:\n        if (\n            (nxt[\"ci\"] == current[\"ci\"]) and\n            (nxt[\"rj\"] == current[\"rj\"]) and\n            (nxt[\"tag\"] == current[\"tag\"])\n        ):\n            # extend the current block\n            current[\"i1\"] = nxt[\"i1\"]\n            current[\"last_dt\"] = nxt[\"last_dt\"]\n        else:\n            merged.append(current)\n            current = nxt\n    merged.append(current)\n\n    # 4) For each merged visit, compute attributes and within-day order\n    visit_order = 0     \n    action_order = 0    \n\n    for k, rec in enumerate(merged):\n        ci, rj = rec[\"ci\"], rec[\"rj\"]\n        tag, pid = rec[\"tag\"], rec[\"place_id\"]\n        first_dt, last_dt = rec[\"first_dt\"], rec[\"last_dt\"]\n\n        visit_order += 1\n\n        if tag == \"home\":\n            action_order = 0    \n            action_ord = 0\n        else:\n            if action_order == 0:\n                action_order = 1  \n            else:\n                action_order += 1\n            action_ord = action_order\n\n        # Next-step label (based on the next merged block)\n        if k &lt; len(merged) - 1:\n            next_tag = merged[k + 1][\"tag\"]\n        else:\n            next_tag = \"none\"\n\n        # Geometry-based attributes\n        cx, cy = cell_center_xy(ci, rj)\n        hx, hy = cell_center_xy(*home_cell)\n        dist_home = math.hypot(cx - hx, cy - hy)\n        center_lon, center_lat = cell_center_lonlat(ci, rj)\n\n        visit_rows.append({\n            \"person\": USER_ID,\n            \"date\": d.isoformat(),\n            \"first_time\": first_dt.time().isoformat(),\n            \"last_time\": last_dt.time().isoformat(),\n            \"duration_min\": round((last_dt - first_dt) / timedelta(minutes=1), 1),\n            \"grid_ci\": ci,\n            \"grid_rj\": rj,\n            \"grid_center_lon\": round(center_lon, 6),\n            \"grid_center_lat\": round(center_lat, 6),\n            \"dist_home_m\": round(dist_home, 1),\n            \"is_home\": 1 if tag == \"home\" else 0,\n            \"is_sw\":   1 if tag == \"sw\"   else 0,\n            \"is_pv\":   1 if tag == \"pv\"   else 0,\n            \"is_pn\":   1 if tag == \"pn\"   else 0,\n            \"place_id\": pid,          # HOME / SW / Pv# / Pn#\n            \"next_step\": next_tag,    # home / sw / pv / pn / none\n            \"visit_order_in_day\": visit_order,  \n            \"action_order\": action_ord         \n        })\n\n# Final visit-level table\nvisit_table = (\n    pd.DataFrame(visit_rows)\n    .sort_values([\"date\", \"first_time\", \"grid_ci\", \"grid_rj\"])\n    .reset_index(drop=True)\n)\n\nprint(\"Number of visit events:\", len(visit_table))\nvisit_table.head(10)\n\n\nTotal unique Pv cells: 106\nTotal Pn labels assigned: 294\nNumber of visit events: 1841\n\n\n\n\n\n\n\n\n\nperson\ndate\nfirst_time\nlast_time\nduration_min\ngrid_ci\ngrid_rj\ngrid_center_lon\ngrid_center_lat\ndist_home_m\nis_home\nis_sw\nis_pv\nis_pn\nplace_id\nnext_step\nvisit_order_in_day\naction_order\n\n\n\n\n0\n000\n2008-10-23\n03:00:55\n04:13:32\n72.6\n40\n50\n116.300184\n39.984308\n3201.6\n0\n0\n1\n0\nPv1\nsw\n1\n1\n\n\n1\n000\n2008-10-23\n09:42:25\n09:42:30\n0.1\n43\n55\n116.317527\n40.006935\n500.0\n0\n1\n0\n0\nSW\nhome\n2\n2\n\n\n2\n000\n2008-10-23\n09:42:35\n10:05:29\n22.9\n44\n55\n116.323385\n40.006970\n0.0\n1\n0\n0\n0\nHOME\nsw\n3\n0\n\n\n3\n000\n2008-10-23\n10:05:34\n10:30:15\n24.7\n43\n55\n116.317527\n40.006935\n500.0\n0\n1\n0\n0\nSW\nhome\n4\n1\n\n\n4\n000\n2008-10-23\n10:30:20\n11:10:57\n40.6\n44\n55\n116.323385\n40.006970\n0.0\n1\n0\n0\n0\nHOME\nnone\n5\n0\n\n\n5\n000\n2008-10-24\n02:09:59\n02:10:54\n0.9\n43\n55\n116.317527\n40.006935\n500.0\n0\n1\n0\n0\nSW\nhome\n1\n1\n\n\n6\n000\n2008-10-24\n02:10:59\n02:47:06\n36.1\n44\n55\n116.323385\n40.006970\n0.0\n1\n0\n0\n0\nHOME\nnone\n2\n0\n\n\n7\n000\n2008-10-26\n14:04:27\n14:12:42\n8.2\n55\n33\n116.388704\n39.908225\n12298.4\n0\n0\n1\n0\nPv4\npv\n1\n1\n\n\n8\n000\n2008-10-26\n14:23:42\n14:35:17\n11.6\n56\n31\n116.394633\n39.899246\n13416.4\n0\n0\n1\n0\nPv5\nnone\n2\n2\n\n\n9\n000\n2008-10-27\n12:03:59\n12:05:54\n1.9\n44\n55\n116.323385\n40.006970\n0.0\n1\n0\n0\n0\nHOME\nnone\n1\n0\n\n\n\n\n\n\n\n\n\nShow code\n# save visit-level table for later notebooks\nOUT_DIR = \"data\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nout_path = os.path.join(OUT_DIR, f\"visit_level_table_{USER_ID}.csv\")\nvisit_table.to_csv(out_path, index=False, encoding=\"utf-8-sig\")",
    "crumbs": [
      "Analysis",
      "Notebook 1 – Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#mapping-inferred-activity-places",
    "href": "Data Cleaning.html#mapping-inferred-activity-places",
    "title": "Notebook 1 — From raw Geolife trajectories to visit-level events",
    "section": "8. Mapping inferred activity places",
    "text": "8. Mapping inferred activity places\nTo summarise the cleaned visit-level data, I project the inferred activity locations back onto an interactive map. Starting from the visit table, I collapse repeated visits to the same 500 m grid cell and draw one square for each distinct cell that ever appears as a visit. Each cell is coloured according to its role in the visit history (explored at least once versus only revisited), while the HOME and SW cells are highlighted separately as point markers. Compared with the raw trajectories, this map shows a much simpler picture of the user’s daily activity space: a small set of recurrent places organised around home, rather than every individual GPS point and transit segment.\n\n\nShow code\n# --- 8.1 Aggregate unique places from the visit-level table ---\n\n# We keep only non-home visits (HOME is shown separately as a point),\n# and collapse repeated visits to the same grid cell.\nplaces = (\n    visit_table\n    .groupby([\"grid_ci\", \"grid_rj\"], as_index=False)\n    .agg(\n        any_home=(\"is_home\", \"max\"),\n        any_sw=(\"is_sw\", \"max\"),\n        any_pv=(\"is_pv\", \"max\"),\n        any_pn=(\"is_pn\", \"max\"),\n        grid_center_lon=(\"grid_center_lon\", \"first\"),\n        grid_center_lat=(\"grid_center_lat\", \"first\"),\n        dist_home_m=(\"dist_home_m\", \"min\")\n    )\n)\n\n\nprint(\"Number of unique grid cells appearing in visits:\", len(places))\n\n\n# --- 8.2 Helper: cell polygon in lat/lon ---\n\ndef cell_bounds_lonlat(ci, rj):\n    \"\"\"\n    Return the 4 corners (and closed ring) of grid cell (ci, rj)\n    as (lat, lon) pairs for use in folium.Polygon.\n    \"\"\"\n    x0 = gx0 + ci * GRID_SIZE\n    y0 = gy0 + rj * GRID_SIZE\n    x1 = x0 + GRID_SIZE\n    y1 = y0 + GRID_SIZE\n\n    lon0, lat0 = to_wgs(x0, y0)\n    lon1, lat1 = to_wgs(x1, y0)\n    lon2, lat2 = to_wgs(x1, y1)\n    lon3, lat3 = to_wgs(x0, y1)\n\n    # folium expects (lat, lon)\n    return [\n        (lat0, lon0),\n        (lat1, lon1),\n        (lat2, lon2),\n        (lat3, lon3),\n        (lat0, lon0),\n    ]\n\n\n# --- 8.3 Colour rule for places ---\n\ndef color_for_place(row):\n    cell = (int(row[\"grid_ci\"]), int(row[\"grid_rj\"]))\n    # HOME and SW will be shown as point markers; here we colour only squares\n    if sw_cell is not None and cell == sw_cell:\n        return \"green\"\n    # Pv vs Pn logic\n    if row[\"any_pn\"] == 1:\n        return \"orange\"   # ever seen as Pv at least once\n    if row[\"any_pv\"] == 1:\n        return \"purple\"   # only returns\n    return \"gray\"\n\n\n# --- 8.4 Initialise a folium map centred at HOME ---\n\n# Compute HOME centre in lat/lon (from grid indices)\nhx = gx0 + (home_cell[0] + 0.5) * GRID_SIZE\nhy = gy0 + (home_cell[1] + 0.5) * GRID_SIZE\nhome_lon, home_lat = to_wgs(hx, hy)\n\nm = folium.Map(location=[home_lat, home_lon],\n               zoom_start=12,\n               tiles=\"cartodbpositron\")\n\n\n# --- 8.5 Add grid-cell polygons for activity places ---\n\nfor _, row in places.iterrows():\n    ci = int(row[\"grid_ci\"])\n    rj = int(row[\"grid_rj\"])\n    poly_latlon = cell_bounds_lonlat(ci, rj)\n    col = color_for_place(row)\n\n    popup_html = (\n        f\"Cell: ({ci}, {rj})&lt;br&gt;\"\n        f\"Center: ({row['grid_center_lat']:.5f}, {row['grid_center_lon']:.5f})&lt;br&gt;\"\n        f\"Dist. to HOME: {int(round(row['dist_home_m']))} m&lt;br&gt;\"\n        f\"Any Pv: {int(row['any_pv'])} &nbsp; Any Pn: {int(row['any_pn'])}\"\n    )\n\n    folium.Polygon(\n        locations=poly_latlon,\n        color=col,\n        weight=2,\n        fill=True,\n        fill_opacity=0.35,\n        popup=popup_html\n    ).add_to(m)\n\n\n# --- 8.6 Add HOME and SW as point markers ---\n\nfolium.CircleMarker(\n    location=[home_lat, home_lon],\n    radius=7,\n    color=\"blue\",\n    fill=True,\n    fill_opacity=0.9,\n    popup=\"HOME\"\n).add_to(m)\n\nif sw_cell is not None:\n    sx = gx0 + (sw_cell[0] + 0.5) * GRID_SIZE\n    sy = gy0 + (sw_cell[1] + 0.5) * GRID_SIZE\n    sw_lon, sw_lat = to_wgs(sx, sy)\n\n    folium.CircleMarker(\n        location=[sw_lat, sw_lon],\n        radius=7,\n        color=\"green\",\n        fill=True,\n        fill_opacity=0.9,\n        popup=\"SW\"\n    ).add_to(m)\n\n\nm\n\n\nNumber of unique grid cells appearing in visits: 106\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Analysis",
      "Notebook 1 – Data cleaning"
    ]
  },
  {
    "objectID": "by action.html",
    "href": "by action.html",
    "title": "Notebook 2 — Trip-level exploration and “going home” behaviour",
    "section": "",
    "text": "This notebook starts from the visit-level table produced in Notebook 1. I load visit_level_table_000.csv from the data folder and set up the basic Python environment (NumPy, pandas, Matplotlib). I then combine the date and time fields into full start and end timestamps for each visit (first_dt and last_dt), and extract the user identifier from the person column for labeling plots. Finally, I sort the table by person, date, and start time to create a clean, chronological sequence of visit events (rather than daily aggregates), which serves as the input for identifying home–home trip episodes (“outings”) in later sections.\n\n\nShow code\nimport warnings\n\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"DataFrameGroupBy.apply operated on the grouping columns\",\n    category=FutureWarning,\n)\n\n\n\n\nShow code\n# 1. Environment and data loading\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nplt.style.use(\"default\")\n\n# Path to the visit-level table saved from Notebook 1\npath = \"data/visit_level_table_000.csv\"\nvisit_table = pd.read_csv(path)\n\nprint(\"Rows in visit_table:\", len(visit_table))\ndisplay(visit_table.head())\n\n# Parse date + times into full timestamps\nvisit_table[\"date\"] = pd.to_datetime(visit_table[\"date\"]).dt.date\n\nvisit_table[\"first_dt\"] = pd.to_datetime(\n    visit_table[\"date\"].astype(str) + \" \" + visit_table[\"first_time\"]\n)\nvisit_table[\"last_dt\"] = pd.to_datetime(\n    visit_table[\"date\"].astype(str) + \" \" + visit_table[\"last_time\"]\n)\n\n# Identify the user ID from the table\nUSER_ID = str(visit_table[\"person\"].iloc[0])\nprint(\"Unique persons in this file:\", visit_table[\"person\"].unique())\nprint(\"USER_ID used in plots:\", USER_ID)\n\n# Ensure rows are ordered in time\nvisit_table = (\n    visit_table\n    .sort_values([\"person\", \"date\", \"first_dt\"])\n    .reset_index(drop=True)\n)\nprint(\"Rows in visit_table after sorting:\", len(visit_table))\n\n\nRows in visit_table: 1841\n\n\n\n\n\n\n\n\n\nperson\ndate\nfirst_time\nlast_time\nduration_min\ngrid_ci\ngrid_rj\ngrid_center_lon\ngrid_center_lat\ndist_home_m\nis_home\nis_sw\nis_pv\nis_pn\nplace_id\nnext_step\nvisit_order_in_day\naction_order\n\n\n\n\n0\n0\n2008-10-23\n03:00:55\n04:13:32\n72.6\n40\n50\n116.300184\n39.984308\n3201.6\n0\n0\n1\n0\nPv1\nsw\n1\n1\n\n\n1\n0\n2008-10-23\n09:42:25\n09:42:30\n0.1\n43\n55\n116.317527\n40.006935\n500.0\n0\n1\n0\n0\nSW\nhome\n2\n2\n\n\n2\n0\n2008-10-23\n09:42:35\n10:05:29\n22.9\n44\n55\n116.323385\n40.006970\n0.0\n1\n0\n0\n0\nHOME\nsw\n3\n0\n\n\n3\n0\n2008-10-23\n10:05:34\n10:30:15\n24.7\n43\n55\n116.317527\n40.006935\n500.0\n0\n1\n0\n0\nSW\nhome\n4\n1\n\n\n4\n0\n2008-10-23\n10:30:20\n11:10:57\n40.6\n44\n55\n116.323385\n40.006970\n0.0\n1\n0\n0\n0\nHOME\nnone\n5\n0\n\n\n\n\n\n\n\nUnique persons in this file: [0]\nUSER_ID used in plots: 0\nRows in visit_table after sorting: 1841",
    "crumbs": [
      "Analysis",
      "Notebook 2 – Trips by action"
    ]
  },
  {
    "objectID": "by action.html#import-the-datasets-from-000",
    "href": "by action.html#import-the-datasets-from-000",
    "title": "Notebook 2 — Trip-level exploration and “going home” behaviour",
    "section": "",
    "text": "This notebook starts from the visit-level table produced in Notebook 1. I load visit_level_table_000.csv from the data folder and set up the basic Python environment (NumPy, pandas, Matplotlib). I then combine the date and time fields into full start and end timestamps for each visit (first_dt and last_dt), and extract the user identifier from the person column for labeling plots. Finally, I sort the table by person, date, and start time to create a clean, chronological sequence of visit events (rather than daily aggregates), which serves as the input for identifying home–home trip episodes (“outings”) in later sections.\n\n\nShow code\nimport warnings\n\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"DataFrameGroupBy.apply operated on the grouping columns\",\n    category=FutureWarning,\n)\n\n\n\n\nShow code\n# 1. Environment and data loading\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nplt.style.use(\"default\")\n\n# Path to the visit-level table saved from Notebook 1\npath = \"data/visit_level_table_000.csv\"\nvisit_table = pd.read_csv(path)\n\nprint(\"Rows in visit_table:\", len(visit_table))\ndisplay(visit_table.head())\n\n# Parse date + times into full timestamps\nvisit_table[\"date\"] = pd.to_datetime(visit_table[\"date\"]).dt.date\n\nvisit_table[\"first_dt\"] = pd.to_datetime(\n    visit_table[\"date\"].astype(str) + \" \" + visit_table[\"first_time\"]\n)\nvisit_table[\"last_dt\"] = pd.to_datetime(\n    visit_table[\"date\"].astype(str) + \" \" + visit_table[\"last_time\"]\n)\n\n# Identify the user ID from the table\nUSER_ID = str(visit_table[\"person\"].iloc[0])\nprint(\"Unique persons in this file:\", visit_table[\"person\"].unique())\nprint(\"USER_ID used in plots:\", USER_ID)\n\n# Ensure rows are ordered in time\nvisit_table = (\n    visit_table\n    .sort_values([\"person\", \"date\", \"first_dt\"])\n    .reset_index(drop=True)\n)\nprint(\"Rows in visit_table after sorting:\", len(visit_table))\n\n\nRows in visit_table: 1841\n\n\n\n\n\n\n\n\n\nperson\ndate\nfirst_time\nlast_time\nduration_min\ngrid_ci\ngrid_rj\ngrid_center_lon\ngrid_center_lat\ndist_home_m\nis_home\nis_sw\nis_pv\nis_pn\nplace_id\nnext_step\nvisit_order_in_day\naction_order\n\n\n\n\n0\n0\n2008-10-23\n03:00:55\n04:13:32\n72.6\n40\n50\n116.300184\n39.984308\n3201.6\n0\n0\n1\n0\nPv1\nsw\n1\n1\n\n\n1\n0\n2008-10-23\n09:42:25\n09:42:30\n0.1\n43\n55\n116.317527\n40.006935\n500.0\n0\n1\n0\n0\nSW\nhome\n2\n2\n\n\n2\n0\n2008-10-23\n09:42:35\n10:05:29\n22.9\n44\n55\n116.323385\n40.006970\n0.0\n1\n0\n0\n0\nHOME\nsw\n3\n0\n\n\n3\n0\n2008-10-23\n10:05:34\n10:30:15\n24.7\n43\n55\n116.317527\n40.006935\n500.0\n0\n1\n0\n0\nSW\nhome\n4\n1\n\n\n4\n0\n2008-10-23\n10:30:20\n11:10:57\n40.6\n44\n55\n116.323385\n40.006970\n0.0\n1\n0\n0\n0\nHOME\nnone\n5\n0\n\n\n\n\n\n\n\nUnique persons in this file: [0]\nUSER_ID used in plots: 0\nRows in visit_table after sorting: 1841",
    "crumbs": [
      "Analysis",
      "Notebook 2 – Trips by action"
    ]
  },
  {
    "objectID": "by action.html#from-daily-visits-to-homehome-trips",
    "href": "by action.html#from-daily-visits-to-homehome-trips",
    "title": "Notebook 2 — Trip-level exploration and “going home” behaviour",
    "section": "2. From daily visits to home–home trips",
    "text": "2. From daily visits to home–home trips\nThe next stage groups visit events into home-based trips. For each person-day, visits are ordered by start time, and any record with is_home == 1 is treated as being at HOME. A trip starts when the user leaves HOME for a non-home location, continues across consecutive non-home stops, and ends upon the next return to HOME. If there is no return within the same day, the trip is truncated at the final observed non-home visit. This converts a raw sequence of places into interpretable “outings” anchored at home.\nTo support downstream analysis, I assign two within-day indices to each visit. trip_id labels which trip the visit belongs to (with 0 reserved for visits not assigned to any trip), and action_order_in_trip records the stop order within that trip. Non-home stops are numbered 1, 2, 3, …, while HOME visits at the start or end of a trip are coded as 0. Together, these indices provide a clean structure for tracking how behaviour evolves over the course of an outing.\n\n\nShow code\ndef add_trip_cols(df_day):\n    \"\"\"\n    For a single (person, date), label home–home trip episodes.\n\n    - trip_id: which trip on that day (1, 2, ...), 0 if not in a trip.\n    - action_order_in_trip: order within the current trip\n      (1, 2, 3, ... for non-home visits, 0 for HOME).\n    \"\"\"\n    df_day = df_day.sort_values(\"first_dt\").copy()\n\n    trip_id = 0\n    action_order = 0\n    out = False  # currently away from HOME?\n\n    trip_ids = []\n    action_orders = []\n\n    for _, row in df_day.iterrows():\n        if row[\"is_home\"] == 1:\n            # Being at HOME: mark as outside any within-trip sequence\n            out = False\n            action_order = 0\n            trip_ids.append(trip_id)\n            action_orders.append(0)\n        else:\n            # Non-home visit\n            if not out:\n                # Leaving home: start a new trip\n                trip_id += 1\n                action_order = 1\n                out = True\n            else:\n                # Continuing within the same trip\n                action_order += 1\n\n            trip_ids.append(trip_id)\n            action_orders.append(action_order)\n\n    df_day[\"trip_id\"] = trip_ids\n    df_day[\"action_order_in_trip\"] = action_orders\n    return df_day\n\nvisit_table = (\n    visit_table\n    .groupby([\"person\", \"date\"], group_keys=False)\n    .apply(add_trip_cols)\n    .reset_index(drop=True)\n)\n\nprint(\"Trip-related columns (first 20 rows):\")\ndisplay(\n    visit_table[\n        [\"person\", \"date\", \"place_id\", \"is_home\",\n         \"visit_order_in_day\", \"trip_id\", \"action_order_in_trip\",\n         \"first_time\", \"last_time\"]\n    ].head(10)\n)\n\nprint(\"Number of trips (trip_id &gt; 0):\", (visit_table[\"trip_id\"] &gt; 0).sum())\n\n\nTrip-related columns (first 20 rows):\n\n\n\n\n\n\n\n\n\nperson\ndate\nplace_id\nis_home\nvisit_order_in_day\ntrip_id\naction_order_in_trip\nfirst_time\nlast_time\n\n\n\n\n0\n0\n2008-10-23\nPv1\n0\n1\n1\n1\n03:00:55\n04:13:32\n\n\n1\n0\n2008-10-23\nSW\n0\n2\n1\n2\n09:42:25\n09:42:30\n\n\n2\n0\n2008-10-23\nHOME\n1\n3\n1\n0\n09:42:35\n10:05:29\n\n\n3\n0\n2008-10-23\nSW\n0\n4\n2\n1\n10:05:34\n10:30:15\n\n\n4\n0\n2008-10-23\nHOME\n1\n5\n2\n0\n10:30:20\n11:10:57\n\n\n5\n0\n2008-10-24\nSW\n0\n1\n1\n1\n02:09:59\n02:10:54\n\n\n6\n0\n2008-10-24\nHOME\n1\n2\n1\n0\n02:10:59\n02:47:06\n\n\n7\n0\n2008-10-26\nPv4\n0\n1\n1\n1\n14:04:27\n14:12:42\n\n\n8\n0\n2008-10-26\nPv5\n0\n2\n1\n2\n14:23:42\n14:35:17\n\n\n9\n0\n2008-10-27\nHOME\n1\n1\n0\n0\n12:03:59\n12:05:54\n\n\n\n\n\n\n\nNumber of trips (trip_id &gt; 0): 1805",
    "crumbs": [
      "Analysis",
      "Notebook 2 – Trips by action"
    ]
  },
  {
    "objectID": "by action.html#trip-level-descriptive-statistics",
    "href": "by action.html#trip-level-descriptive-statistics",
    "title": "Notebook 2 — Trip-level exploration and “going home” behaviour",
    "section": "3. Trip-level descriptive statistics",
    "text": "3. Trip-level descriptive statistics\nNext, I summarize the visit-level records into trip-level summaries, so that each “outing” is represented by one row instead of many individual stops. Concretely, for each person–date–trip_id combination, I aggregate all visits that belong to the same trip and compute a small set of intuitive measures.\nFor each trip, I record: (1) how many non-home stops it contains, saved as n_stops (this counts the distinct places visited after leaving HOME, not the HOME stays themselves); (2) when the trip starts and ends, trip_start and trip_end, defined as the timestamps of the first and last visit in that trip; and (3) how far the user ranges from HOME during that outing, measured by the maximum and average distance to HOME across its stops, max_dist_home and mean_dist_home.\nThese trip-level summaries make the patterns easy to see. Histograms of n_stops and max_dist_home quickly reveal whether most outings are short one-stop errands or longer chains of stops, and whether the user typically stays close to HOME or often makes long-distance excursions.\n\n\nShow code\n# Keep only visits that belong to a trip\nvt_trip = visit_table[visit_table[\"trip_id\"] &gt; 0].copy()\n\ntrip_summary = (\n    vt_trip\n    .groupby([\"person\", \"date\", \"trip_id\"], as_index=False)\n    .agg(\n        n_stops=(\"action_order_in_trip\", lambda x: (x &gt; 0).sum()),\n        trip_start=(\"first_dt\", \"min\"),\n        trip_end=(\"last_dt\", \"max\"),\n        max_dist_home=(\"dist_home_m\", \"max\"),\n        mean_dist_home=(\"dist_home_m\", \"mean\"),\n    )\n)\n\ntrip_summary[\"trip_duration_min\"] = (\n    (trip_summary[\"trip_end\"] - trip_summary[\"trip_start\"])\n    .dt.total_seconds() / 60.0\n)\n\nprint(\"Number of trips:\", len(trip_summary))\ndisplay(trip_summary.head())\n\n# --- 3.1 Histogram of stops per trip ---\nplt.figure(figsize=(6, 4))\nplt.hist(trip_summary[\"n_stops\"], bins=20)\nplt.xlabel(\"Number of non-home stops in trip\")\nplt.ylabel(\"Number of trips\")\nplt.title(f\"Stops per trip — user {USER_ID}\")\nplt.tight_layout()\nplt.show()\n\n# --- 3.2 Histogram of max distance from HOME per trip ---\nplt.figure(figsize=(6, 4))\nplt.hist(trip_summary[\"max_dist_home\"], bins=30)\nplt.xlabel(\"Maximum distance from HOME (m)\")\nplt.ylabel(\"Number of trips\")\nplt.title(f\"Maximum distance from HOME per trip — user {USER_ID}\")\nplt.tight_layout()\nplt.show()\n\n\nNumber of trips: 429\n\n\n\n\n\n\n\n\n\nperson\ndate\ntrip_id\nn_stops\ntrip_start\ntrip_end\nmax_dist_home\nmean_dist_home\ntrip_duration_min\n\n\n\n\n0\n0\n2008-10-23\n1\n2\n2008-10-23 03:00:55\n2008-10-23 10:05:29\n3201.6\n1233.866667\n424.566667\n\n\n1\n0\n2008-10-23\n2\n1\n2008-10-23 10:05:34\n2008-10-23 11:10:57\n500.0\n250.000000\n65.383333\n\n\n2\n0\n2008-10-24\n1\n1\n2008-10-24 02:09:59\n2008-10-24 02:47:06\n500.0\n250.000000\n37.116667\n\n\n3\n0\n2008-10-26\n1\n2\n2008-10-26 14:04:27\n2008-10-26 14:35:17\n13416.4\n12857.400000\n30.833333\n\n\n4\n0\n2008-10-28\n1\n20\n2008-10-28 00:38:26\n2008-10-28 02:52:11\n2549.5\n2214.280952\n133.750000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor user 000, I identify 429 home-based trips. The histogram of n_stops shows that most outings are simple: the median trip has one non-home stop, and most trips include only one or two stops. There is also a long but thin right tail of unusually complex outings, reaching nearly 100 recorded stops.\nThe distribution of max_dist_home tells a similar story spatially: most trips stay within a few kilometres of HOME, but a small number of excursions extend tens of kilometres away. Overall, the pattern is one of frequent short errands near home, punctuated by rare, much longer journeys.",
    "crumbs": [
      "Analysis",
      "Notebook 2 – Trips by action"
    ]
  },
  {
    "objectID": "by action.html#next-step-behaviour-within-trips",
    "href": "by action.html#next-step-behaviour-within-trips",
    "title": "Notebook 2 — Trip-level exploration and “going home” behaviour",
    "section": "4. Next-step behaviour within trips",
    "text": "4. Next-step behaviour within trips\n\n4.1 Calculate the non home stops\nThe analysis now zooms in from whole home–home trips to the individual non-home stops within each trip. At each stop, the key question is simple: what happens immediately next? Instead of modeling the full sequence at once, I focus on the next move after each non-home visit.\nFor every non-home stop, I define three next-step outcomes. Home means the next recorded visit is a return to HOME. Explore means the trip continues to another non-home place, which can be SW, a first-time place (Pv), or a return place (Pn). End means the trip stops in the dataset and no later visit is observed.\nOne complication is the last non-home stop of a trip. When its next-step is recorded as end, this often reflects the end of the day or missing data, not that the person truly stayed out forever. To reduce this artifact, I apply a conservative adjustment: if an end stop happens late at night (hour ≥ 23) or very close to HOME (≤ 750 m), I reclassify it as effectively going home. This makes the outcome categories more comparable across stops and prevents the “end” category from being inflated by day-boundary effects.\nAfter reclassification, I tabulate the outcome shares by stop order k within the trip (k = 1 for the first stop after leaving HOME, k = 2 for the second, etc.). This shows how the balance between going home, continuing to explore, and ending changes as a trip progresses, giving a clearer view of within-trip behavioural dynamics.\n\n\nShow code\nMAXK = 30        # only consider the first 30 stops within each trip\nNIGHT_HOUR = 23  # &gt;= 23:00 considered \"late\"\nNEAR_HOME_M = 750\n\n# Non-home visits that are part of a trip and have a positive action order\nvis = visit_table[\n    (visit_table[\"is_home\"] == 0) &\n    (visit_table[\"trip_id\"] &gt; 0) &\n    (visit_table[\"action_order_in_trip\"] &gt; 0)\n].copy()\n\nvis = vis[vis[\"action_order_in_trip\"] &lt;= MAXK].copy()\n\nprint(\"Non-home visit events inside trips:\", len(vis))\nprint(\"\\nRaw next_step value counts (non-home):\")\nprint(vis[\"next_step\"].value_counts())\n\n# --- Classify next-step outcomes ---\n\nvis[\"outcome\"] = np.where(\n    vis[\"next_step\"] == \"home\", \"home\",\n    np.where(vis[\"next_step\"].isin([\"sw\", \"pv\", \"pn\"]), \"explore\", \"end\")\n)\n\n# Identify the last non-home visit of each trip\nlast_mask = vis.groupby(\n    [\"person\", \"date\", \"trip_id\"]\n).cumcount(ascending=False).eq(0)\n\n# Late or near home?\nlast_time_dt = pd.to_datetime(vis[\"last_time\"], format=\"%H:%M:%S\", errors=\"coerce\")\nlate = last_time_dt.dt.hour.ge(NIGHT_HOUR)\nnear = vis[\"dist_home_m\"].le(NEAR_HOME_M)\n\n# Reclassify terminal 'end' as 'home' if late or near HOME\nvis.loc[\n    last_mask & (vis[\"outcome\"] == \"end\") & (late | near),\n    \"outcome\"\n] = \"home\"\n\nprint(\"\\nOutcome counts after reclassification:\")\nprint(vis[\"outcome\"].value_counts())\n\n# --- Outcome proportions by stop order k in trip ---\n\ncounts = (\n    vis.groupby(\"action_order_in_trip\")[\"outcome\"]\n       .value_counts()\n       .unstack(fill_value=0)\n       .reindex(range(1, MAXK + 1), fill_value=0)\n)\ncounts[\"total\"] = counts.sum(axis=1)\n\nprops = counts.div(\n    counts[\"total\"].where(counts[\"total\"] &gt; 0, np.nan),\n    axis=0\n)\n\nprint(\"\\nOutcome composition (first 10 k):\")\ndisplay(props[[\"explore\", \"home\", \"end\"]].head(10))\n\n\nNon-home visit events inside trips: 1306\n\nRaw next_step value counts (non-home):\nnext_step\npn      632\nhome    338\npv      219\nnone     85\nsw       32\nName: count, dtype: int64\n\nOutcome counts after reclassification:\noutcome\nexplore    883\nhome       403\nend         20\nName: count, dtype: int64\n\nOutcome composition (first 10 k):\n\n\n\n\n\n\n\n\noutcome\nexplore\nhome\nend\n\n\naction_order_in_trip\n\n\n\n\n\n\n\n1\n0.333333\n0.657343\n0.009324\n\n\n2\n0.874126\n0.090909\n0.034965\n\n\n3\n0.648000\n0.328000\n0.024000\n\n\n4\n0.901235\n0.086420\n0.012346\n\n\n5\n0.684932\n0.301370\n0.013699\n\n\n6\n0.880000\n0.100000\n0.020000\n\n\n7\n0.840909\n0.113636\n0.045455\n\n\n8\n0.918919\n0.081081\n0.000000\n\n\n9\n0.911765\n0.088235\n0.000000\n\n\n10\n0.935484\n0.064516\n0.000000\n\n\n\n\n\n\n\nFor user 000, there are 1,306 non-home stops within trips: 883 are followed by another non-home place (explore), 403 are followed by a return home, and only 20 end the trip after the late/near-home reclassification. By stop order 𝑘, the pattern is clear: at the first stop, about two thirds of trips go straight home, while only about one third continue. After that, exploration dominates—starting from 𝑘=2, roughly 65–90% of stops are followed by another non-home visit, and the chance of going home drops to around 10–30%. In short, many outings are one-stop errands, but if the user doesn’t go home immediately, they typically keep chaining stops before the trip ends.\n\n\n4.2 Discrete-time hazard of going home\nBased on the hazards, the analysis also derives the survival probability \\(S_k\\), defined as the probability that a trip is still ongoing after \\(k\\) non-home stops. In discrete time, this is obtained by multiplying \\((1 - h_k)\\) across steps, since \\((1 - h_k)\\) is the probability of not going home immediately after stop \\(k\\). Intuitively, \\(S_k\\) is the probability the person is “still out” after \\(k\\) stops.\nThe first panel plots the empirical hazards \\(h_k\\) alongside a simple logit trend fitted as a function of \\(k\\) (using \\(k \\ge 2\\)). The second panel compares the empirical survival curve \\(S_k\\) with the model-implied survival curve from the fitted hazards. Together, these plots show how both the “go home now” probability and the “still out” probability evolve as trips get longer.\n\n\nShow code\n# Per-trip maximum stop order (within the MAXK truncation)\npertrip_maxk = (\n    vis.groupby([\"person\", \"date\", \"trip_id\"])[\"action_order_in_trip\"]\n       .max()\n)\n\n# Risk set: number of trips that reach at least stop k\nat_risk = pd.Series(\n    {k: int((pertrip_maxk &gt;= k).sum()) for k in range(1, MAXK + 1)},\n    name=\"at_risk\"\n)\n\n# Number of trips where the k-th stop is followed by \"home\"\nhome_k = (\n    vis[vis[\"outcome\"] == \"home\"]\n    .groupby(\"action_order_in_trip\")\n    .size()\n    .reindex(range(1, MAXK + 1), fill_value=0)\n    .rename(\"home_k\")\n)\n\nhazard = (home_k / at_risk.replace(0, np.nan)).rename(\"hazard\")\n\nhaz_df = pd.concat([home_k, at_risk, hazard], axis=1)\nprint(\"Hazard table (first 10 k):\")\ndisplay(haz_df.head(10))\n\n# ---------- Empirical survival S_k ----------\nk_axis = np.arange(1, MAXK + 1)\nhaz_vals_emp = hazard.reindex(k_axis).fillna(0).values\n\nS_emp = []\ns = 1.0\nfor hk in haz_vals_emp:\n    s *= (1 - hk)\n    S_emp.append(s)\nS_emp = np.array(S_emp)\n\n# ---------- Simple logit trend for hazard (k&gt;=2) ----------\nimport statsmodels.api as sm\n\nMIN_RISK = 20\n\nhaz_fit = (\n    haz_df\n    .loc[haz_df.index &gt;= 2] \n    .loc[lambda df: df[\"at_risk\"] &gt;= MIN_RISK]\n    .copy()\n)\n\nprint(\"Ks used in hazard fit:\", haz_fit.index.tolist())\n\nk_fit = haz_fit.index.values.astype(float)\nX = sm.add_constant(k_fit)\n\ny = np.column_stack([\n    haz_fit[\"home_k\"].values,\n    (haz_fit[\"at_risk\"] - haz_fit[\"home_k\"]).values\n])\n\nglm = sm.GLM(y, X, family=sm.families.Binomial())\nres_haz = glm.fit()\nprint(res_haz.summary2().tables[1])\n\nalpha = res_haz.params[0]\nbeta  = res_haz.params[1]\n\nX_pred = sm.add_constant(k_axis[1:])\nhaz_hat_tail = res_haz.predict(X_pred)\n\nh1_emp = haz_df.loc[1, \"hazard\"]\nhaz_hat = np.concatenate([[h1_emp], haz_hat_tail])\n\nS_hat = []\ns = 1.0\nfor hk in haz_hat:\n    s *= (1 - hk)\n    S_hat.append(s)\nS_hat = np.array(S_hat)\n\n# ---------- Plot hazard: empirical vs fitted ----------\nplt.figure(figsize=(10, 4))\nplt.plot(k_axis, haz_vals_emp, marker=\"o\", label=\"Empirical hazard\")\nplt.plot(k_axis, haz_hat, linestyle=\"--\", label=\"Logit trend (k≥2)\")\nplt.ylim(0, 1)\nplt.xlabel(\"Stop order k in trip\")\nplt.ylabel(\"P(go home | trip has reached k)\")\nplt.title(f\"Hazard of going home after k-th stop in a trip — user {USER_ID}\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# ---------- Plot survival: empirical vs fitted ----------\nplt.figure(figsize=(10, 4))\nplt.plot(k_axis, S_emp, marker=\"o\", label=\"Empirical S_k\")\nplt.plot(k_axis, S_hat, linestyle=\"--\", label=\"Model-based S_k\")\nplt.ylim(0, 1)\nplt.xlabel(\"Stop order k in trip\")\nplt.ylabel(\"P(still out after k)\")\nplt.title(f\"Survival of staying out within a trip — user {USER_ID}\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nHazard table (first 10 k):\n\n\n\n\n\n\n\n\n\nhome_k\nat_risk\nhazard\n\n\n\n\n1\n282\n429\n0.657343\n\n\n2\n13\n143\n0.090909\n\n\n3\n41\n125\n0.328000\n\n\n4\n7\n81\n0.086420\n\n\n5\n22\n73\n0.301370\n\n\n6\n5\n50\n0.100000\n\n\n7\n5\n44\n0.113636\n\n\n8\n3\n37\n0.081081\n\n\n9\n3\n34\n0.088235\n\n\n10\n2\n31\n0.064516\n\n\n\n\n\n\n\nKs used in hazard fit: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n          Coef.  Std.Err.         z         P&gt;|z|    [0.025   0.975]\nconst -1.217758  0.194916 -6.247605  4.167954e-10 -1.599786 -0.83573\nx1    -0.092337  0.033081 -2.791279  5.250023e-03 -0.157174 -0.02750\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3 Within-Trip Exploration: When Do New Places (Pv) Appear?\nThe analysis now shifts from “going home” to within-trip exploration. For each non-home stop, the place label tells us whether the stop is a first-time place (\\(Pv\\)) or a return place (\\(Pn\\)).\nTo see how exploration changes as a trip progresses, I order non-home stops by their within-trip stop number \\(k\\) (1st stop after leaving HOME, 2nd stop, etc.). For each \\(k\\), I compute the fraction of stops that are \\(Pv\\). Plotting this fraction against \\(k\\) gives an empirical curve showing when new places tend to appear during an outing.\nFor user 000, about 18% of non-home stops are first-time places. The \\(Pv\\) rate is relatively low at early stops, then rises as trips continue, reaching roughly 30% around stops 8–10. To summarize the pattern with a smooth trend, I fit a simple logit regression of whether a stop is \\(Pv\\) on \\(k\\), using only stop numbers with sufficient observations. The fitted effect implies that each additional stop increases the odds of visiting a new place by roughly 25%.\n\n\nShow code\n# Non-home visits inside trips (same base as for hazard)\nvis_places = visit_table[\n    (visit_table[\"is_home\"] == 0) &\n    (visit_table[\"trip_id\"] &gt; 0) &\n    (visit_table[\"action_order_in_trip\"] &gt; 0)\n].copy()\n\nvis_places = vis_places[vis_places[\"action_order_in_trip\"] &lt;= MAXK].copy()\n\n# Counts by stop order: how often is the stop Pv vs Pn?\npv_by_k = (\n    vis_places\n    .groupby(\"action_order_in_trip\")[[\"is_pv\", \"is_pn\"]]\n    .agg(\n        pv_cnt=(\"is_pv\", \"sum\"),\n        pn_cnt=(\"is_pn\", \"sum\"),\n        n=(\"is_pv\", \"size\")\n    )\n    .reset_index()\n)\n\npv_by_k[\"pv_rate\"] = pv_by_k[\"pv_cnt\"] / (\n    pv_by_k[\"pv_cnt\"] + pv_by_k[\"pn_cnt\"]\n).replace(0, np.nan)\n\noverall_pv = vis_places[\"is_pv\"].mean()\n\nprint(\"Overall share of first-time places (Pv) among non-home trip stops:\",\n      round(overall_pv, 3))\nprint(\"\\nPv rate by stop order k (first 10 k):\")\ndisplay(pv_by_k[[\"action_order_in_trip\", \"pv_rate\"]].head(10))\n\n\nOverall share of first-time places (Pv) among non-home trip stops: 0.181\n\nPv rate by stop order k (first 10 k):\n\n\n\n\n\n\n\n\n\naction_order_in_trip\npv_rate\n\n\n\n\n0\n1\n0.097561\n\n\n1\n2\n0.153285\n\n\n2\n3\n0.126126\n\n\n3\n4\n0.164557\n\n\n4\n5\n0.144928\n\n\n5\n6\n0.265306\n\n\n6\n7\n0.279070\n\n\n7\n8\n0.297297\n\n\n8\n9\n0.272727\n\n\n9\n10\n0.322581\n\n\n\n\n\n\n\n\n\nShow code\npv_by_k = pv_by_k.copy() \n\nMIN_N = 30\nvalid_k = pv_by_k.loc[pv_by_k[\"n\"] &gt;= MIN_N, \"action_order_in_trip\"]\nprint(\"Ks with n &gt;= 30:\", valid_k.tolist())\n\nvis_lr_sub = vis_places[\n    vis_places[\"action_order_in_trip\"].isin(valid_k)\n].copy()\n\nvis_lr_sub[\"k_centered\"] = (\n    vis_lr_sub[\"action_order_in_trip\"]\n    - vis_lr_sub[\"action_order_in_trip\"].mean()\n)\n\nX = sm.add_constant(vis_lr_sub[\"k_centered\"])\ny = vis_lr_sub[\"is_pv\"]\n\nlogit_model = sm.Logit(y, X)\nres = logit_model.fit(disp=False)\nprint(res.summary2().tables[1])\n\nbeta0 = res.params[\"const\"]\nbeta1 = res.params[\"k_centered\"]\n\nodds_ratio = np.exp(beta1)\nprint(f\"Odds ratio for Pv per additional stop (n&gt;={MIN_N}): {odds_ratio:.3f}\")\n\nk_max = int(valid_k.max())\nk_grid = np.arange(1, k_max + 1)\nk_grid_c = k_grid - vis_lr_sub[\"action_order_in_trip\"].mean()\nlin_pred = beta0 + beta1 * k_grid_c\np_hat = 1 / (1 + np.exp(-lin_pred))\n\npv_plot = pv_by_k[pv_by_k[\"action_order_in_trip\"] &lt;= k_max]\n\nplt.figure(figsize=(8,4))\nplt.plot(\n    pv_plot[\"action_order_in_trip\"],\n    pv_plot[\"pv_rate\"],\n    marker=\"o\",\n    label=\"Empirical Pv share\"\n)\nplt.plot(\n    k_grid,\n    p_hat,\n    linestyle=\"--\",\n    label=f\"Logit trend (n≥{MIN_N})\"\n)\nplt.axhline(\n    overall_pv,\n    color=\"gray\",\n    linestyle=\"--\",\n    linewidth=1,\n    label=\"Overall Pv share\"\n)\nplt.ylim(0, 0.5)\nplt.xlim(1, k_max)\nplt.xlabel(\"Stop order k in trip (non-home)\")\nplt.ylabel(\"P(stop is Pv | stop at k)\")\nplt.title(f\"Within-trip Pv probability — user {USER_ID}\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nKs with n &gt;= 30: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n               Coef.  Std.Err.          z         P&gt;|z|    [0.025    0.975]\nconst      -2.074610  0.103630 -20.019307  3.738832e-89 -2.277722 -1.871498\nk_centered  0.236422  0.031935   7.403330  1.328116e-13  0.173831  0.299013\nOdds ratio for Pv per additional stop (n&gt;=30): 1.267\n\n\n\n\n\n\n\n\n\n\n\n4.4 Behavioural summary\n\nEveryday mobility is dominated by short, nearby home–home trips. Most outings contain only a few non-home stops, and the typical activity radius is tightly clustered around HOME; long-distance excursions show up as rare outliers.\nWithin a single trip, there is a large mass of “one-stop errands” that return straight home, producing a very high probability of going home after the first stop. Conditional on surviving this initial stage, the per-stop hazard of returning home declines with stop order: once a trip has been running for a while, the person becomes increasingly “sticky” to staying out.\nAlong the same trip, early stops are mostly revisits to familiar locations, whereas the share of first-time places (Pv) rises steadily with stop order. Exploration is therefore concentrated in the middle and later parts of a trip, once the outing has “warmed up”.\n\nTaken together, these patterns can be summarised as follows: most days involve short errands close to home; when a longer outing does occur, the person tends to stay out for a while and becomes progressively more likely to explore new places.",
    "crumbs": [
      "Analysis",
      "Notebook 2 – Trips by action"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Results and Discussion",
    "section": "",
    "text": "Notebook 2 (within-user modelling) and Notebook 3 (cross-user comparison) together show which aspects of daily mobility are common across people and which vary substantially between individuals. This section therefore emphasises the behavioural patterns implied by the Geolife sample, rather than the implementation details.",
    "crumbs": [
      "Analysis",
      "Notebook 4 - Findings and conclusion"
    ]
  },
  {
    "objectID": "conclusion.html#population-level-behavioural-structure",
    "href": "conclusion.html#population-level-behavioural-structure",
    "title": "Results and Discussion",
    "section": "1. Population-level behavioural structure",
    "text": "1. Population-level behavioural structure\nAcross long-coverage users, a consistent structural pattern appears: daily movement is organised into short, home-anchored outings rather than long, continuous trajectories. Most observed behaviour can be represented as sequences of HOME → non-home stop(s) → HOME, supporting the use of home–home trips (non-home visits bracketed by home) as a meaningful unit for describing everyday mobility.",
    "crumbs": [
      "Analysis",
      "Notebook 4 - Findings and conclusion"
    ]
  },
  {
    "objectID": "conclusion.html#heterogeneity-in-trip-termination-going-home",
    "href": "conclusion.html#heterogeneity-in-trip-termination-going-home",
    "title": "Results and Discussion",
    "section": "2. Heterogeneity in trip termination (“going home”)",
    "text": "2. Heterogeneity in trip termination (“going home”)\nWhile the home–home structure is shared, how trips end differs sharply across users. The discrete-time hazard curves show substantial heterogeneity in both level and shape. Many users exhibit a pronounced one-stop errand effect—a high probability of returning home immediately after the first non-home stop—whereas others show a weaker early spike. A smaller subset display relatively higher hazards later in the trip, though these late-trip patterns often coincide with sparse data at large stop orders. The most consistent population-level feature is that, once a trip extends beyond the first few stops, the probability of returning home typically drops and stabilises at a lower level, rather than continuing to rise steadily with trip length.",
    "crumbs": [
      "Analysis",
      "Notebook 4 - Findings and conclusion"
    ]
  },
  {
    "objectID": "conclusion.html#heterogeneity-in-exploration-first-time-places-pv",
    "href": "conclusion.html#heterogeneity-in-exploration-first-time-places-pv",
    "title": "Results and Discussion",
    "section": "3. Heterogeneity in exploration (first-time places, Pv)",
    "text": "3. Heterogeneity in exploration (first-time places, Pv)\nExploration behaviour is also diverse. For many users, the probability that a stop is a first-time place (Pv) increases with stop order, implying that novelty-seeking tends to occur after the outing has already continued for multiple stops (i.e., mid- to late-trip exploration). For other users, Pv probability is nearly flat across stop order, consistent with trips dominated by revisits to familiar locations. Overall, stop order is a useful axis for explaining exploration dynamics, but there is no single universal Pv curve that fits everyone.",
    "crumbs": [
      "Analysis",
      "Notebook 4 - Findings and conclusion"
    ]
  },
  {
    "objectID": "conclusion.html#user-typology-and-systematic-differences",
    "href": "conclusion.html#user-typology-and-systematic-differences",
    "title": "Results and Discussion",
    "section": "4. User typology and systematic differences",
    "text": "4. User typology and systematic differences\nWhen trip frequency, trip complexity (stops per trip), going-home parameters, and exploration parameters are combined into a multivariate profile, users do not form a single homogeneous group. Clustering in this feature space consistently separates three behavioural types: (1) multi-stop revisitors, characterised by longer trips with low exploration; (2) errand-oriented users, characterised by high first-stop return probabilities and shorter outings; and (3) exploratory roamers, characterised by fewer outings but comparatively stronger exploration conditional on being out. Their separation in PCA space suggests that these differences reflect systematic behavioural variation, not only random sampling noise.",
    "crumbs": [
      "Analysis",
      "Notebook 4 - Findings and conclusion"
    ]
  },
  {
    "objectID": "conclusion.html#summary",
    "href": "conclusion.html#summary",
    "title": "Results and Discussion",
    "section": "5. Summary",
    "text": "5. Summary\nDaily mobility in Geolife therefore shows a clear shared backbone—movement structured into home–home trips and a tendency for “going home” probabilities to settle into a lower regime after early stops—alongside substantial, interpretable individual differences. The strongest sources of differentiation are (i) the strength of the one-stop errand pattern, (ii) whether exploration increases with stop order, and (iii) how users balance novelty against routine across trips.",
    "crumbs": [
      "Analysis",
      "Notebook 4 - Findings and conclusion"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geolife Daily Mobility",
    "section": "",
    "text": "I’m Jingqi (Luciano) Lu. This site is a notebook-style project using Geolife GPS trajectories to build visit events, reconstruct home-based trips, and summarize within-trip “going home” and exploration patterns.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Geolife Daily Mobility",
    "section": "",
    "text": "I’m Jingqi (Luciano) Lu. This site is a notebook-style project using Geolife GPS trajectories to build visit events, reconstruct home-based trips, and summarize within-trip “going home” and exploration patterns.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#notebooks",
    "href": "index.html#notebooks",
    "title": "Geolife Daily Mobility",
    "section": "Notebooks",
    "text": "Notebooks\n\nNotebook 0: Overview\n\nNotebook 1: Data cleaning → visit-level table (HOME/SW, Pv/Pn, visits)\n\nNotebook 2: Trip reconstruction and within-trip behaviour (next-step, hazard/survival, Pv trend)\n\nNotebook 3: Cross-user checks (user summaries, hazard/Pv curves, clustering)\n\nNotebook 4: Conclusion (key findings, limitations, next steps)\nReference",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#data-and-outputs",
    "href": "index.html#data-and-outputs",
    "title": "Geolife Daily Mobility",
    "section": "Data and outputs",
    "text": "Data and outputs\nData: Microsoft Research Geolife GPS trajectories.\nMain output files include visit_level_table_XXX.csv, plus trip summaries and fitted parameters produced in later notebooks.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#reproducibility",
    "href": "index.html#reproducibility",
    "title": "Geolife Daily Mobility",
    "section": "Reproducibility",
    "text": "Reproducibility\nRun notebooks top-to-bottom. If needed, update paths under the Geolife Trajectories 1.3/ folder.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Geolife Daily Mobility",
    "section": "Contact",
    "text": "Contact\nGitHub: https://github.com/lluluciano0505/\nEmail: lluluciano0505@gmail.com",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "Reference.html",
    "href": "Reference.html",
    "title": "References",
    "section": "",
    "text": "Nielsen, M. (2019). Augmented intelligence: The future of human intelligence. Distill.\nPappalardo, L., Rinzivillo, S., Qu, Z., Pedreschi, D., & Giannotti, F. (2015). Understanding the patterns of human mobility in cities. EPJ Data Science, 4(1), 1–23.\nPappalardo, L., Rinzivillo, S., & Simini, F. (2016). Human mobility modelling: Exploration and preferential return meet the gravity model. Physica A: Statistical Mechanics and its Applications, 462, 201–214.\nPirolli, P., & Card, S. K. (1999). Information foraging. Psychological Review, 106(4), 643–675.\nSchläpfer, M., Dong, L., O’Keefe, K., Santi, P., Szell, M., Salat, H., … Ratti, C. (2021). The universal visitation law of human mobility. Nature, 593, 522–527.\nSong, C., Koren, T., Wang, P., & Barabási, A.-L. (2010). Modelling the scaling properties of human mobility. Nature Physics, 6(10), 818–823.\nSong, C., Qu, Z., Blumm, N., & Barabási, A.-L. (2010). Limits of predictability in human mobility. Science, 327(5968), 1018–1021.\nZheng, Y., Xie, X., & Ma, W.-Y. (2010). GeoLife: A collaborative social networking service among user, location and trajectory. IEEE Data Engineering Bulletin, 33(2), 32–39.",
    "crumbs": [
      "Analysis",
      "Reference"
    ]
  }
]