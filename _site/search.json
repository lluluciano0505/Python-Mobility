[
  {
    "objectID": "verification.html",
    "href": "verification.html",
    "title": "Notebook 3 — Cross-user verification of daily exploration patterns",
    "section": "",
    "text": "Notebook 3 moves from single-user analysis to a cross-user perspective on daily mobility. Building on the visit-level tables constructed in the previous notebooks, it aggregates data for a set of long-coverage Geolife users (data/visit_level_table_XXX.csv) and asks how their trip-level “going home” and exploration behaviour compares.\nThe workflow has three components.\nFirst, I define a consistent sample of users with sufficiently long observation windows and, for each of them, reconstruct home–home trips by attaching a trip identifier and within-trip stop order to every visit.\nSecond, I summarise each user’s mobility with a compact set of trip-level statistics, including the number of active days, the frequency and length of home–home trips, and simple indicators of how often non-home stops correspond to first-time places (Pv) versus revisits.\nThird, I compare going-home hazards and within-trip exploration patterns across individuals by fitting user-specific discrete-time logit models in the stop order index, which yield a small number of interpretable parameters describing both the overall level and the within-trip dynamics of “going home” and “exploring” for each person.\n\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"DataFrameGroupBy.apply operated on the grouping columns\",\n    category=FutureWarning,\n)\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"Parsing dates in %d/%m/%Y format\",\n    category=UserWarning,\n)\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\nplt.style.use(\"default\")\n\nDATA_DIR = \"data\"  # folder containing visit_level_table_XXX.csv\n\n# Pre-selected long-coverage users (can be updated if needed)\nLONG_USERS = [\n    \"004\", \"003\", \"017\", \"025\", \"030\", \"126\",\n    \"062\", \"084\", \"039\", \"041\", \"022\", \"014\", \"000\",\n    \"002\", \"092\", \"112\", \"104\", \"052\"\n]\n\n\ndef load_visit_table_from_csv(user_id: str,\n                              data_dir: str = DATA_DIR,\n                              verbose: bool = True) -&gt; pd.DataFrame | None:\n    \"\"\"\n    Load visit_level_table_XXX.csv for one user and parse timestamps.\n    Assumes the schema from Notebook 1 / 2:\n      - person\n      - date, first_time, last_time\n      - is_home, next_step, dist_home_m\n      - visit_order_in_day, is_pv, is_pn\n    \"\"\"\n    fname = f\"visit_level_table_{user_id}.csv\"\n    path = os.path.join(data_dir, fname)\n\n    if not os.path.exists(path):\n        if verbose:\n            print(f\"[skip] {fname} not found at {path}\")\n        return None\n\n    vt = pd.read_csv(path)\n    if vt.empty:\n        if verbose:\n            print(f\"[skip] {fname} is empty\")\n        return None\n\n    vt[\"date\"] = pd.to_datetime(vt[\"date\"]).dt.date\n    vt[\"first_dt\"] = pd.to_datetime(vt[\"date\"].astype(str) + \" \" + vt[\"first_time\"])\n    vt[\"last_dt\"] = pd.to_datetime(vt[\"date\"].astype(str) + \" \" + vt[\"last_time\"])\n\n    vt = (\n        vt.sort_values([\"person\", \"date\", \"first_dt\"])\n          .reset_index(drop=True)\n    )\n    return vt\n\n\ndef add_trip_cols_one_day(df_day: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    For a single (person, date), label home–home trip episodes.\n\n    trip_id:\n      0 if no trip yet; 1,2,... for successive trips in that day\n    action_order_in_trip:\n      0 for HOME visits; 1,2,... for non-home stops within a trip\n    \"\"\"\n    df_day = df_day.sort_values(\"first_dt\").copy()\n\n    trip_id = 0\n    action_order = 0\n    out = False  # currently away from HOME?\n\n    trip_ids = []\n    action_orders = []\n\n    for _, row in df_day.iterrows():\n        if row[\"is_home\"] == 1:\n            out = False\n            action_order = 0\n            trip_ids.append(trip_id)\n            action_orders.append(0)\n        else:\n            if not out:\n                trip_id += 1\n                action_order = 1\n                out = True\n            else:\n                action_order += 1\n\n            trip_ids.append(trip_id)\n            action_orders.append(action_order)\n\n    df_day[\"trip_id\"] = trip_ids\n    df_day[\"action_order_in_trip\"] = action_orders\n    return df_day\n\n\ndef attach_trip_structure(vt: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Apply add_trip_cols_one_day to each (person, date).\"\"\"\n    vt_trips = (\n        vt.groupby([\"person\", \"date\"], group_keys=False)\n          .apply(add_trip_cols_one_day)\n          .reset_index(drop=True)\n    )\n    return vt_trips",
    "crumbs": [
      "Analysis",
      "Notebook 3 – Cross-user verification"
    ]
  },
  {
    "objectID": "verification.html#cross-user-within-trip-exploration-pv",
    "href": "verification.html#cross-user-within-trip-exploration-pv",
    "title": "Notebook 3 — Cross-user verification of daily exploration patterns",
    "section": "4. Cross-user within-trip exploration (Pv)",
    "text": "4. Cross-user within-trip exploration (Pv)\nIn this step I summarise within-trip exploration behaviour across users using the Pv logit model. For each user, I first restrict attention to non-home stops within home–home trips and count, for each stop order (k ), how many of these stops are first-time places (Pv) versus revisits (Pn). Using these counts, I estimate a user-specific logit model where the stop order in the trip enters as a linear predictor (after recentring), and use the fitted model to obtain a smooth Pv probability curve over (k). The resulting fitted curves are then overlaid across users, so that differences in both the baseline exploration rate and the slope with respect to stop order can be inspected on a common scale.\n\nMAXK_PV = 12      # maximum stop order for Pv curves\nMIN_N_PV = 20     # minimum n per k for fitting / error\n\n\ndef compute_pv_curve(vt_trips: pd.DataFrame,\n                     maxk: int = MAXK_PV,\n                     min_n: int = MIN_N_PV) -&gt; dict | None:\n    \"\"\"\n    Given a visit table with trip_id and action_order_in_trip,\n    compute:\n      - empirical Pv share by stop order k\n      - fitted Pv probability via logit in k\n    Returns dict with:\n      - k_axis, pv_emp, pv_hat, pv_df\n    \"\"\"\n    vis_places = vt_trips[\n        (vt_trips[\"is_home\"] == 0) &\n        (vt_trips[\"trip_id\"] &gt; 0) &\n        (vt_trips[\"action_order_in_trip\"] &gt; 0)\n    ].copy()\n    vis_places = vis_places[vis_places[\"action_order_in_trip\"] &lt;= maxk].copy()\n\n    if vis_places.empty:\n        return None\n\n    pv_by_k = (\n        vis_places\n        .groupby(\"action_order_in_trip\")[[\"is_pv\", \"is_pn\"]]\n        .agg(\n            pv_cnt=(\"is_pv\", \"sum\"),\n            pn_cnt=(\"is_pn\", \"sum\"),\n            n=(\"is_pv\", \"size\")\n        )\n    )\n\n    pv_by_k = pv_by_k.reindex(range(1, maxk + 1), fill_value=0)\n\n    denom = pv_by_k[\"pv_cnt\"] + pv_by_k[\"pn_cnt\"]\n    pv_emp = (pv_by_k[\"pv_cnt\"] / denom.where(denom &gt; 0, np.nan)).to_numpy()\n\n    k_axis = np.arange(1, maxk + 1)\n\n    valid_k = pv_by_k.index[pv_by_k[\"n\"] &gt;= min_n]\n    if len(valid_k) == 0:\n        return {\n            \"k_axis\": k_axis,\n            \"pv_emp\": pv_emp,\n            \"pv_hat\": pv_emp.copy(),\n            \"pv_df\": pv_by_k.reset_index().rename(columns={\"index\": \"k\"})\n        }\n\n    vis_lr_sub = vis_places[\n        vis_places[\"action_order_in_trip\"].isin(valid_k)\n    ].copy()\n\n    mean_k = vis_lr_sub[\"action_order_in_trip\"].mean()\n    vis_lr_sub[\"k_centered\"] = vis_lr_sub[\"action_order_in_trip\"] - mean_k\n\n    X = sm.add_constant(vis_lr_sub[\"k_centered\"])\n    y = vis_lr_sub[\"is_pv\"]\n\n    logit_model = sm.Logit(y, X)\n    res = logit_model.fit(disp=False)\n\n    beta0 = res.params[\"const\"]\n    beta1 = res.params[\"k_centered\"]\n\n    k_centered_grid = k_axis - mean_k\n    lin_pred = beta0 + beta1 * k_centered_grid\n    pv_hat = 1.0 / (1.0 + np.exp(-lin_pred))\n\n    pv_df = pv_by_k.reset_index().rename(columns={\"index\": \"k\"})\n\n    return {\n        \"k_axis\": k_axis,\n        \"pv_emp\": pv_emp,\n        \"pv_hat\": pv_hat,\n        \"pv_df\": pv_df,\n    }\n\n\n# compute Pv curves for all users\npv_curves = {}\n\nfor uid in summary_df[\"uid\"]:\n    vt = load_visit_table_from_csv(uid, verbose=False)\n    if vt is None or vt.empty:\n        continue\n\n    vt_trips = attach_trip_structure(vt)\n    curve = compute_pv_curve(vt_trips, maxk=MAXK_PV, min_n=MIN_N_PV)\n    if curve is not None:\n        pv_curves[uid] = curve\n\nprint(\"Users with usable Pv curves:\", len(pv_curves))\n\n\n# overlay fitted Pv curves across users\nplt.figure(figsize=(8, 5))\n\nfor uid, curve in pv_curves.items():\n    k_axis = curve[\"k_axis\"]\n    pv_hat = curve[\"pv_hat\"]\n    n_k = curve[\"pv_df\"][\"n\"].to_numpy()\n\n    mask_fit = n_k &gt;= MIN_N_PV\n    if not mask_fit.any():\n        continue\n\n    plt.plot(\n        k_axis[mask_fit],\n        pv_hat[mask_fit],\n        alpha=0.6,\n        linewidth=1,\n        label=uid\n    )\n\nplt.ylim(0, 0.5)\nplt.xlim(1, MAXK_PV)\nplt.xlabel(\"Stop order k in trip (non-home)\")\nplt.ylabel(\"P(stop is Pv)\")\nplt.title(\"Within-trip Pv probability — fitted curves across users\")\nplt.legend(ncol=3, fontsize=8, frameon=False)\nplt.tight_layout()\nplt.show()\n\nUsers with usable Pv curves: 18\n\n\n\n\n\n\n\n\n\nThe panel shows the fitted within-trip Pv probability curves for all long-coverage users, truncated at the first ten non-home stops and only displayed where each stop order has at least ten observations. Vertical differences between curves reflect how exploratory a user is on average, while the slope of each curve captures how exploration changes along the sequence of stops within a trip. Many users exhibit a gently upward-sloping pattern, indicating that the probability of visiting a first-time place tends to rise as the trip proceeds, whereas a few users have nearly flat curves, consistent with a roughly constant exploration rate across stops.",
    "crumbs": [
      "Analysis",
      "Notebook 3 – Cross-user verification"
    ]
  },
  {
    "objectID": "verification.html#model-fit-for-hazard-and-pv-curves",
    "href": "verification.html#model-fit-for-hazard-and-pv-curves",
    "title": "Notebook 3 — Cross-user verification of daily exploration patterns",
    "section": "5. Model fit for hazard and Pv curves",
    "text": "5. Model fit for hazard and Pv curves\nTo assess how well the simple logit specifications reproduce the empirical patterns, I construct a user-level goodness-of-fit measure for both models. For each user and each stop order within the range used in the analysis, I compare the empirical probabilities with the corresponding fitted values and compute a mean squared error (MSE) over the available stop orders. For the going-home hazard, the MSE is based on all non-missing hazard points, while for the Pv model it is restricted to stop orders with at least MIN_N_PV observations. The resulting MSEs are then merged back into the user summary table, providing a compact diagnostic of how well the fitted curves approximate the empirical profiles for each individual.\n\n# === 5.0 Compute per-user MSE for hazard and Pv ===\n\n# 5.0.1 Hazard MSE for each user\nhaz_fit_rows = []\nfor uid, hc in hazard_curves.items():\n    emp = hc[\"haz_emp\"]      # empirical hazard h_k\n    hat = hc[\"haz_hat\"]      # fitted hazard \\hat{h}_k\n\n    mask = ~np.isnan(emp)\n    if not mask.any():\n        continue\n\n    mse = np.mean((emp[mask] - hat[mask])**2)\n    haz_fit_rows.append({\n        \"uid\": uid,\n        \"hazard_mse\": mse,\n    })\n\nhaz_fit_df = (\n    pd.DataFrame(haz_fit_rows)\n      .sort_values(\"uid\")\n      .reset_index(drop=True)\n)\n\n\n# 5.0.2 Pv MSE for each user\npv_fit_rows = []\nfor uid, curve in pv_curves.items():\n    emp = curve[\"pv_emp\"]    # empirical Pv share by k\n    hat = curve[\"pv_hat\"]    # fitted Pv probability by k\n    n_k = curve[\"pv_df\"][\"n\"].to_numpy()  # sample size per k\n\n    mask = (~np.isnan(emp)) & (n_k &gt;= MIN_N_PV)\n    if not mask.any():\n        continue\n\n    mse = np.mean((emp[mask] - hat[mask])**2)\n    pv_fit_rows.append({\n        \"uid\": uid,\n        \"pv_mse\": mse,\n    })\n\npv_fit_df = (\n    pd.DataFrame(pv_fit_rows)\n      .sort_values(\"uid\")\n      .reset_index(drop=True)\n)\n\n\n# 5.1 Hazard MSE summary\nh_min   = haz_fit_df[\"hazard_mse\"].min()\nh_med   = haz_fit_df[\"hazard_mse\"].median()\nh_max   = haz_fit_df[\"hazard_mse\"].max()\n\nbest_haz  = haz_fit_df.nsmallest(2, \"hazard_mse\")\nworst_haz = haz_fit_df.nlargest(2, \"hazard_mse\")\n\nprint(f\"Hazard MSE range: {h_min:.4f} – {h_max:.4f}, median {h_med:.4f}\")\nprint(\"Best hazard fits:\")\nprint(best_haz.to_string(index=False))\n\nprint(\"Worst hazard fits:\")\nprint(worst_haz.to_string(index=False))\n\n\n# 5.2 Pv MSE summary\np_min   = pv_fit_df[\"pv_mse\"].min()\np_med   = pv_fit_df[\"pv_mse\"].median()\np_max   = pv_fit_df[\"pv_mse\"].max()\n\nbest_pv  = pv_fit_df.nsmallest(2, \"pv_mse\")\nworst_pv = pv_fit_df.nlargest(2, \"pv_mse\")\n\nprint(f\"\\nPv MSE range: {p_min:.4f} – {p_max:.4f}, median {p_med:.4f}\")\nprint(\"Best Pv fits:\")\nprint(best_pv.to_string(index=False))\n\nprint(\"Worst Pv fits:\")\nprint(worst_pv.to_string(index=False))\n\n# 5.3 Merge fit stats into summary_df and save\nsummary_fit = (\n    summary_df\n    .merge(haz_fit_df, on=\"uid\", how=\"left\")\n    .merge(pv_fit_df,  on=\"uid\", how=\"left\")\n)\nsummary_fit.to_csv(\"data/user_trip_summary_with_fit.csv\", index=False)\n\nHazard MSE range: 0.0002 – 0.0138, median 0.0013\nBest hazard fits:\nuid  hazard_mse\n003    0.000184\n030    0.000259\nWorst hazard fits:\nuid  hazard_mse\n112    0.013826\n104    0.009189\n\nPv MSE range: 0.0008 – 0.0091, median 0.0022\nBest Pv fits:\nuid   pv_mse\n030 0.000807\n084 0.001045\nWorst Pv fits:\nuid   pv_mse\n112 0.009124\n041 0.003860\n\n\n\n5.a Hazard fit: best and worst users\nFigure 5a compares the empirical trip-level going-home hazard with the fitted logit curves for the two best-fitting users (003, 030) and the two worst-fitting users (112, 104). For Users 003 and 030, the points lie almost exactly on the fitted line across stop orders 1–12, and the MSE is essentially zero, which means the simple logit trend in k captures their going-home probabilities very well. For Users 112 and 104 the overall MSE is still small (≈0.014 and ≈0.009), but the last few stops show visible deviations: with few long trips, individual “go home” events at high k create noisy empirical hazards that the smooth logit curve only approximates.\n\n# 5.a Hazard fit: 2×2 panels for best and worst users\n\nhaz_best  = [\"003\", \"030\"]\nhaz_worst = [\"112\", \"104\"]\nhaz_order = haz_best + haz_worst   # top row: best, bottom row: worst\n\nhaz_mse_dict = haz_fit_df.set_index(\"uid\")[\"hazard_mse\"].to_dict()\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 6), sharex=True, sharey=True)\n\nfor ax, uid in zip(axes.ravel(), haz_order):\n    hc  = hazard_curves[uid]\n    k   = hc[\"k_axis\"]\n    emp = hc[\"haz_emp\"]\n    hat = hc[\"haz_hat\"]\n\n    mask_emp = ~np.isnan(emp)\n\n    ax.scatter(k[mask_emp], emp[mask_emp], s=25, label=\"Empirical hazard\")\n    ax.plot(k, hat, label=\"Fitted hazard\")\n\n    ax.set_title(f\"User {uid} (MSE={haz_mse_dict[uid]:.3f})\")\n    ax.set_xlim(1, MAXK)\n    ax.set_ylim(0, 1)\n\naxes[0, 0].legend()\n\nfig.suptitle(\"Trip-level going-home hazard — best and worst fits\", y=0.98)\nfig.text(0.5, 0.04, \"Stop order k in trip\", ha=\"center\")\nfig.text(0.04, 0.5, \"P(go home | trip has reached k)\", va=\"center\", rotation=\"vertical\")\n\nplt.tight_layout(rect=[0.06, 0.06, 1, 0.94])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5.b Pv fit: best and worst users\nFigure 5b repeats the same exercise for within-trip exploration. For Users 030 and 084 (best fits), the fitted Pv probability is almost indistinguishable from the empirical Pv share for k up to 10, and the MSE stays around 0.001, indicating that a linear logit trend in stop order is an adequate summary of how exploration evolves within their trips. For Users 112 and 041 (worst fits), the fitted line still gets the general level and slope roughly right, but the empirical points at higher k fluctuate more strongly (especially for 112), reflecting sparse data for long trips rather than a systematic failure of the model.\n\n# 5.b Pv fit: 2×2 panels for best and worst users\n\npv_best  = [\"030\", \"084\"]\npv_worst = [\"112\", \"041\"]\npv_order = pv_best + pv_worst\n\npv_mse_dict = pv_fit_df.set_index(\"uid\")[\"pv_mse\"].to_dict()\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 6), sharex=True, sharey=True)\n\nfor ax, uid in zip(axes.ravel(), pv_order):\n    curve = pv_curves[uid]\n    k     = curve[\"k_axis\"]\n    emp   = curve[\"pv_emp\"]\n    hat   = curve[\"pv_hat\"]\n    n_k   = curve[\"pv_df\"][\"n\"].to_numpy()\n\n    mask_emp = ~np.isnan(emp)\n    mask_fit = (n_k &gt;= MIN_N_PV)\n\n    ax.scatter(k[mask_emp], emp[mask_emp], s=25, label=\"Empirical Pv share\")\n    ax.plot(k[mask_fit], hat[mask_fit], label=\"Fitted Pv probability\")\n\n    ax.set_title(f\"User {uid} (MSE={pv_mse_dict[uid]:.3f})\")\n    ax.set_xlim(1, MAXK_PV)\n    ax.set_ylim(0, 0.5)\n\naxes[0, 0].legend()\n\nfig.suptitle(\"Within-trip Pv probability — best and worst fits\", y=0.98)\nfig.text(0.5, 0.04, \"Stop order k in trip (non-home)\", ha=\"center\")\nfig.text(0.04, 0.5, \"P(stop is Pv)\", va=\"center\", rotation=\"vertical\")\n\nplt.tight_layout(rect=[0.06, 0.06, 1, 0.94])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n6. User typology in multivariate trip space\nFor each of the 18 long-coverage users I build a 9-dimensional summary of mobility (trip frequency and length, going-home hazards, and exploration measures). After standardising these features, a k-means with k = 3 yields three user types.\n\nCluster 0 – multi-stop revisitors\nLong trips (around 7 non-home stops), low and fairly flat going-home hazard, and the lowest exploration rate (about 17% of stops are Pv, odds ratio close to 1). Users mainly chain together several familiar places.\nCluster 1 – errand-homebodies\nShorter trips (about 4–5 stops), the highest first-stop going-home hazard, and moderately high exploration (Pv share around 24%, odds ratio &gt; 1). Many trips look like short errands that quickly return home.\nCluster 2 – exploratory roamers\nFewer and shorter trips (about 3 stops), very low later-stop hazards and the steepest decline in hazard, combined with strong within-trip exploration. Once these users stay out beyond the first stops, they rarely go home and are increasingly likely to visit new places.\n\nTo visualise how these types are separated in the multivariate feature space, I project the nine mobility indicators onto their first three principal components (PC1–PC3), which together explain most of the cross-user variation. The loading structure of the PCA shows that PC1 mainly contrasts long, multi-stop, routinised mobility with short, more exploratory errand-like trips (high loadings for mean/median stops and negative loadings for the first-stop hazard and Pv measures). PC2 is dominated by the total number of trips and days with non-home activity and therefore captures the overall frequency and intensity of going out. PC3 is driven by the later-stop hazard parameters and reflects differences in late-trip behaviour, distinguishing users whose return-home probability rises quickly at higher stop orders from those who continue to stay out and explore.\nThe 3D scatter plot displays each user in this (PC1, PC2, PC3) space, coloured by k-means cluster. Cluster 0 tends to occupy the region with high PC1 (multi-stop, revisiting trips) and intermediate PC2 (moderate going-out frequency). Cluster 1 is concentrated at higher PC2 and somewhat lower PC1, corresponding to frequent but relatively short, errand-like trips with a strong tendency to return home early. Cluster 2 lies towards lower PC2 and lower PC3, representing less frequent but more exploratory outings with persistently low late-trip hazards. In this principal-component space the three clusters form well-separated groups, indicating that the typology captures genuinely distinct patterns of everyday mobility.\n\nfeat_cols = [\n    \"mean_stops_per_trip\",     \n    \"median_stops_per_trip\",\n    \"hazard_h1\",              \n    \"hazard_h2_const\",        \n    \"hazard_beta_k\",          \n    \"overall_pv_share\",       \n    \"pv_odds_ratio_per_stop\",  \n    \"n_trips\",                \n    \"n_days_nonhome\",        \n]\n\nX = summary_fit[feat_cols].dropna()\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nkmeans = KMeans(n_clusters=3, random_state=0)\nsummary_fit[\"cluster3\"] = kmeans.fit_predict(X_scaled)\n\n\npca = PCA(n_components=3)\nPCs = pca.fit_transform(X_scaled)\nsummary_fit[[\"PC1\", \"PC2\", \"PC3\"]] = PCs\n\nloadings = pd.DataFrame(\n    pca.components_.T,\n    index=feat_cols,\n    columns=[\"PC1\", \"PC2\", \"PC3\"]\n)\ndisplay(loadings.round(2))\n\nc:\\Users\\Jingqi\\anaconda3\\envs\\geospatial\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\n\n\n\n\nmean_stops_per_trip\n0.46\n-0.16\n0.06\n\n\nmedian_stops_per_trip\n0.43\n-0.29\n0.14\n\n\nhazard_h1\n-0.36\n0.33\n0.08\n\n\nhazard_h2_const\n-0.29\n0.14\n0.62\n\n\nhazard_beta_k\n0.25\n-0.01\n0.71\n\n\noverall_pv_share\n-0.31\n-0.35\n0.25\n\n\npv_odds_ratio_per_stop\n-0.34\n-0.07\n-0.09\n\n\nn_trips\n0.12\n0.66\n0.07\n\n\nn_days_nonhome\n0.32\n0.45\n-0.05\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D \n\nfig = plt.figure(figsize=(7, 6))\nax = fig.add_subplot(111, projection=\"3d\")\n\ncolors = {0: \"tab:orange\", 1: \"tab:blue\", 2: \"tab:green\"}\n\nfor c in sorted(summary_fit[\"cluster3\"].unique()):\n    sub = summary_fit[summary_fit[\"cluster3\"] == c]\n    ax.scatter(\n        sub[\"PC1\"], sub[\"PC2\"], sub[\"PC3\"],\n        c=colors[c], label=f\"Cluster {c}\", s=40\n    )\n    for _, row in sub.iterrows():\n        ax.text(\n            row[\"PC1\"], row[\"PC2\"], row[\"PC3\"],\n            row[\"uid\"],\n            fontsize=8\n        )\n\nax.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.0%})\")\nax.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.0%})\")\nax.set_zlabel(f\"PC3 ({pca.explained_variance_ratio_[2]:.0%})\")\nax.legend()\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Analysis",
      "Notebook 3 – Cross-user verification"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geolife Daily Mobility",
    "section": "",
    "text": "Welcome!\nThis mini site presents an exploratory case study of daily mobility using the Geolife GPS trajectories. The analysis is organised into three main notebooks plus a short synthesis:\n\nNotebook 1 – Data cleaning\nFrom raw Geolife trajectories to a visit-level table: loading .plt files, filtering to Beijing, projecting to a 500 m grid, and detecting home / stay locations.\nNotebook 2 – Trips by action\nBuilds home–home trips from visit events, then studies within-trip behaviour, including the discrete-time hazard of going home and the probability of visiting new places (Pv).\nNotebook 3 – Cross-user verification\nExtends the trip-level analysis to multiple long-coverage users, compares going-home hazards and exploration profiles, and clusters users into behavioural types.\nFindings and conclusion\nSummarises the main empirical patterns and discusses which aspects of daily mobility appear systematic at the population level and which are strongly user-specific.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Results and Discussion",
    "section": "",
    "text": "The individual-level modelling in Notebook 2 and the cross-user analysis in Notebook 3 together clarify which features of daily mobility exhibit robust population structure and which are strongly heterogeneous across individuals. Rather than revisiting implementation details, this section focuses on the behavioural regularities that emerge from the Geolife sample.",
    "crumbs": [
      "Analysis",
      "Findings and conclusion"
    ]
  },
  {
    "objectID": "conclusion.html#population-level-behavioural-structure",
    "href": "conclusion.html#population-level-behavioural-structure",
    "title": "Results and Discussion",
    "section": "Population-level behavioural structure",
    "text": "Population-level behavioural structure\nAcross all long-coverage Geolife users, daily mobility is systematically organised into short, home-anchored trips rather than long, continuous journeys. Most movement takes the form of repeated “home → one or several non-home stops → home” sequences. This pattern confirms that home–home trips, defined as sequences of non-home visits bracketed by home, are an appropriate structural unit for describing everyday mobility.\nWithin these trips, the dynamics of trip termination are clearly not universal. The discrete-time hazard analysis shows substantial heterogeneity in the shape of the going-home curve. Some users display a pronounced peak in the probability of returning home at the first non-home stop; others exhibit only a modest difference between the first stop and subsequent stops; and for a smaller subset the highest hazards occur later in the trip. What appears to generalise across users is not the location of a single peak, but the broader pattern that, once a trip continues beyond the first few stops, the probability of ending it typically settles into a comparatively low and stable regime.\nExploration behaviour, measured by first-time place visits (Pv), shows an equally diverse set of trajectories. For many users, the probability that a non-home stop is a new place increases with stop order within the trip, indicating that exploration tends to be concentrated in the middle and later stages of an outing. For others, the Pv probability is nearly flat across stop orders, suggesting that their trips consist predominantly of revisits to familiar locations. These findings support the use of stop order as a meaningful explanatory axis for exploration decisions, while at the same time rejecting any single “universal” Pv curve (such as a strictly monotonic increase) as a plausible population-level law.\nThe multivariate summary of trip counts, stop counts, hazards and exploration rates further indicates that users do not form a homogeneous behavioural population. Clustering in this feature space yields three coherent mobility types that can be interpreted as: users who undertake relatively long, multi-stop trips concentrated in familiar places; users who make shorter, errand-like trips with relatively high early termination; and users who go out less frequently but, when they do, remain away from home longer and visit new places more intensively. These three groups occupy distinct regions in principal-component space, which suggests that the observed heterogeneity is structured rather than driven by noise.\nTaken together, the cross-user results depict daily mobility as a system with a shared backbone and substantial individual differentiation. The shared backbone lies in the decomposition of behaviour into home–home trips and in the stabilisation of ending probabilities once a trip is underway. The individual differentiation lies in how strongly early stops matter, how exploration evolves along the trip, and how each person combines revisiting and novelty. It is precisely this combination of common structure and user-specific variation that characterises the population-level mobility patterns in the Geolife data.",
    "crumbs": [
      "Analysis",
      "Findings and conclusion"
    ]
  },
  {
    "objectID": "by action.html",
    "href": "by action.html",
    "title": "Notebook 2 — Trip-level exploration and “going home” behaviour",
    "section": "",
    "text": "In this notebook the analysis starts from the visit-level table constructed in Notebook 1. The file visit_level_table_000.csv is loaded from the data folder and the basic Python environment (NumPy, pandas, Matplotlib) is set up. Date and time fields are combined into full start and end timestamps for each visit (first_dt and last_dt), and the user identifier is taken from the person column for use in plots. The table is then sorted by person, date, and start time to produce a clean, time-ordered record of visit events rather than daily aggregates; subsequent sections build on this event sequence to identify complete home–home trips (individual “outings”).\nimport warnings\n\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"DataFrameGroupBy.apply operated on the grouping columns\",\n    category=FutureWarning,\n)\n# 1. Environment and data loading\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nplt.style.use(\"default\")\n\n# Path to the visit-level table saved from Notebook 1\npath = \"data/visit_level_table_000.csv\"\nvisit_table = pd.read_csv(path)\n\nprint(\"Rows in visit_table:\", len(visit_table))\ndisplay(visit_table.head())\n\n# Parse date + times into full timestamps\nvisit_table[\"date\"] = pd.to_datetime(visit_table[\"date\"]).dt.date\n\nvisit_table[\"first_dt\"] = pd.to_datetime(\n    visit_table[\"date\"].astype(str) + \" \" + visit_table[\"first_time\"]\n)\nvisit_table[\"last_dt\"] = pd.to_datetime(\n    visit_table[\"date\"].astype(str) + \" \" + visit_table[\"last_time\"]\n)\n\n# Identify the user ID from the table\nUSER_ID = str(visit_table[\"person\"].iloc[0])\nprint(\"Unique persons in this file:\", visit_table[\"person\"].unique())\nprint(\"USER_ID used in plots:\", USER_ID)\n\n# Ensure rows are ordered in time\nvisit_table = (\n    visit_table\n    .sort_values([\"person\", \"date\", \"first_dt\"])\n    .reset_index(drop=True)\n)\nprint(\"Rows in visit_table after sorting:\", len(visit_table))\n\nRows in visit_table: 1841\n\n\n\n\n\n\n\n\n\nperson\ndate\nfirst_time\nlast_time\nduration_min\ngrid_ci\ngrid_rj\ngrid_center_lon\ngrid_center_lat\ndist_home_m\nis_home\nis_sw\nis_pv\nis_pn\nplace_id\nnext_step\nvisit_order_in_day\naction_order\n\n\n\n\n0\n0\n2008-10-23\n03:00:55\n04:13:32\n72.6\n40\n50\n116.300184\n39.984308\n3201.6\n0\n0\n1\n0\nPv1\nsw\n1\n1\n\n\n1\n0\n2008-10-23\n09:42:25\n09:42:30\n0.1\n43\n55\n116.317527\n40.006935\n500.0\n0\n1\n0\n0\nSW\nhome\n2\n2\n\n\n2\n0\n2008-10-23\n09:42:35\n10:05:29\n22.9\n44\n55\n116.323385\n40.006970\n0.0\n1\n0\n0\n0\nHOME\nsw\n3\n0\n\n\n3\n0\n2008-10-23\n10:05:34\n10:30:15\n24.7\n43\n55\n116.317527\n40.006935\n500.0\n0\n1\n0\n0\nSW\nhome\n4\n1\n\n\n4\n0\n2008-10-23\n10:30:20\n11:10:57\n40.6\n44\n55\n116.323385\n40.006970\n0.0\n1\n0\n0\n0\nHOME\nnone\n5\n0\n\n\n\n\n\n\n\nUnique persons in this file: [0]\nUSER_ID used in plots: 0\nRows in visit_table after sorting: 1841",
    "crumbs": [
      "Analysis",
      "Notebook 2 – Trips by action"
    ]
  },
  {
    "objectID": "by action.html#from-daily-visits-to-homehome-trips",
    "href": "by action.html#from-daily-visits-to-homehome-trips",
    "title": "Notebook 2 — Trip-level exploration and “going home” behaviour",
    "section": "2. From daily visits to home–home trips",
    "text": "2. From daily visits to home–home trips\nThe next step is to group the visit-level events into home–home trip episodes. Within each (person, date) sequence (ordered by first_dt), visits with is_home == 1 are treated as being at HOME. A new trip starts when the next visit is to a non-home place, continues through consecutive non-home visits, and ends when HOME is observed again; if there is no return to HOME, the trip is taken to end at the last non-home visit of the day.\nTwo indices are attached to every visit. The trip_id variable identifies which trip on that day a visit belongs to (1, 2, …), with 0 reserved for visits that are not part of any trip. The action_order_in_trip variable records the position of the visit within the trip: it takes values 1, 2, 3, … for non-home visits after leaving HOME and 0 for HOME visits at the start or end of a trip.\n\ndef add_trip_cols(df_day):\n    \"\"\"\n    For a single (person, date), label home–home trip episodes.\n\n    - trip_id: which trip on that day (1, 2, ...), 0 if not in a trip.\n    - action_order_in_trip: order within the current trip\n      (1, 2, 3, ... for non-home visits, 0 for HOME).\n    \"\"\"\n    df_day = df_day.sort_values(\"first_dt\").copy()\n\n    trip_id = 0\n    action_order = 0\n    out = False  # currently away from HOME?\n\n    trip_ids = []\n    action_orders = []\n\n    for _, row in df_day.iterrows():\n        if row[\"is_home\"] == 1:\n            # Being at HOME: mark as outside any within-trip sequence\n            out = False\n            action_order = 0\n            trip_ids.append(trip_id)\n            action_orders.append(0)\n        else:\n            # Non-home visit\n            if not out:\n                # Leaving home: start a new trip\n                trip_id += 1\n                action_order = 1\n                out = True\n            else:\n                # Continuing within the same trip\n                action_order += 1\n\n            trip_ids.append(trip_id)\n            action_orders.append(action_order)\n\n    df_day[\"trip_id\"] = trip_ids\n    df_day[\"action_order_in_trip\"] = action_orders\n    return df_day\n\nvisit_table = (\n    visit_table\n    .groupby([\"person\", \"date\"], group_keys=False)\n    .apply(add_trip_cols)\n    .reset_index(drop=True)\n)\n\nprint(\"Trip-related columns (first 20 rows):\")\ndisplay(\n    visit_table[\n        [\"person\", \"date\", \"place_id\", \"is_home\",\n         \"visit_order_in_day\", \"trip_id\", \"action_order_in_trip\",\n         \"first_time\", \"last_time\"]\n    ].head(10)\n)\n\nprint(\"Number of trips (trip_id &gt; 0):\", (visit_table[\"trip_id\"] &gt; 0).sum())\n\nTrip-related columns (first 20 rows):\n\n\n\n\n\n\n\n\n\nperson\ndate\nplace_id\nis_home\nvisit_order_in_day\ntrip_id\naction_order_in_trip\nfirst_time\nlast_time\n\n\n\n\n0\n0\n2008-10-23\nPv1\n0\n1\n1\n1\n03:00:55\n04:13:32\n\n\n1\n0\n2008-10-23\nSW\n0\n2\n1\n2\n09:42:25\n09:42:30\n\n\n2\n0\n2008-10-23\nHOME\n1\n3\n1\n0\n09:42:35\n10:05:29\n\n\n3\n0\n2008-10-23\nSW\n0\n4\n2\n1\n10:05:34\n10:30:15\n\n\n4\n0\n2008-10-23\nHOME\n1\n5\n2\n0\n10:30:20\n11:10:57\n\n\n5\n0\n2008-10-24\nSW\n0\n1\n1\n1\n02:09:59\n02:10:54\n\n\n6\n0\n2008-10-24\nHOME\n1\n2\n1\n0\n02:10:59\n02:47:06\n\n\n7\n0\n2008-10-26\nPv4\n0\n1\n1\n1\n14:04:27\n14:12:42\n\n\n8\n0\n2008-10-26\nPv5\n0\n2\n1\n2\n14:23:42\n14:35:17\n\n\n9\n0\n2008-10-27\nHOME\n1\n1\n0\n0\n12:03:59\n12:05:54\n\n\n\n\n\n\n\nNumber of trips (trip_id &gt; 0): 1805",
    "crumbs": [
      "Analysis",
      "Notebook 2 – Trips by action"
    ]
  },
  {
    "objectID": "by action.html#trip-level-descriptive-statistics",
    "href": "by action.html#trip-level-descriptive-statistics",
    "title": "Notebook 2 — Trip-level exploration and “going home” behaviour",
    "section": "3. Trip-level descriptive statistics",
    "text": "3. Trip-level descriptive statistics\nThe visit-level data are then collapsed to the trip level. For each (person, date, trip_id) combination, a single row is constructed with:\n\nn_stops: number of non-home stops in the trip\n\ntrip_start, trip_end: start and end timestamps of the trip\n\nmax_dist_home, mean_dist_home: maximum and average distance from HOME reached during the trip\n\nFrom this trip-level table, simple descriptive summaries and histograms of n_stops and max_dist_home describe how long typical outings last and how far the user tends to travel away from home.\n\n# Keep only visits that belong to a trip\nvt_trip = visit_table[visit_table[\"trip_id\"] &gt; 0].copy()\n\ntrip_summary = (\n    vt_trip\n    .groupby([\"person\", \"date\", \"trip_id\"], as_index=False)\n    .agg(\n        n_stops=(\"action_order_in_trip\", lambda x: (x &gt; 0).sum()),\n        trip_start=(\"first_dt\", \"min\"),\n        trip_end=(\"last_dt\", \"max\"),\n        max_dist_home=(\"dist_home_m\", \"max\"),\n        mean_dist_home=(\"dist_home_m\", \"mean\"),\n    )\n)\n\ntrip_summary[\"trip_duration_min\"] = (\n    (trip_summary[\"trip_end\"] - trip_summary[\"trip_start\"])\n    .dt.total_seconds() / 60.0\n)\n\nprint(\"Number of trips:\", len(trip_summary))\ndisplay(trip_summary.head())\n\n# --- 3.1 Histogram of stops per trip ---\nplt.figure(figsize=(6, 4))\nplt.hist(trip_summary[\"n_stops\"], bins=20)\nplt.xlabel(\"Number of non-home stops in trip\")\nplt.ylabel(\"Number of trips\")\nplt.title(f\"Stops per trip — user {USER_ID}\")\nplt.tight_layout()\nplt.show()\n\n# --- 3.2 Histogram of max distance from HOME per trip ---\nplt.figure(figsize=(6, 4))\nplt.hist(trip_summary[\"max_dist_home\"], bins=30)\nplt.xlabel(\"Maximum distance from HOME (m)\")\nplt.ylabel(\"Number of trips\")\nplt.title(f\"Maximum distance from HOME per trip — user {USER_ID}\")\nplt.tight_layout()\nplt.show()\n\nNumber of trips: 429\n\n\n\n\n\n\n\n\n\nperson\ndate\ntrip_id\nn_stops\ntrip_start\ntrip_end\nmax_dist_home\nmean_dist_home\ntrip_duration_min\n\n\n\n\n0\n0\n2008-10-23\n1\n2\n2008-10-23 03:00:55\n2008-10-23 10:05:29\n3201.6\n1233.866667\n424.566667\n\n\n1\n0\n2008-10-23\n2\n1\n2008-10-23 10:05:34\n2008-10-23 11:10:57\n500.0\n250.000000\n65.383333\n\n\n2\n0\n2008-10-24\n1\n1\n2008-10-24 02:09:59\n2008-10-24 02:47:06\n500.0\n250.000000\n37.116667\n\n\n3\n0\n2008-10-26\n1\n2\n2008-10-26 14:04:27\n2008-10-26 14:35:17\n13416.4\n12857.400000\n30.833333\n\n\n4\n0\n2008-10-28\n1\n20\n2008-10-28 00:38:26\n2008-10-28 02:52:11\n2549.5\n2214.280952\n133.750000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor user 000 there are 429 identified home–home trips. The histogram of n_stops shows that most trips are very simple: the median is one non-home stop and the vast majority of outings contain only one or two stops, with a long but thin right tail of unusually complex trips (up to nearly 100 recorded stops). The distribution of max_dist_home tells a similar story on the spatial side. Most trips remain within a few kilometres of HOME, but there are occasional long-distance excursions that reach tens of kilometres away. Together, these summaries highlight a pattern of frequent short errands around home punctuated by rare, much longer journeys.",
    "crumbs": [
      "Analysis",
      "Notebook 2 – Trips by action"
    ]
  },
  {
    "objectID": "by action.html#next-step-behaviour-within-trips",
    "href": "by action.html#next-step-behaviour-within-trips",
    "title": "Notebook 2 — Trip-level exploration and “going home” behaviour",
    "section": "4. Next-step behaviour within trips",
    "text": "4. Next-step behaviour within trips\n\n4.1 Calculate the non home stops\nThe focus now shifts from whole trips to individual non-home stops inside each trip. For every such stop, the main question is: what happens next?\nWithin trip episodes, three next-step outcomes are distinguished:\n\nhome: the next recorded visit is to HOME\n\nexplore: the next visit is to another non-home place (SW, Pv, or Pn)\n\nend: the trip ends in the data with no further visit recorded\n\nFor the last non-home stop of each trip, an end outcome is treated as effectively going home if the stop is either late at night (hour ≥ 23) or already very close to HOME (within 750 m). This reclassification avoids counting end-of-day records as if the person were still out, and makes the outcome categories more comparable across stops. The resulting outcome labels are then tabulated by stop order (k) within the trip to describe how the composition of “go home”, “keep exploring”, and “end” changes as a trip unfolds.\n\nMAXK = 30        # only consider the first 30 stops within each trip\nNIGHT_HOUR = 23  # &gt;= 23:00 considered \"late\"\nNEAR_HOME_M = 750\n\n# Non-home visits that are part of a trip and have a positive action order\nvis = visit_table[\n    (visit_table[\"is_home\"] == 0) &\n    (visit_table[\"trip_id\"] &gt; 0) &\n    (visit_table[\"action_order_in_trip\"] &gt; 0)\n].copy()\n\nvis = vis[vis[\"action_order_in_trip\"] &lt;= MAXK].copy()\n\nprint(\"Non-home visit events inside trips:\", len(vis))\nprint(\"\\nRaw next_step value counts (non-home):\")\nprint(vis[\"next_step\"].value_counts())\n\n# --- Classify next-step outcomes ---\n\nvis[\"outcome\"] = np.where(\n    vis[\"next_step\"] == \"home\", \"home\",\n    np.where(vis[\"next_step\"].isin([\"sw\", \"pv\", \"pn\"]), \"explore\", \"end\")\n)\n\n# Identify the last non-home visit of each trip\nlast_mask = vis.groupby(\n    [\"person\", \"date\", \"trip_id\"]\n).cumcount(ascending=False).eq(0)\n\n# Late or near home?\nlast_time_dt = pd.to_datetime(vis[\"last_time\"], format=\"%H:%M:%S\", errors=\"coerce\")\nlate = last_time_dt.dt.hour.ge(NIGHT_HOUR)\nnear = vis[\"dist_home_m\"].le(NEAR_HOME_M)\n\n# Reclassify terminal 'end' as 'home' if late or near HOME\nvis.loc[\n    last_mask & (vis[\"outcome\"] == \"end\") & (late | near),\n    \"outcome\"\n] = \"home\"\n\nprint(\"\\nOutcome counts after reclassification:\")\nprint(vis[\"outcome\"].value_counts())\n\n# --- Outcome proportions by stop order k in trip ---\n\ncounts = (\n    vis.groupby(\"action_order_in_trip\")[\"outcome\"]\n       .value_counts()\n       .unstack(fill_value=0)\n       .reindex(range(1, MAXK + 1), fill_value=0)\n)\ncounts[\"total\"] = counts.sum(axis=1)\n\nprops = counts.div(\n    counts[\"total\"].where(counts[\"total\"] &gt; 0, np.nan),\n    axis=0\n)\n\nprint(\"\\nOutcome composition (first 10 k):\")\ndisplay(props[[\"explore\", \"home\", \"end\"]].head(10))\n\nNon-home visit events inside trips: 1306\n\nRaw next_step value counts (non-home):\nnext_step\npn      632\nhome    338\npv      219\nnone     85\nsw       32\nName: count, dtype: int64\n\nOutcome counts after reclassification:\noutcome\nexplore    883\nhome       403\nend         20\nName: count, dtype: int64\n\nOutcome composition (first 10 k):\n\n\n\n\n\n\n\n\noutcome\nexplore\nhome\nend\n\n\naction_order_in_trip\n\n\n\n\n\n\n\n1\n0.333333\n0.657343\n0.009324\n\n\n2\n0.874126\n0.090909\n0.034965\n\n\n3\n0.648000\n0.328000\n0.024000\n\n\n4\n0.901235\n0.086420\n0.012346\n\n\n5\n0.684932\n0.301370\n0.013699\n\n\n6\n0.880000\n0.100000\n0.020000\n\n\n7\n0.840909\n0.113636\n0.045455\n\n\n8\n0.918919\n0.081081\n0.000000\n\n\n9\n0.911765\n0.088235\n0.000000\n\n\n10\n0.935484\n0.064516\n0.000000\n\n\n\n\n\n\n\nFor user 000 there are 1,306 non-home stops inside trips, of which 883 are followed by another non-home place (explore), 403 by a return home, and only 20 end the trip without a further visit after the late/near-home reclassification. The composition by stop order (k) shows a clear pattern: at the first stop, going home is very common (around two thirds of trips return home immediately and only one third continue to explore). Once a trip survives this first stop, however, exploration dominates: for (k = 2) and beyond, 65–90% of stops are followed by another non-home place and the hazard of going home falls to around 10–30%. In other words, many trips are short one-stop errands, but conditional on not going straight home, the user tends to string together several exploratory stops before ending the outing.\n\n\n4.2 Discrete-time hazard of going home\nNon-home stops inside a trip are treated as discrete time steps: first stop after leaving HOME, second stop, third stop, and so on. For each step number (k), the data across all trips are used to compute:\n\nthe number of trips that have reached at least the (k)-th non-home stop; and\n\namong those trips, the number that go straight back HOME immediately after stop (k).\n\nTheir ratio defines the hazard of going home at step (k),h_k = P(go home next | trip has reached stop k), which answers: “given that this trip has already made it to stop (k), what is the chance that the next move is to go home rather than continue exploring?”.\nBy multiplying ((1 - h_k)) over successive steps, the analysis also obtains the survival probability (S_k), the probability that a trip is still ongoing (the user is still out) after (k) non-home stops. The first panel plots the empirical hazards (h_k) together with a simple logit trend in (k) fitted for (k ), while the second panel compares the empirical survival curve (S_k) with the corresponding model-based curve. Together, these plots show how the “go home now” probability and the “still out” probability evolve as trips become longer.\n\n# Per-trip maximum stop order (within the MAXK truncation)\npertrip_maxk = (\n    vis.groupby([\"person\", \"date\", \"trip_id\"])[\"action_order_in_trip\"]\n       .max()\n)\n\n# Risk set: number of trips that reach at least stop k\nat_risk = pd.Series(\n    {k: int((pertrip_maxk &gt;= k).sum()) for k in range(1, MAXK + 1)},\n    name=\"at_risk\"\n)\n\n# Number of trips where the k-th stop is followed by \"home\"\nhome_k = (\n    vis[vis[\"outcome\"] == \"home\"]\n    .groupby(\"action_order_in_trip\")\n    .size()\n    .reindex(range(1, MAXK + 1), fill_value=0)\n    .rename(\"home_k\")\n)\n\nhazard = (home_k / at_risk.replace(0, np.nan)).rename(\"hazard\")\n\nhaz_df = pd.concat([home_k, at_risk, hazard], axis=1)\nprint(\"Hazard table (first 10 k):\")\ndisplay(haz_df.head(10))\n\n# ---------- Empirical survival S_k ----------\nk_axis = np.arange(1, MAXK + 1)\nhaz_vals_emp = hazard.reindex(k_axis).fillna(0).values\n\nS_emp = []\ns = 1.0\nfor hk in haz_vals_emp:\n    s *= (1 - hk)\n    S_emp.append(s)\nS_emp = np.array(S_emp)\n\n# ---------- Simple logit trend for hazard (k&gt;=2) ----------\nimport statsmodels.api as sm\n\nMIN_RISK = 20\n\nhaz_fit = (\n    haz_df\n    .loc[haz_df.index &gt;= 2] \n    .loc[lambda df: df[\"at_risk\"] &gt;= MIN_RISK]\n    .copy()\n)\n\nprint(\"Ks used in hazard fit:\", haz_fit.index.tolist())\n\nk_fit = haz_fit.index.values.astype(float)\nX = sm.add_constant(k_fit)\n\ny = np.column_stack([\n    haz_fit[\"home_k\"].values,\n    (haz_fit[\"at_risk\"] - haz_fit[\"home_k\"]).values\n])\n\nglm = sm.GLM(y, X, family=sm.families.Binomial())\nres_haz = glm.fit()\nprint(res_haz.summary2().tables[1])\n\nalpha = res_haz.params[0]\nbeta  = res_haz.params[1]\n\nX_pred = sm.add_constant(k_axis[1:])\nhaz_hat_tail = res_haz.predict(X_pred)\n\nh1_emp = haz_df.loc[1, \"hazard\"]\nhaz_hat = np.concatenate([[h1_emp], haz_hat_tail])\n\nS_hat = []\ns = 1.0\nfor hk in haz_hat:\n    s *= (1 - hk)\n    S_hat.append(s)\nS_hat = np.array(S_hat)\n\n# ---------- Plot hazard: empirical vs fitted ----------\nplt.figure(figsize=(10, 4))\nplt.plot(k_axis, haz_vals_emp, marker=\"o\", label=\"Empirical hazard\")\nplt.plot(k_axis, haz_hat, linestyle=\"--\", label=\"Logit trend (k≥2)\")\nplt.ylim(0, 1)\nplt.xlabel(\"Stop order k in trip\")\nplt.ylabel(\"P(go home | trip has reached k)\")\nplt.title(f\"Hazard of going home after k-th stop in a trip — user {USER_ID}\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# ---------- Plot survival: empirical vs fitted ----------\nplt.figure(figsize=(10, 4))\nplt.plot(k_axis, S_emp, marker=\"o\", label=\"Empirical S_k\")\nplt.plot(k_axis, S_hat, linestyle=\"--\", label=\"Model-based S_k\")\nplt.ylim(0, 1)\nplt.xlabel(\"Stop order k in trip\")\nplt.ylabel(\"P(still out after k)\")\nplt.title(f\"Survival of staying out within a trip — user {USER_ID}\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nHazard table (first 10 k):\n\n\n\n\n\n\n\n\n\nhome_k\nat_risk\nhazard\n\n\n\n\n1\n282\n429\n0.657343\n\n\n2\n13\n143\n0.090909\n\n\n3\n41\n125\n0.328000\n\n\n4\n7\n81\n0.086420\n\n\n5\n22\n73\n0.301370\n\n\n6\n5\n50\n0.100000\n\n\n7\n5\n44\n0.113636\n\n\n8\n3\n37\n0.081081\n\n\n9\n3\n34\n0.088235\n\n\n10\n2\n31\n0.064516\n\n\n\n\n\n\n\nKs used in hazard fit: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n          Coef.  Std.Err.         z         P&gt;|z|    [0.025   0.975]\nconst -1.217758  0.194916 -6.247605  4.167954e-10 -1.599786 -0.83573\nx1    -0.092337  0.033081 -2.791279  5.250023e-03 -0.157174 -0.02750\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3 Within-trip exploration: when do new places (Pv) appear?\nThe focus now shifts from going-home decisions to exploration within a trip. For every non-home stop it is known whether the location is\n\na first-time place for this user (Pv), or\n\na return visit to an already known place (Pn).\n\nLet (k) denote the stop order within a trip. For each (k) the analysis computes the proportion of non-home stops that are Pv and tracks how this share changes with (k). This yields an empirical curve P(stop is Pv | stop order = k) that describes when new places tend to appear along the sequence of stops.\nFor user 000 the overall share of first-time places among non-home trip stops is about 18%. The Pv rate is relatively low at the first few stops and then increases with stop order, reaching around 30% by the 8th–10th stop. A simple logit regression of the Pv indicator on stop order (restricted to values of (k) with at least 30 observations) provides the smooth trend line in the figure. The estimated odds ratio of about 1.27 per additional stop implies that, within a trip, each extra stop raises the odds of visiting a new place by roughly one quarter.\n\n# Non-home visits inside trips (same base as for hazard)\nvis_places = visit_table[\n    (visit_table[\"is_home\"] == 0) &\n    (visit_table[\"trip_id\"] &gt; 0) &\n    (visit_table[\"action_order_in_trip\"] &gt; 0)\n].copy()\n\nvis_places = vis_places[vis_places[\"action_order_in_trip\"] &lt;= MAXK].copy()\n\n# Counts by stop order: how often is the stop Pv vs Pn?\npv_by_k = (\n    vis_places\n    .groupby(\"action_order_in_trip\")[[\"is_pv\", \"is_pn\"]]\n    .agg(\n        pv_cnt=(\"is_pv\", \"sum\"),\n        pn_cnt=(\"is_pn\", \"sum\"),\n        n=(\"is_pv\", \"size\")\n    )\n    .reset_index()\n)\n\npv_by_k[\"pv_rate\"] = pv_by_k[\"pv_cnt\"] / (\n    pv_by_k[\"pv_cnt\"] + pv_by_k[\"pn_cnt\"]\n).replace(0, np.nan)\n\noverall_pv = vis_places[\"is_pv\"].mean()\n\nprint(\"Overall share of first-time places (Pv) among non-home trip stops:\",\n      round(overall_pv, 3))\nprint(\"\\nPv rate by stop order k (first 10 k):\")\ndisplay(pv_by_k[[\"action_order_in_trip\", \"pv_rate\"]].head(10))\n\nOverall share of first-time places (Pv) among non-home trip stops: 0.181\n\nPv rate by stop order k (first 10 k):\n\n\n\n\n\n\n\n\n\naction_order_in_trip\npv_rate\n\n\n\n\n0\n1\n0.097561\n\n\n1\n2\n0.153285\n\n\n2\n3\n0.126126\n\n\n3\n4\n0.164557\n\n\n4\n5\n0.144928\n\n\n5\n6\n0.265306\n\n\n6\n7\n0.279070\n\n\n7\n8\n0.297297\n\n\n8\n9\n0.272727\n\n\n9\n10\n0.322581\n\n\n\n\n\n\n\n\npv_by_k = pv_by_k.copy() \n\nMIN_N = 30\nvalid_k = pv_by_k.loc[pv_by_k[\"n\"] &gt;= MIN_N, \"action_order_in_trip\"]\nprint(\"Ks with n &gt;= 30:\", valid_k.tolist())\n\nvis_lr_sub = vis_places[\n    vis_places[\"action_order_in_trip\"].isin(valid_k)\n].copy()\n\nvis_lr_sub[\"k_centered\"] = (\n    vis_lr_sub[\"action_order_in_trip\"]\n    - vis_lr_sub[\"action_order_in_trip\"].mean()\n)\n\nX = sm.add_constant(vis_lr_sub[\"k_centered\"])\ny = vis_lr_sub[\"is_pv\"]\n\nlogit_model = sm.Logit(y, X)\nres = logit_model.fit(disp=False)\nprint(res.summary2().tables[1])\n\nbeta0 = res.params[\"const\"]\nbeta1 = res.params[\"k_centered\"]\n\nodds_ratio = np.exp(beta1)\nprint(f\"Odds ratio for Pv per additional stop (n&gt;={MIN_N}): {odds_ratio:.3f}\")\n\nk_max = int(valid_k.max())\nk_grid = np.arange(1, k_max + 1)\nk_grid_c = k_grid - vis_lr_sub[\"action_order_in_trip\"].mean()\nlin_pred = beta0 + beta1 * k_grid_c\np_hat = 1 / (1 + np.exp(-lin_pred))\n\npv_plot = pv_by_k[pv_by_k[\"action_order_in_trip\"] &lt;= k_max]\n\nplt.figure(figsize=(8,4))\nplt.plot(\n    pv_plot[\"action_order_in_trip\"],\n    pv_plot[\"pv_rate\"],\n    marker=\"o\",\n    label=\"Empirical Pv share\"\n)\nplt.plot(\n    k_grid,\n    p_hat,\n    linestyle=\"--\",\n    label=f\"Logit trend (n≥{MIN_N})\"\n)\nplt.axhline(\n    overall_pv,\n    color=\"gray\",\n    linestyle=\"--\",\n    linewidth=1,\n    label=\"Overall Pv share\"\n)\nplt.ylim(0, 0.5)\nplt.xlim(1, k_max)\nplt.xlabel(\"Stop order k in trip (non-home)\")\nplt.ylabel(\"P(stop is Pv | stop at k)\")\nplt.title(f\"Within-trip Pv probability — user {USER_ID}\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nKs with n &gt;= 30: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n               Coef.  Std.Err.          z         P&gt;|z|    [0.025    0.975]\nconst      -2.074610  0.103630 -20.019307  3.738832e-89 -2.277722 -1.871498\nk_centered  0.236422  0.031935   7.403330  1.328116e-13  0.173831  0.299013\nOdds ratio for Pv per additional stop (n&gt;=30): 1.267\n\n\n\n\n\n\n\n\n\n\n\n4.3 Behavioural summary\n\nEveryday mobility is dominated by short, nearby home–home trips. Most outings contain only a few non-home stops, and the typical activity radius is tightly clustered around HOME; long-distance excursions show up as rare outliers.\nWithin a single trip, there is a large mass of “one-stop errands” that return straight home, producing a very high probability of going home after the first stop. Conditional on surviving this initial stage, the per-stop hazard of returning home declines with stop order: once a trip has been running for a while, the person becomes increasingly “sticky” to staying out.\nAlong the same trip, early stops are mostly revisits to familiar locations, whereas the share of first-time places (Pv) rises steadily with stop order. Exploration is therefore concentrated in the middle and later parts of a trip, once the outing has “warmed up”.\n\nTaken together, these patterns can be summarised as follows: most days involve short errands close to home; when a longer outing does occur, the person tends to stay out for a while and becomes progressively more likely to explore new places.",
    "crumbs": [
      "Analysis",
      "Notebook 2 – Trips by action"
    ]
  },
  {
    "objectID": "Data Cleaning.html",
    "href": "Data Cleaning.html",
    "title": "Notebook 1 — From raw Geolife trajectories to visit-level events",
    "section": "",
    "text": "This notebook shows how raw Geolife GPS trajectories for one user are turned into a cleaned visit-level dataset. I load and filter the original .plt files, restrict the data to Beijing, and project all points onto a 500 m grid. Night-time points are used to infer the user’s home (HOME) and a secondary frequently visited place (SW), and a simple movement-based rule selects additional grid cells as daily activity locations. Consecutive points within the same activity cell are then collapsed into single visit events and labelled as HOME, SW, first-time place (Pv), or return place (Pn). The resulting visit-level table is the starting point for all subsequent analyses.",
    "crumbs": [
      "Analysis",
      "Notebook 1 – Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#environment-and-data-paths",
    "href": "Data Cleaning.html#environment-and-data-paths",
    "title": "Notebook 1 — From raw Geolife trajectories to visit-level events",
    "section": "1. Environment and data paths",
    "text": "1. Environment and data paths\nThe Geolife data folder Geolife Trajectories 1.3 is stored in the same directory as this notebook. Below I set the base directory and import the Python packages used throughout the analysis.\n\nimport os\nimport glob\nimport math\n\nimport numpy as np\nimport pandas as pd\nimport pyproj\n\n# Path to the Geolife data (relative to the notebook)\nBASE_DIR = os.path.join(\"Geolife Trajectories 1.3\", \"Data\")\n\n# Example user (we start with user 000)\nUSER_ID = \"000\"\n\n# Optional: restrict to Beijing area\nFILTER_BEIJING = True\nBEIJING_BBOX = (115.42, 39.44, 117.50, 41.06)   # (minLon, minLat, maxLon, maxLat)\n\n# Column names for Geolife .plt files (after skipping the 6-line header)\nPLT_COLS = [\"lat\", \"lon\", \"unused\", \"altitude_feet\", \"days\", \"date\", \"time\"]",
    "crumbs": [
      "Analysis",
      "Notebook 1 – Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#reading-raw-.plt-files-for-one-user",
    "href": "Data Cleaning.html#reading-raw-.plt-files-for-one-user",
    "title": "Notebook 1 — From raw Geolife trajectories to visit-level events",
    "section": "2. Reading raw .plt files for one user",
    "text": "2. Reading raw .plt files for one user\nI first read all available .plt files for user 000, stack them into a single time-ordered table, and inspect the raw GPS points. The interactive map below shows each day’s trajectory as a polyline in geographic coordinates, which makes the overall spatial extent and the dense clusters of movement visible at a glance. Simple summaries of the inter-point time gaps (see the code below) also show that the sampling is quite irregular: some periods have very high-frequency logging, while others contain long gaps or missing days. These data limitations are important to keep in mind when interpreting the later behavioural models.\n\ndef read_one_plt(path, user_id=\"000\"):\n    \"\"\"Read a single Geolife .plt file, clean basic issues, and return a DataFrame.\"\"\"\n    df = pd.read_csv(path, skiprows=6, names=PLT_COLS)\n\n    # Basic cleaning: drop rows with missing coords or time\n    df = df[\n        pd.notnull(df[\"lat\"]) &\n        pd.notnull(df[\"lon\"]) &\n        pd.notnull(df[\"date\"]) &\n        pd.notnull(df[\"time\"])\n    ].copy()\n\n    # Build timestamp\n    df[\"datetime\"] = pd.to_datetime(\n        df[\"date\"].astype(str) + \" \" + df[\"time\"].astype(str),\n        errors=\"coerce\"\n    )\n    df = df[pd.notnull(df[\"datetime\"])].copy()\n\n    # Derive date and hour\n    df[\"user\"] = user_id\n    df[\"file\"] = os.path.basename(path)\n    df[\"date_only\"] = df[\"datetime\"].dt.date\n    df[\"hour\"] = df[\"datetime\"].dt.hour\n\n    # Keep core columns\n    df = df[[\"user\", \"file\", \"datetime\", \"date_only\", \"hour\",\n             \"lat\", \"lon\", \"altitude_feet\"]]\n    return df\n\n\n# Scan all trajectory files for the chosen user\ntraj_glob = os.path.join(BASE_DIR, USER_ID, \"Trajectory\", \"*.plt\")\nfiles = sorted(glob.glob(traj_glob))\n\n\ndfs = []\nfor fp in files:\n    try:\n        dfs.append(read_one_plt(fp, user_id=USER_ID))\n    except Exception as e:\n        print(f\"[warning] failed to read {fp}: {e}\")\n\nif not dfs:\n    raise RuntimeError(\"No trajectories were successfully read. Check the data path.\")\n\ntraj = pd.concat(dfs, ignore_index=True)\ntraj.sort_values(\"datetime\", inplace=True)\ntraj.reset_index(drop=True, inplace=True)\ntraj.head(10)\n\n\n\n\n\n\n\n\nuser\nfile\ndatetime\ndate_only\nhour\nlat\nlon\naltitude_feet\n\n\n\n\n0\n000\n20081023025304.plt\n2008-10-23 02:53:04\n2008-10-23\n2\n39.984702\n116.318417\n492\n\n\n1\n000\n20081023025304.plt\n2008-10-23 02:53:10\n2008-10-23\n2\n39.984683\n116.318450\n492\n\n\n2\n000\n20081023025304.plt\n2008-10-23 02:53:15\n2008-10-23\n2\n39.984686\n116.318417\n492\n\n\n3\n000\n20081023025304.plt\n2008-10-23 02:53:20\n2008-10-23\n2\n39.984688\n116.318385\n492\n\n\n4\n000\n20081023025304.plt\n2008-10-23 02:53:25\n2008-10-23\n2\n39.984655\n116.318263\n492\n\n\n5\n000\n20081023025304.plt\n2008-10-23 02:53:30\n2008-10-23\n2\n39.984611\n116.318026\n493\n\n\n6\n000\n20081023025304.plt\n2008-10-23 02:53:35\n2008-10-23\n2\n39.984608\n116.317761\n493\n\n\n7\n000\n20081023025304.plt\n2008-10-23 02:53:40\n2008-10-23\n2\n39.984563\n116.317517\n496\n\n\n8\n000\n20081023025304.plt\n2008-10-23 02:53:45\n2008-10-23\n2\n39.984539\n116.317294\n500\n\n\n9\n000\n20081023025304.plt\n2008-10-23 02:53:50\n2008-10-23\n2\n39.984606\n116.317065\n505\n\n\n\n\n\n\n\n\nimport folium\ntraj_for_map = traj.copy()\n\n# Center the map at the median lat/lon of all points\ncenter_lat = traj_for_map[\"lat\"].median()\ncenter_lon = traj_for_map[\"lon\"].median()\n\nm_raw = folium.Map(\n    location=[center_lat, center_lon],\n    zoom_start=8,\n    tiles=\"cartodbpositron\"\n)\n\n# Draw one polyline per day to show daily trajectories\nfor d, sub in traj_for_map.groupby(\"date_only\"):\n    sub = sub.sort_values(\"datetime\")\n    if len(sub) &lt; 2:\n        continue\n\n    coords = list(zip(sub[\"lat\"].values, sub[\"lon\"].values))  # (lat, lon) pairs\n    folium.PolyLine(\n        locations=coords,\n        weight=1,\n        opacity=0.4\n    ).add_to(m_raw)\n\nm_raw\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n# Daily number of GPS points\nday_counts = (\n    traj\n    .groupby(\"date_only\")\n    .size()\n    .rename(\"n_points\")\n    .reset_index()\n)\n\n# Simple time-series style plot of counts per day\nday_counts_plot = day_counts.copy()\nday_counts_plot[\"date_only\"] = pd.to_datetime(day_counts_plot[\"date_only\"])\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(7, 3.5))\nplt.plot(\n    day_counts_plot[\"date_only\"],\n    day_counts_plot[\"n_points\"],\n    marker=\"o\",\n    linewidth=1\n)\nplt.xlabel(\"Date\")\nplt.ylabel(\"Number of GPS points\")\nplt.title(\"Daily number of recorded GPS points for user 000\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nI first read all available .plt files for user 000, stack them into a single time-ordered table, and plot the raw trajectories. The interactive map below shows each day’s path as a polyline: most movement is concentrated in Beijing, but there is also a long high-speed corridor to Shanghai and back. The plot of daily point counts confirms that tracking is far from continuous, with some days recorded very densely and others having almost no data. This pattern reflects the fact that the logging device was only turned on intermittently and at changing sampling rates, so all subsequent analyses have to be interpreted conditional.",
    "crumbs": [
      "Analysis",
      "Notebook 1 – Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#spatial-filter-restricting-to-the-beijing-area",
    "href": "Data Cleaning.html#spatial-filter-restricting-to-the-beijing-area",
    "title": "Notebook 1 — From raw Geolife trajectories to visit-level events",
    "section": "3. Spatial filter: restricting to the Beijing area",
    "text": "3. Spatial filter: restricting to the Beijing area\nGeolife users may have trajectories in multiple cities. In this project we focus on the Beijing region. We therefore apply a simple bounding box filter on longitude and latitude.\n\nif FILTER_BEIJING:\n    minLon, minLat, maxLon, maxLat = BEIJING_BBOX\n    before = len(traj)\n    traj = traj[\n        (traj[\"lon\"] &gt;= minLon) & (traj[\"lon\"] &lt;= maxLon) &\n        (traj[\"lat\"] &gt;= minLat) & (traj[\"lat\"] &lt;= maxLat)\n    ].copy()\n    after = len(traj)\n    print(f\"Beijing filter: {before} -&gt; {after} points\")\nelse:\n    print(\"Beijing filter is disabled.\")\n\nBeijing filter: 173870 -&gt; 157646 points",
    "crumbs": [
      "Analysis",
      "Notebook 1 – Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#projection-to-utm-and-construction-of-a-500-m-grid",
    "href": "Data Cleaning.html#projection-to-utm-and-construction-of-a-500-m-grid",
    "title": "Notebook 1 — From raw Geolife trajectories to visit-level events",
    "section": "4. Projection to UTM and construction of a 500 m grid",
    "text": "4. Projection to UTM and construction of a 500 m grid\nTo work in metres and build a regular grid, I project all points from WGS84 (lat/lon) to UTM zone 50N. In this projected coordinate system I construct a 500 m × 500 m grid that covers all observed points and assign each trajectory point to a grid cell (ci, rj). This spatial discretisation helps smooth out GPS noise and small positional jitter—rather than following every wiggly segment of the raw traces, later steps work with visits to grid cells, which are less sensitive to measurement error and over-detailed geometry.\nmaup problem\n\ndef cell_center_lonlat(ci, rj):\n    cx = gx0 + (ci + 0.5) * GRID_SIZE\n    cy = gy0 + (rj + 0.5) * GRID_SIZE\n    lon, lat = to_wgs(cx, cy)\n    return lon, lat\n\n# Coordinate reference systems\nCRS_WGS84 = pyproj.CRS(\"EPSG:4326\")\nCRS_UTM50 = pyproj.CRS(\"EPSG:32650\")\nto_utm = pyproj.Transformer.from_crs(CRS_WGS84, CRS_UTM50, always_xy=True).transform\nto_wgs = pyproj.Transformer.from_crs(CRS_UTM50, CRS_WGS84, always_xy=True).transform\n\n# Lat/lon -&gt; projected (metres)\nx, y = to_utm(traj[\"lon\"].values, traj[\"lat\"].values)\ntraj[\"x\"] = x\ntraj[\"y\"] = y\n\nGRID_SIZE = 500  # metres\n\n# Grid extent with one extra cell of padding on each side\nminx, miny = traj[\"x\"].min(), traj[\"y\"].min()\nmaxx, maxy = traj[\"x\"].max(), traj[\"y\"].max()\n\ngx0 = math.floor(minx / GRID_SIZE) * GRID_SIZE - GRID_SIZE\ngy0 = math.floor(miny / GRID_SIZE) * GRID_SIZE - GRID_SIZE\ngx1 = math.ceil (maxx / GRID_SIZE) * GRID_SIZE + GRID_SIZE\ngy1 = math.ceil (maxy / GRID_SIZE) * GRID_SIZE + GRID_SIZE\n\nncol = int((gx1 - gx0) / GRID_SIZE)\nnrow = int((gy1 - gy0) / GRID_SIZE)\nprint(f\"Grid columns × rows: {ncol} × {nrow} (total {ncol*nrow:,} cells)\")\n\n# Assign each point to a grid cell\ntraj[\"ci\"] = ((traj[\"x\"] - gx0) // GRID_SIZE).astype(int)\ntraj[\"rj\"] = ((traj[\"y\"] - gy0) // GRID_SIZE).astype(int)\n\ntraj[[\"datetime\", \"lat\", \"lon\", \"x\", \"y\", \"ci\", \"rj\"]].head()\n\nGrid columns × rows: 128 × 102 (total 13,056 cells)\n\n\n\n\n\n\n\n\n\ndatetime\nlat\nlon\nx\ny\nci\nrj\n\n\n\n\n0\n2008-10-23 02:53:04\n39.984702\n116.318417\n441807.056623\n4.426282e+06\n43\n50\n\n\n1\n2008-10-23 02:53:10\n39.984683\n116.318450\n441809.858037\n4.426280e+06\n43\n50\n\n\n2\n2008-10-23 02:53:15\n39.984686\n116.318417\n441807.043048\n4.426280e+06\n43\n50\n\n\n3\n2008-10-23 02:53:20\n39.984688\n116.318385\n441804.312590\n4.426280e+06\n43\n50\n\n\n4\n2008-10-23 02:53:25\n39.984655\n116.318263\n441793.868245\n4.426277e+06\n43\n50",
    "crumbs": [
      "Analysis",
      "Notebook 1 – Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#detecting-home-and-sw-from-night-time-points",
    "href": "Data Cleaning.html#detecting-home-and-sw-from-night-time-points",
    "title": "Notebook 1 — From raw Geolife trajectories to visit-level events",
    "section": "5. Detecting HOME and SW from night-time points",
    "text": "5. Detecting HOME and SW from night-time points\nI identify the user’s home location (HOME) and a secondary frequent place (SW) from night-time points between 00:00 and 06:00, assuming that locations where the user repeatedly appears at night are likely to be home-like places. In practice, I first filter the trajectory to these hours and count how many night-time points fall in each 500 m grid cell, also recording the time span between the first and last night-time observation in that cell. HOME is defined as the cell with the largest night-time count, breaking ties by the longest time span. After removing this cell, SW is defined as the second-strongest night-time cell, if such a candidate exists.\n\nnight = traj[(traj[\"hour\"] &gt;= 0) & (traj[\"hour\"] &lt; 6)].copy()\nnight[\"cell\"] = list(zip(night[\"ci\"], night[\"rj\"]))\nprint(\"Number of night-time points:\", len(night))\n\ncell_counts = night[\"cell\"].value_counts()\ncell_counts.head()\n\nNumber of night-time points: 47296\n\n\ncell\n(44, 55)    4779\n(43, 55)    3531\n(44, 54)    3157\n(45, 53)    2547\n(43, 56)    2354\nName: count, dtype: int64\n\n\n\ndef night_span_seconds(cell):\n    sub = night[night[\"cell\"] == cell][\"datetime\"]\n    return (sub.max() - sub.min()).total_seconds() if not sub.empty else 0\n\n# HOME: cell with the largest night-time count; if tied, pick the one with largest time span\ncandidates = cell_counts[cell_counts == cell_counts.max()].index.tolist()\nhome_cell = max(candidates, key=night_span_seconds) if len(candidates) &gt; 1 else cell_counts.index[0]\n\n# SW: second-strongest night-time cell after removing HOME\ncell_counts_wo_home = cell_counts[cell_counts.index != home_cell]\nsw_cell = None\nif not cell_counts_wo_home.empty:\n    cand2 = cell_counts_wo_home[cell_counts_wo_home == cell_counts_wo_home.max()].index.tolist()\n    sw_cell = max(cand2, key=night_span_seconds) if len(cand2) &gt; 1 else cell_counts_wo_home.index[0]\n\nhome_lon, home_lat = cell_center_lonlat(*home_cell)\nprint(f\"HOME cell = {home_cell} @ ({home_lon:.6f}, {home_lat:.6f})\")\n\nif sw_cell is not None:\n    sw_lon, sw_lat = cell_center_lonlat(*sw_cell)\n    print(f\"SW   cell = {sw_cell} @ ({sw_lon:.6f}, {sw_lat:.6f})\")\nelse:\n    print(\"SW   cell = None\")\n\nHOME cell = (44, 55) @ (116.323385, 40.006970)\nSW   cell = (43, 55) @ (116.317527, 40.006935)",
    "crumbs": [
      "Analysis",
      "Notebook 1 – Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#daily-activity-cells-via-linegrid-intersections",
    "href": "Data Cleaning.html#daily-activity-cells-via-linegrid-intersections",
    "title": "Notebook 1 — From raw Geolife trajectories to visit-level events",
    "section": "6. Daily activity cells via line–grid intersections",
    "text": "6. Daily activity cells via line–grid intersections\nNot every grid cell that contains a raw GPS point should be treated as an “activity location”. Points along fast road segments are typically in transit rather than places where the user actually stops. To obtain a more conservative set of daily activity cells, I work with line segments rather than points: for each day I connect consecutive projected points into segments in the (x, y) plane and count, for every grid cell, how many of these segments intersect the cell polygon.\nA cell is classified as an activity location if its number of intersecting segments exceeds a daily threshold. I start from a baseline threshold of 100 crossings, which is high enough to screen out most purely in-transit cells but still retains the dense clusters around home and other frequently visited areas. This value was chosen empirically by inspecting several days of data: smaller thresholds admitted long stretches of road as “places”, whereas larger thresholds began to discard plausible stops. The threshold is then adapted per day to avoid unrealistically rich days: if more than 30 cells clear the threshold on a given day, the threshold is increased and the classification is recomputed.\n\nfrom shapely.geometry import LineString, box\nfrom shapely.strtree import STRtree\n\n# Parameters for the line–grid crossing rule\nTHRESH_START = 100          # initial threshold for \"enough crossings\"\nTHRESH_STEP  = 25          # how much to increase the threshold if a day has too many cells\nTHRESH_MAX   = 1000        # upper bound on the threshold\n\nMAX_PLACES_PER_DAY = 30    # per day: at most this many DISTINCT activity cells (places)\n\n# Cache for grid-cell polygons (speeds things up)\n_poly_cache = {}\n\ndef cell_poly(ci, rj):\n    \"\"\"Return the shapely Polygon for grid cell (ci, rj).\"\"\"\n    key = (ci, rj)\n    if key not in _poly_cache:\n        x0 = gx0 + ci * GRID_SIZE\n        y0 = gy0 + rj * GRID_SIZE\n        _poly_cache[key] = box(x0, y0, x0 + GRID_SIZE, y0 + GRID_SIZE)\n    return _poly_cache[key]\n\ndef query_segments(tree, poly, segs):\n    \"\"\"\n    Wrapper around STRtree.query to handle both 'index' and 'geometry' return types,\n    depending on Shapely version.\n    \"\"\"\n    res = tree.query(poly)\n    if len(res) == 0:\n        return []\n    first = res[0]\n    # Some Shapely versions return indices, some return geometries\n    if isinstance(first, (int, np.integer)):\n        return [segs[i] for i in res]\n    return res\n\n# Containers for results\nvisited_cells_by_day = {}   # date -&gt; set of (ci, rj) cells considered \"activity locations\"\ncross_threshold_used = {}   # date -&gt; threshold value actually used for that day\n\n# We iterate over each date present in the trajectory\ndates = sorted(traj[\"date_only\"].unique().tolist())\nprint(f\"Number of days with data for user {USER_ID}: {len(dates)}\")\n\nfor d in dates:\n    # All points for this day, ordered in time\n    day = traj[traj[\"date_only\"] == d].sort_values(\"datetime\").copy()\n    if len(day) &lt; 2:\n        # Not enough points to form line segments\n        visited_cells_by_day[d] = set()\n        cross_threshold_used[d] = THRESH_START\n        continue\n\n    # Build line segments between consecutive points (in projected coordinates)\n    pts = list(zip(day[\"x\"].values, day[\"y\"].values))\n    segs = [LineString([pts[i], pts[i+1]]) for i in range(len(pts) - 1)]\n\n    # Drop empty / zero-length segments\n    segs = [s for s in segs if (not s.is_empty) and (s.length &gt; 0)]\n    if not segs:\n        visited_cells_by_day[d] = set()\n        cross_threshold_used[d] = THRESH_START\n        continue\n\n    tree = STRtree(segs)\n\n    # Only check the grid cells that actually appear for this day\n    cmin, cmax = int(day[\"ci\"].min()), int(day[\"ci\"].max())\n    rmin, rmax = int(day[\"rj\"].min()), int(day[\"rj\"].max())\n\n    # Start from the base threshold and adapt if needed\n    thr = THRESH_START\n    while True:\n        today_cells = set()\n\n        # Loop over all relevant grid cells for that day\n        for ci in range(cmin, cmax + 1):\n            for rj in range(rmin, rmax + 1):\n                poly = cell_poly(ci, rj)\n                cnt = 0\n\n                # Candidate segments intersecting this cell\n                for seg in query_segments(tree, poly, segs):\n                    if seg.intersects(poly):\n                        cnt += 1\n                        if cnt &gt;= thr:\n                            today_cells.add((ci, rj))\n                            break   # no need to count further for this cell\n\n        # If the day has too many activity CELLS, raise the threshold and try again\n        if len(today_cells) &gt; MAX_PLACES_PER_DAY and thr &lt; THRESH_MAX:\n            thr = min(thr + THRESH_STEP, THRESH_MAX)\n        else:\n            visited_cells_by_day[d] = today_cells\n            cross_threshold_used[d] = thr\n            break\n\n# Summarise: how many activity cells per day, and what threshold was used\nperday_summary = (\n    pd.DataFrame({\n        \"date\": [str(d) for d in dates],\n        \"visited_cells\": [len(visited_cells_by_day[d]) for d in dates],\n        \"threshold_used\": [cross_threshold_used[d] for d in dates],\n    })\n    .sort_values(\"date\")\n    .reset_index(drop=True)\n)\n\nprint(\"Daily activity-cell counts (first 10 days):\")\nperday_summary.head(10)\n\nNumber of days with data for user 000: 122\nDaily activity-cell counts (first 10 days):\n\n\n\n\n\n\n\n\n\ndate\nvisited_cells\nthreshold_used\n\n\n\n\n0\n2008-10-23\n3\n100\n\n\n1\n2008-10-24\n1\n100\n\n\n2\n2008-10-26\n2\n100\n\n\n3\n2008-10-27\n0\n100\n\n\n4\n2008-10-28\n3\n100\n\n\n5\n2008-10-29\n0\n100\n\n\n6\n2008-11-03\n0\n100\n\n\n7\n2008-11-04\n4\n100\n\n\n8\n2008-11-10\n0\n100\n\n\n9\n2008-11-11\n4\n100",
    "crumbs": [
      "Analysis",
      "Notebook 1 – Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#from-daily-activity-cells-to-visit-level-events",
    "href": "Data Cleaning.html#from-daily-activity-cells-to-visit-level-events",
    "title": "Notebook 1 — From raw Geolife trajectories to visit-level events",
    "section": "7. From daily activity cells to visit-level events",
    "text": "7. From daily activity cells to visit-level events\nGiven the set of daily activity cells, I next distinguish first-time places from returns and compress the trajectories into visit-level events. Across the whole observation window, a grid cell is labelled as a first-time place (Pv) on the first date on which it appears as an activity cell; on any later day when the same cell is active it is treated as a return place (Pn). HOME and SW are kept as separate categories and override the Pv/Pn labels.\nUsing these labels, I then build the visit-level table day by day. For each calendar day, the sequence of grid cells visited by the user is first compressed with a simple run-length encoding, so that consecutive points in the same cell form a single block. Each block is looked up in the activity cell set for that day and assigned a tag HOME, SW, Pv or Pn; blocks that are not classified as activity locations are dropped as in-transit segments. Adjacent blocks with the same cell and tag are merged so that brief GPS losses do not split a single stay into multiple visits.\nThe remaining blocks define the visit events. For each visit I record its start and end time, duration, grid indices and cell centre (in both projected and geographic coordinates), distance to HOME, and a place identifier (HOME, SW, Pv#, Pn#). I also keep two within-day order variables: a running visit order that counts all visits during the day, and an action-order index that restarts at 1 when the user leaves HOME and resets to 0 when they return. Finally, each visit is given a next_step label indicating whether the following visit is to home, SW, a Pv, a Pn, or none (end of the day). Stacking all days produces the final visit-level table, which is saved as visit_level_table_000.csv and used as input for the subsequent notebooks.\n\nfrom datetime import timedelta\n\npv_label = {}              # (ci, rj) -&gt; \"Pv#\"\npn_label = {}              # ((ci, rj), date) -&gt; \"Pn#\"\nfirst_visit_date = {}      # (ci, rj) -&gt; first date when this cell is an activity cell\n\npv_counter = 0\npn_counter = 0\n\nfor d in sorted(visited_cells_by_day.keys()):\n    cells = visited_cells_by_day[d]\n    for cell in sorted(cells):\n        if cell not in first_visit_date:\n            # First time this activity cell appears in the whole sample -&gt; Pv\n            pv_counter += 1\n            first_visit_date[cell] = d\n            pv_label[cell] = f\"Pv{pv_counter}\"\n        else:\n            # Subsequent days when the same activity cell is active -&gt; Pn\n            pn_counter += 1\n            pn_label[(cell, d)] = f\"Pn{pn_counter}\"\n\nprint(f\"Total unique Pv cells: {len(pv_label)}\")\nprint(f\"Total Pn labels assigned: {len(pn_label)}\")\n\n\n# --- 7.2 Helper: classify a cell on a given day ---\n\ndef classify_cell(cell, day_date):\n    \"\"\"\n    Classify a grid cell on a given date into:\n      - ('home', 'HOME')\n      - ('sw', 'SW')\n      - ('pv', 'Pv#')\n      - ('pn', 'Pn#')\n      - ('none', '')  for cells that are not treated as activity locations on that day.\n    \"\"\"\n    if cell == home_cell:\n        return \"home\", \"HOME\"\n    if (sw_cell is not None) and (cell == sw_cell):\n        return \"sw\", \"SW\"\n\n    visited_today = visited_cells_by_day.get(day_date, set())\n    if cell in visited_today:\n        if first_visit_date.get(cell) == day_date:\n            return \"pv\", pv_label[cell]\n        else:\n            return \"pn\", pn_label.get((cell, day_date), \"PN\")\n\n    return \"none\", \"\"\n\n\ndef cell_center_xy(ci, rj):\n    \"\"\"Return the projected (x, y) centre of grid cell (ci, rj).\"\"\"\n    cx = gx0 + (ci + 0.5) * GRID_SIZE\n    cy = gy0 + (rj + 0.5) * GRID_SIZE\n    return cx, cy\n\n\n# --- 7.3 Build the merged visit-level table ---\n\nvisit_rows = []\nall_dates = sorted(traj[\"date_only\"].unique().tolist())\n\nfor d in all_dates:\n    day = traj[traj[\"date_only\"] == d].sort_values(\"datetime\").copy()\n    if day.empty:\n        continue\n\n    # Sequence of grid cells and timestamps for this day\n    cells = list(zip(day[\"ci\"].astype(int), day[\"rj\"].astype(int)))\n    times = day[\"datetime\"].tolist()\n\n    # 1) Raw run-length encoding by grid cell:\n    #    consecutive identical (ci, rj) are grouped into one block.\n    runs = []  # list of (cell, i0, i1) with indices into 'times'\n    if cells:\n        start = 0\n        for i in range(1, len(cells)):\n            if cells[i] != cells[i-1]:\n                runs.append((cells[i-1], start, i-1))\n                start = i\n        runs.append((cells[-1], start, len(cells) - 1))\n\n    # 2) Attach labels and time bounds; drop blocks that are not activity cells\n    tagged = []\n    for (cell, i0, i1) in runs:\n        tag, pid = classify_cell(cell, d)\n        if tag == \"none\":\n            continue  # ignore transit-only cells\n        tagged.append({\n            \"cell\": cell,\n            \"ci\": cell[0],\n            \"rj\": cell[1],\n            \"tag\": tag,\n            \"place_id\": pid,\n            \"i0\": i0,\n            \"i1\": i1,\n            \"first_dt\": times[i0],\n            \"last_dt\": times[i1],\n        })\n\n    if not tagged:\n        continue\n\n    # 3) Merge adjacent blocks with the same (ci, rj, tag)\n    merged = []\n    current = tagged[0]\n    for nxt in tagged[1:]:\n        if (\n            (nxt[\"ci\"] == current[\"ci\"]) and\n            (nxt[\"rj\"] == current[\"rj\"]) and\n            (nxt[\"tag\"] == current[\"tag\"])\n        ):\n            # extend the current block\n            current[\"i1\"] = nxt[\"i1\"]\n            current[\"last_dt\"] = nxt[\"last_dt\"]\n        else:\n            merged.append(current)\n            current = nxt\n    merged.append(current)\n\n    # 4) For each merged visit, compute attributes and within-day order\n    visit_order = 0     \n    action_order = 0    \n\n    for k, rec in enumerate(merged):\n        ci, rj = rec[\"ci\"], rec[\"rj\"]\n        tag, pid = rec[\"tag\"], rec[\"place_id\"]\n        first_dt, last_dt = rec[\"first_dt\"], rec[\"last_dt\"]\n\n        visit_order += 1\n\n        if tag == \"home\":\n            action_order = 0    \n            action_ord = 0\n        else:\n            if action_order == 0:\n                action_order = 1  \n            else:\n                action_order += 1\n            action_ord = action_order\n\n        # Next-step label (based on the next merged block)\n        if k &lt; len(merged) - 1:\n            next_tag = merged[k + 1][\"tag\"]\n        else:\n            next_tag = \"none\"\n\n        # Geometry-based attributes\n        cx, cy = cell_center_xy(ci, rj)\n        hx, hy = cell_center_xy(*home_cell)\n        dist_home = math.hypot(cx - hx, cy - hy)\n        center_lon, center_lat = cell_center_lonlat(ci, rj)\n\n        visit_rows.append({\n            \"person\": USER_ID,\n            \"date\": d.isoformat(),\n            \"first_time\": first_dt.time().isoformat(),\n            \"last_time\": last_dt.time().isoformat(),\n            \"duration_min\": round((last_dt - first_dt) / timedelta(minutes=1), 1),\n            \"grid_ci\": ci,\n            \"grid_rj\": rj,\n            \"grid_center_lon\": round(center_lon, 6),\n            \"grid_center_lat\": round(center_lat, 6),\n            \"dist_home_m\": round(dist_home, 1),\n            \"is_home\": 1 if tag == \"home\" else 0,\n            \"is_sw\":   1 if tag == \"sw\"   else 0,\n            \"is_pv\":   1 if tag == \"pv\"   else 0,\n            \"is_pn\":   1 if tag == \"pn\"   else 0,\n            \"place_id\": pid,          # HOME / SW / Pv# / Pn#\n            \"next_step\": next_tag,    # home / sw / pv / pn / none\n            \"visit_order_in_day\": visit_order,  \n            \"action_order\": action_ord         \n        })\n\n# Final visit-level table\nvisit_table = (\n    pd.DataFrame(visit_rows)\n    .sort_values([\"date\", \"first_time\", \"grid_ci\", \"grid_rj\"])\n    .reset_index(drop=True)\n)\n\nprint(\"Number of visit events:\", len(visit_table))\nvisit_table.head(10)\n\nTotal unique Pv cells: 106\nTotal Pn labels assigned: 294\nNumber of visit events: 1841\n\n\n\n\n\n\n\n\n\nperson\ndate\nfirst_time\nlast_time\nduration_min\ngrid_ci\ngrid_rj\ngrid_center_lon\ngrid_center_lat\ndist_home_m\nis_home\nis_sw\nis_pv\nis_pn\nplace_id\nnext_step\nvisit_order_in_day\naction_order\n\n\n\n\n0\n000\n2008-10-23\n03:00:55\n04:13:32\n72.6\n40\n50\n116.300184\n39.984308\n3201.6\n0\n0\n1\n0\nPv1\nsw\n1\n1\n\n\n1\n000\n2008-10-23\n09:42:25\n09:42:30\n0.1\n43\n55\n116.317527\n40.006935\n500.0\n0\n1\n0\n0\nSW\nhome\n2\n2\n\n\n2\n000\n2008-10-23\n09:42:35\n10:05:29\n22.9\n44\n55\n116.323385\n40.006970\n0.0\n1\n0\n0\n0\nHOME\nsw\n3\n0\n\n\n3\n000\n2008-10-23\n10:05:34\n10:30:15\n24.7\n43\n55\n116.317527\n40.006935\n500.0\n0\n1\n0\n0\nSW\nhome\n4\n1\n\n\n4\n000\n2008-10-23\n10:30:20\n11:10:57\n40.6\n44\n55\n116.323385\n40.006970\n0.0\n1\n0\n0\n0\nHOME\nnone\n5\n0\n\n\n5\n000\n2008-10-24\n02:09:59\n02:10:54\n0.9\n43\n55\n116.317527\n40.006935\n500.0\n0\n1\n0\n0\nSW\nhome\n1\n1\n\n\n6\n000\n2008-10-24\n02:10:59\n02:47:06\n36.1\n44\n55\n116.323385\n40.006970\n0.0\n1\n0\n0\n0\nHOME\nnone\n2\n0\n\n\n7\n000\n2008-10-26\n14:04:27\n14:12:42\n8.2\n55\n33\n116.388704\n39.908225\n12298.4\n0\n0\n1\n0\nPv4\npv\n1\n1\n\n\n8\n000\n2008-10-26\n14:23:42\n14:35:17\n11.6\n56\n31\n116.394633\n39.899246\n13416.4\n0\n0\n1\n0\nPv5\nnone\n2\n2\n\n\n9\n000\n2008-10-27\n12:03:59\n12:05:54\n1.9\n44\n55\n116.323385\n40.006970\n0.0\n1\n0\n0\n0\nHOME\nnone\n1\n0\n\n\n\n\n\n\n\n\n# save visit-level table for later notebooks\nOUT_DIR = \"data\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nout_path = os.path.join(OUT_DIR, f\"visit_level_table_{USER_ID}.csv\")\nvisit_table.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\nprint(\"Saved visit-level table to:\", out_path)\n\nSaved visit-level table to: data\\visit_level_table_000.csv",
    "crumbs": [
      "Analysis",
      "Notebook 1 – Data cleaning"
    ]
  },
  {
    "objectID": "Data Cleaning.html#mapping-inferred-activity-places",
    "href": "Data Cleaning.html#mapping-inferred-activity-places",
    "title": "Notebook 1 — From raw Geolife trajectories to visit-level events",
    "section": "8. Mapping inferred activity places",
    "text": "8. Mapping inferred activity places\nTo summarise the cleaned visit-level data, I project the inferred activity locations back onto an interactive map. Starting from the visit table, I collapse repeated visits to the same 500 m grid cell and draw one square for each distinct cell that ever appears as a visit. Each cell is coloured according to its role in the visit history (explored at least once versus only revisited), while the HOME and SW cells are highlighted separately as point markers. Compared with the raw trajectories, this map shows a much simpler picture of the user’s daily activity space: a small set of recurrent places organised around home, rather than every individual GPS point and transit segment.\n\n# --- 8.1 Aggregate unique places from the visit-level table ---\n\n# We keep only non-home visits (HOME is shown separately as a point),\n# and collapse repeated visits to the same grid cell.\nplaces = (\n    visit_table\n    .groupby([\"grid_ci\", \"grid_rj\"], as_index=False)\n    .agg(\n        any_home=(\"is_home\", \"max\"),\n        any_sw=(\"is_sw\", \"max\"),\n        any_pv=(\"is_pv\", \"max\"),\n        any_pn=(\"is_pn\", \"max\"),\n        grid_center_lon=(\"grid_center_lon\", \"first\"),\n        grid_center_lat=(\"grid_center_lat\", \"first\"),\n        dist_home_m=(\"dist_home_m\", \"min\")\n    )\n)\n\n\nprint(\"Number of unique grid cells appearing in visits:\", len(places))\n\n\n# --- 8.2 Helper: cell polygon in lat/lon ---\n\ndef cell_bounds_lonlat(ci, rj):\n    \"\"\"\n    Return the 4 corners (and closed ring) of grid cell (ci, rj)\n    as (lat, lon) pairs for use in folium.Polygon.\n    \"\"\"\n    x0 = gx0 + ci * GRID_SIZE\n    y0 = gy0 + rj * GRID_SIZE\n    x1 = x0 + GRID_SIZE\n    y1 = y0 + GRID_SIZE\n\n    lon0, lat0 = to_wgs(x0, y0)\n    lon1, lat1 = to_wgs(x1, y0)\n    lon2, lat2 = to_wgs(x1, y1)\n    lon3, lat3 = to_wgs(x0, y1)\n\n    # folium expects (lat, lon)\n    return [\n        (lat0, lon0),\n        (lat1, lon1),\n        (lat2, lon2),\n        (lat3, lon3),\n        (lat0, lon0),\n    ]\n\n\n# --- 8.3 Colour rule for places ---\n\ndef color_for_place(row):\n    cell = (int(row[\"grid_ci\"]), int(row[\"grid_rj\"]))\n    # HOME and SW will be shown as point markers; here we colour only squares\n    if sw_cell is not None and cell == sw_cell:\n        return \"green\"\n    # Pv vs Pn logic\n    if row[\"any_pn\"] == 1:\n        return \"orange\"   # ever seen as Pv at least once\n    if row[\"any_pv\"] == 1:\n        return \"purple\"   # only returns\n    return \"gray\"\n\n\n# --- 8.4 Initialise a folium map centred at HOME ---\n\n# Compute HOME centre in lat/lon (from grid indices)\nhx = gx0 + (home_cell[0] + 0.5) * GRID_SIZE\nhy = gy0 + (home_cell[1] + 0.5) * GRID_SIZE\nhome_lon, home_lat = to_wgs(hx, hy)\n\nm = folium.Map(location=[home_lat, home_lon],\n               zoom_start=12,\n               tiles=\"cartodbpositron\")\n\n\n# --- 8.5 Add grid-cell polygons for activity places ---\n\nfor _, row in places.iterrows():\n    ci = int(row[\"grid_ci\"])\n    rj = int(row[\"grid_rj\"])\n    poly_latlon = cell_bounds_lonlat(ci, rj)\n    col = color_for_place(row)\n\n    popup_html = (\n        f\"Cell: ({ci}, {rj})&lt;br&gt;\"\n        f\"Center: ({row['grid_center_lat']:.5f}, {row['grid_center_lon']:.5f})&lt;br&gt;\"\n        f\"Dist. to HOME: {int(round(row['dist_home_m']))} m&lt;br&gt;\"\n        f\"Any Pv: {int(row['any_pv'])} &nbsp; Any Pn: {int(row['any_pn'])}\"\n    )\n\n    folium.Polygon(\n        locations=poly_latlon,\n        color=col,\n        weight=2,\n        fill=True,\n        fill_opacity=0.35,\n        popup=popup_html\n    ).add_to(m)\n\n\n# --- 8.6 Add HOME and SW as point markers ---\n\nfolium.CircleMarker(\n    location=[home_lat, home_lon],\n    radius=7,\n    color=\"blue\",\n    fill=True,\n    fill_opacity=0.9,\n    popup=\"HOME\"\n).add_to(m)\n\nif sw_cell is not None:\n    sx = gx0 + (sw_cell[0] + 0.5) * GRID_SIZE\n    sy = gy0 + (sw_cell[1] + 0.5) * GRID_SIZE\n    sw_lon, sw_lat = to_wgs(sx, sy)\n\n    folium.CircleMarker(\n        location=[sw_lat, sw_lon],\n        radius=7,\n        color=\"green\",\n        fill=True,\n        fill_opacity=0.9,\n        popup=\"SW\"\n    ).add_to(m)\n\n\nm\n\nNumber of unique grid cells appearing in visits: 106\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Analysis",
      "Notebook 1 – Data cleaning"
    ]
  },
  {
    "objectID": "LR.html",
    "href": "LR.html",
    "title": "Project Overview",
    "section": "",
    "text": "This project uses high-resolution Geolife GPS trajectories to study everyday mobility as a sequence of discrete visits, trips, and exploration decisions.\nThe analysis is organised into three main notebooks, which move from data engineering to behavioural modelling and finally to cross-user comparison.\n\nNotebook 1 — From raw trajectories to visit-level events\nNotebook 1 establishes the data foundation. Starting from the original Geolife .plt files, it performs a series of spatial and temporal transformations to convert noisy GPS logs into a clean visit-level dataset. Raw points are first loaded and cleaned, then restricted to the Beijing study area and projected into a 500 m UTM grid that smooths GPS jitter and provides a common spatial reference. Night-time observations are used to infer the user’s primary home location (HOME) and a secondary frequent place (SW), while line–grid intersections identify grid cells that are genuinely “visited” rather than simply passed through in transit. Finally, consecutive points within the same activity cell are collapsed into visit events, each labelled as HOME, SW, first-time place (Pv), or return place (Pn), together with timing, distance-from-home, and within-day order attributes.\nThe resulting visit_level_table_XXX.csv is the input to all subsequent notebooks.\n\n\n\nNotebook 1 overview\n\n\n\n\nVisit-level concepts (Notebook 1)\n\nRaw GPS point / trajectory – Single Geolife record (lat, lon, timestamp); ordered points form a trajectory.\nGrid cell (ci, rj) – 500 m × 500 m UTM grid; every GPS point is assigned to a cell.\nHOME – Grid cell with the highest and most persistent night-time (00:00–06:00) presence.\nSW – Second-strongest night-time cell after removing HOME (e.g. workplace / dorm).\nActivity cell – Grid cell that is crossed by “enough” line segments in a day; road-only cells are excluded.\nVisit (visit-level event) – Continuous stay in the same activity cell; consecutive points merged, short GPS gaps bridged.\nPv (first-time place) – First day an activity cell (non-HOME/SW) appears.\nPn (return place) – Later days when the same activity cell is revisited.\nplace_id – Label for the cell: HOME, SW, Pv#, or Pn#.\ndist_home_m – Distance (metres) from the visit cell to HOME in UTM space.\nvisit_order_in_day – Chronological index of visits within a calendar day.\naction_order – Index that resets to 0 at HOME and counts non-home visits while the user is away.\nnext_step – Category of the following visit: home, sw, pv, pn, or none (end of day).\n\n\n\nNotebook 2 — Trip-level behaviour within a single user\nNotebook 2 takes the visit-level table for one user and reconstructs complete home–home trips. Visits are grouped into outings that start at HOME, move through one or more non-home stops, and end when HOME is reached again (or the day ends). Within each trip, the notebook analyses two key dimensions of behaviour.\nFirst, it estimates a discrete-time hazard of going home: for each stop order (k), it measures the probability that the next move is a return to HOME, conditional on the trip having already reached stop (k). This yields both empirical hazard and survival curves and a simple logit trend in stop order.\nSecond, it models the propensity for exploration by tracking when first-time places (Pv) occur along the sequence of stops, and fitting a logistic regression of the Pv indicator on within-trip stop order. Together, these components characterise whether a trip is a short one-stop errand or a longer outing, and how the balance between revisiting known places and exploring new ones evolves as the trip unfolds.\n\n\n\nNotebook 2 overview\n\n\n\n\nTrip-level variables (Notebook 2)\n\ntrip_id – Identifier of a home–home trip within a day (0 = not in any trip).\naction_order_in_trip – Order of non-home stops within a trip (0 at HOME, 1, 2, … away from HOME).\nn_stops – Number of non-home stops in a trip.\ntrip_start, trip_end – Start and end timestamps of the trip.\ntrip_duration_min – Trip duration in minutes.\nmax_dist_home, mean_dist_home – Maximum and average distance from HOME during the trip.\noutcome – For each non-home stop: home, explore (another non-home place), or end (trip ends).\nHazard of going home h_k – Probability that the next stop is HOME, conditional on the trip having reached stop (k).\nSurvival S_k – Probability that the trip is still ongoing after stop (k).\noverall_pv_share (within-trip) – Share of non-home stops that are Pv.\npv_odds_ratio_per_stop – Multiplicative change in the odds of Pv per extra stop in the trip (from a logit model).\n\n\n\nNotebook 3 — Cross-user comparison of daily exploration patterns\nNotebook 3 extends the analysis from a single individual to a panel of long-coverage Geolife users. For each user in the sample, it reloads the corresponding visit-level table, re-attaches the same home–home trip structure as in Notebook 2, and computes a compact set of trip-level and behavioural summaries: number of active days and trips, distribution of non-home stops per trip, user-specific going-home hazards, and within-trip exploration rates. The notebook then fits separate discrete-time logit models for the going-home hazard and the Pv probability for each user, producing a small set of interpretable parameters that describe their overall level of “home-boundness” and their tendency to explore as trips grow longer.\nFinally, these features are combined into a multivariate “trip behaviour space”, where users are compared and clustered into broad types (e.g. errand-oriented, revisiting-oriented, or exploratory roamers). In this way, Notebook 3 acts as a cross-sectional validation of the patterns identified in Notebooks 1 and 2 and shows how similar or heterogeneous daily exploration behaviour is across individuals.\n\n\nUser-level summary variables (Notebook 3)\nEach row of summary_fit summarises one user: - uid – User ID. - n_days_total – Days with any GPS data. - n_days_nonhome – Days with at least one non-home stop. - n_trips – Number of detected home–home trips. - mean_stops_per_trip – Average number of non-home stops per trip. - median_stops_per_trip – Median number of non-home stops per trip. - max_stops_per_trip – Maximum number of non-home stops in any trip. - hazard_h1 – Empirical probability of going home right after the first non-home stop. - hazard_h2_const – Average going-home hazard for stops (k ). - hazard_beta_k – Logit slope of the going-home hazard with respect to stop order (k). - overall_pv_share – Fraction of non-home trip stops that are Pv (first-time places). - pv_odds_ratio_per_stop – Odds ratio for Pv per additional stop (from the within-trip Pv logit model). - hazard_mse – Mean squared error between empirical and fitted going-home hazards. - pv_mse – Mean squared error between empirical and fitted Pv probabilities. - cluster3 – k-means cluster label (0, 1, 2), corresponding to multi-stop revisitors, errand-homebodies, and exploratory roamers. - PC1, PC2, PC3 – Scores on the first three principal components of the nine user-level features:\n- PC1 – Trip complexity and routine (long multi-stop vs short exploratory outings).\n- PC2 – Mobility frequency and intensity (how often and on how many days users go out).\n- PC3 – Late-trip behaviour (how quickly the return-home hazard rises or stays low at higher stop orders).",
    "crumbs": [
      "Analysis",
      "Overview"
    ]
  }
]